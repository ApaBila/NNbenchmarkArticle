% !TeX root = RJwrapper.tex
\title{A review of R neural network packages (with NNbenchmark)\(:\) accuracy
and ease of use}
\author{by Salsabila Mahdi, Akshaj Verma, Christophe Dutang, Patrice Kiener, John C. Nash}

\maketitle


\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

In the last three decades, neural networks (NN) have evolved from an
academic topic to a common scientific computing tool. CRAN currently
hosts approximately 80 packages in May 2020 involving neural network
modeling, some offering more than one algorithm. However, to our
knowledge, there is no comprehensive study which checks the accuracy,
the reliability and the ease-of-use of those NN packages.

In this paper, we attempted to test this rather large number of packages
against a common set of datasets with different levels of complexity,
and to benchmark and rank them with certain metrics.

Restricting our evaluation to regression algorithms applied on the
one-hidden layer perceptron and ignoring those for classification or
other specialized purposes, there were approximately 60
package::algorithm pairs left to test. The criteria used in our
benchmark were: (i) the accuracy, i.e.~the ability to find the global
minima on 13 datasets, measured by the Root Mean Square Error (RMSE) in
a limited number of iterations; (ii) the speed of the training
algorithm; (iii) the availability of helpful utilities; (iv) and the
quality of the documentation.

We have attempted to give a score for each evaluation criterion and to
rank each package::algorithm pair in a global table. Overall, 15 pairs
are considered accurate and reliable and can be recommended for daily
usage. Most others should be avoided as they are either less accurate,
too slow, too difficult to handle, or have poor or no documentation.

To carry out this work, we developed various codes and templates, as
well as the NNbenchmark package used for testing. This material is
available at \url{https://akshajverma.com/NNbenchmarkWeb/index.html} and
\url{https://github.com/pkR-pkR/NNbenchmark}, and can be used to verify
our work and, we hope, improve both packages and their evaluation.
Finally, we provide some hints and features to guide the development of
an idealized neural network package for R.

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The R Project for Statistical Computing (\url{www.r-project.org}), as
any opensource platform, relies on its contributors to keep it up to
date. Neural networks (NN), inspired on the brain's own connections
system, are a class of models in the growing field of machine learning
for which R has a number of tools. During the last 30 years, neural
networks have evolved from an academic topic to a common tool in
scientific computing. Previously, neural networks were considered more
theory than practice, partly because the algorithms used were
computationally demanding.

As a convenience in the general conversation, the same term is used in a
generic manner for different model structures and applications:
multilayer perceptron for regression, multilayer perceptron for
classification, multilayer perceptron for specialized applications,
recurrent neural network for autoregressive time series, convolutional
neural networks for dimension reduction and pattern recognition, deep
neural networks for image or voice recognition. Most of the above types
of neural networks can be found in R packages hosted on CRAN but without
any warranty about the accuracy or the speed of computation. This is an
issue as many poor algorithms are available in the literature and hence
poor packages implemented on CRAN.

A neural network algorithm requires complicated calculations to improve
the model control parameters. As with other optimization problems, the
gradient of the chosen cost function that indicates the lack of
suitability of the model is sought. This lets us improve the model by
changing the parameters in the negative gradient direction. Parameters
for the model are generally obtained using part of the available data (a
training set) and tested on the remaining data. Modern software allows
much of this work, including approximation of the gradient, to be
carried out without a large effort by the the user.

The training process can generally be made more efficient if we can also
approximate second derivatives of the cost function, allowing us to use
its curvature via the Hessian matrix. There are a large number of
approaches, of which quasi-Newton algorithms are perhaps the most common
and useful. Within this group, methods based on the
Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for updating the
(inverse) Hessian approximation provide several well-known examples. In
conducting this study, we believed that these second-order algorithms
would perform better than first-order methods for fit-in-memory
datasets.

Regardless of our belief, we wished to be able to conduct a thorough
examination of these training algorithms in R. There are many packages,
but barely any information to allow comparison. Our work, reported here,
aims to provide a framework for benchmarking neural network packages. We
restrict our examination to packages for R, and in this report focus on
those that provide neural networks of the perceptron type, that is, one
input layer, one normalized layer, one hidden layer with a nonlinear
activation function that is usually the hyperbolic tangent tanh(), and
one output output layer. The criteria used in our benchmark were: (i)
the accuracy, i.e.~the ability to find the global minima on 13 datasets
in a limited number of iterations; (ii) the speed of the training
algorithm; (iii) the availability of helpful utilities; (iv) and the
quality of the documentation. We restricted our evaluation to regression
algorithms applied on the one-hidden layer perceptron and ignored those
for classification or other specialized purposes.

\hypertarget{neural-networks-the-perceptron}{%
\section{Neural Networks: the
perceptron}\label{neural-networks-the-perceptron}}

Here, we give a short description of the one hidden layer perceptron. As
the ``layer'' term suggests it, some terms come from the representation
of graphs whereas some other terms come from the traditional literature
on nonlinear models.

Using the graph description, a one-hidden layer neural network is made
of 3 parts: (i) the layer of the input(s), (ii) the hidden layer which
consists of independant neurons, each of them performing two operations:
a linear combination of the inputs plus an offset, then a nonlinear
function applied on this linear combination. (iii) the layer of the
output(s) which is a linear combination of the output of the nonlinar
functions in the hidden layer.

The nonlinear function used in the hidden layer must have the following
four properties: continuous, differentiable, monotonic, bounded. The
logistic function, the hyperbolic tangent function and the arctangent
functions are the usual candidates.

The above description has a simple mathematical equivalence. Let us give
two examples.

The model
\(y = a1 + a2*tanh(a3 + a4*x) + a5*tanh(a6 + a7*x) + a8*tanh(a9 + a10*x)\)
describes a neural network with one input, three hidden neurons, one
output model where x is the input, tanh() is the activation function, y
is the output and \(a1,..,a10\) are the parameters.

The model
\(y = a1 + a2*atan(a3 + a4*x1 + a5*x2 + a6*x3 + a7*x4 + a8*x5) + a9*atan(a10 + a11*x1 + a12*x2 + a13*x3 + a14*x4 + a15*x5) + a16*atan(a17 + a18*x1 + a19*x2 + a20*x3 + a21*x4 + a22*x5)\)
describes a neural network with five inputs, three hidden neurons, one
output model where x is the input, atan() is the activation function, y
is the output and \(a1,..,a22\) are the parameters.

In order to get large gradients at the first steps of the training
algorithm, it is recommended to use normalized inputs, normalized
outputs, odd functions like the hyperbolic tangent function or the
arctangent function and small random values to initialize the
parameters, for instance extracted from the N(0, 0.1) distribution. Such
good practices help find good local minima and possibly the global
minimum.

The dataset used for the training is assumed to have a number of rows
much larger than the number of parameters. While «much larger» is
subject to discussion, values of 3 to 5 are generally accepted. (In
experimental design, some iterative strategies start with a dataset
having a number of experiments/lines equal to 1.8 times the number of
parameters and then increase the number of experiments to finetune the
model.)

It is rather clear from the mathematical formula above that neural
networks of perceptron type are nonlinear models and require for their
parameter estimation some training algorithms that can handle (highly)
nonlinear models. Indeed, the intrinsic and parametric curvatures of
such models are usually very high and, with so many parameters, the
Jacobian matrix might exhibit some collinearities between its columns
and become nearly singular. As a result, appropriate algorithms for such
dataset::model pairs are rather limited and well-known. They are the
second-order algorithms like BFGS and Levenberg-Marquardt (and
Horsehoe?).

JN??: What is the Horseshoe? Probably I know it by a different name.
Also Levenberg-Marquardt is a stabilization of the Jacobian that could
be applied to several algorithms. Should we say `Levenberg-Marquardt
stabilized Gauss-Newton', which is what nlsr uses.

Unfortunately, due to some simple literature on the gradient and the
hype around ``deep neural networks'' that manipulate ultra-large models
with hundreds or thousands parameters and sometimes more parameters than
examples in the datasets, many papers and many R packages emphasize the
use of first-order gradient algorithms. In the case of the perceptron,
this is an error and the goal of this paper is to demonstrate it.

JN??: replace previous paragraph with ??

Unfortunately, there are widely-discussed articles concerning the
gradient and ``deep neural networks'' that manipulate ultra-large models
with hundreds or thousands parameters and sometimes more parameters than
examples in the datasets. These, along with some R packages, emphasize
the use of first-order gradient algorithms. In the case of the
perceptron, we contend this is an error, and provide evidence to that
effect in this paper.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

When training neural networks, we attempt to tune a set of
hyperparameters so that the RMSE is minimized. When our method for such
adjustment can no longer reduce the RMSE, we say that the given
algorithm has converged. In practice, some implementations of algorithms
require that we stop the optimization process in exceptional situations
(e.g., a divide by zero), or a pre-set limit on the number of steps or
elapsed time is reached. Thus the implementation may terminate even when
the algorithm has not converged. At termination, we desire that the RMSE
be ``small''. Note that other measures could be used to judge when to
accept that the optimization is converged, for instance the Mean
Absolute Error (MAE). However, the MAE is not used either in a
convergence test nor in our overall ranking as there does not appear to
be consensus on its use. See, e.g.,
\citep{willmott2005advantages,chai2014root}.

In our tests, a termination limit for second-order algorithms is 200
iterations. On the other hand, first-order algorithms were set to
several values, depending on how well and how fast they converged:
\texttt{maxit1storderA=1000} iterations, \texttt{maxit1storderB=10000}
iterations, and \texttt{maxit1storderC=100000} iterations. The full list
of the maximum iteration number per package:algorithm is given in
Appendix C. We were unable to completely harmonize the hyperparameters
as an appropriate learning rate differed between package, despite the
algorithm being similarly named.

We measure performance primarily by relative computing time between
methods on a particular computing platform. We could also count measures
of iterations, function evaluations or similar quantities that indicate
the computing effort. We note that differences in machine architecture
and in the attached libraries (e.g., BLAS choices for R) will modify our
measures. We are putting our tools on a Github repository so that
further evaluation can be made by ourselves and others as hardware and
software evolves.

The resulting files recording performance that are in our repository
were mostly generated by one of us (SM) on a Windows system build
10.0.18362.752 with an i7-8750H CPU, a Intel(R) UHD Graphics 630 and
NVIDIA GeForce GTX 1060 chip, and 16 GB of RAM. The authors have
multiple computers with different operating software and configurations,
and our experiences on those systems appear to be consistent with the
results from the test system described above.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Our research process was divided into 3 phases.

\hypertarget{phase-1---preparation-of-benchmark-datasets}{%
\subsection{Phase 1 - Preparation of benchmark
datasets}\label{phase-1---preparation-of-benchmark-datasets}}

\hypertarget{datasets-need-to-be-finished}{%
\subsubsection{Datasets =\textgreater{} NEED TO BE
FINISHED??}\label{datasets-need-to-be-finished}}

All the datasets we use cannot generally be modeled using a
non-iterative calculation such as Ordinary Least Squares. Varying levels
of difficulty in modeling the different data sets are intended to allow
us to further classify different algorithms and the packages that
implement them. Sonja Surjanovic and Derek Bingham of Simon Fraser
University created a useful website from which three of the multivariate
datasets were drawn. We note the link, name and difficulty level of the
three datasets:\\
- \url{http://www.sfu.ca/~ssurjano/fried.html} (Friedman - average)\\
- \url{http://www.sfu.ca/~ssurjano/detpep10curv.html} (Dette - medium)\\
- \url{http://www.sfu.ca/~ssurjano/ishigami.html} (Ishigami - high)\\
The other multivariate dataset, Ref153, was taken from \ldots{}

Three of the univariate datasets we used were taken from a website of
the US National Institute for Standards and Technology (NIST):
\url{https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml}. (Gauss1 -
low; Gauss2 - low; Gauss3 - average)

Univariate datasets Dmod1, Dmod2 are from \ldots{}

Dreyfus1 is a pure neural network which has no error. This can make it
difficult for algorithms that assume an error exists. Dreyfus2 is
Dreyfus1 with errors. NeuroOne from \ldots{}

Finally, we also consider a Simon Wood test dataset, used in
\citep{wood2011fast} for benchmarking generalized additive models.
Precisely, we consider a generation of Gaussian random variates \(Y_i\),
\(i=1,\dots,n\) with the mean \(\mu_i\) defined as \[
\mu_i = 1+ f_0(x_{i,0})+f_1(x_{i,1})+f_2(x_{i,2})+f_3(x_{i,3})
+f_4(x_{i,4})+f_0(x_{i,5})
\] and standard deviation \(\sigma=1/4\) where \(f_j\) are Simon Wood's
smooth functions defined in Appendix B, \(x_{i,j}\) are uniform variates
and \(n=20,000\).

\hypertarget{packages}{%
\subsubsection{Packages}\label{packages}}

Using \CRANpkg{RWsearch} \citep{R-RWsearch} we sought to automate the
process of searching for neural network packages. All packages that have
``neural network'' as a keyword in the package title or in the package
description were included. In May 2020, around 80 packages falls into
this category. Packages \pkg{nlsr}, \pkg{minpack.lm}, \pkg{caret} were
added because the former 2 are important implementations of second-order
algorithms while the latter is the first cited meta package in the
CRAN's task view for machine learning,
\url{https://CRAN.R-project.org/view=MachineLearning}, as well as the
dependency for some of the other packages tested. Restricting to
regression analysis left us with 49 package::algorithm pairs in 2019 and
60 package::algorithm pairs in 2020.

\hypertarget{phase-2---review-of-packages-and-development-of-a-benchmarking-template}{%
\subsection{Phase 2 - Review of packages and development of a
benchmarking
template}\label{phase-2---review-of-packages-and-development-of-a-benchmarking-template}}

From documentation and example code, we learned that not all packages
selected by the automated search fit the scope of our research. Some
have no function to generate neural networks. Others were not regression
neural networks of the perceptron type or were only intended for very
specific purposes. Basically, each package was inspected 3 times.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The discard/not discard phase: depending on the package, this could be
  decided as easily as looking at the DESCRIPTION file or having to go
  through the process of making the code and seeing the results.
\item
  Benchmarking with template that was developed in 2019 and encapsulated
  in the functions of 2020, keeping notes of whether or not the package
  was easy to use.
\end{enumerate}

\textbf{Templates for Testing Accuracy and Speed}

As we inspected the packages, we developed a template for benchmarking.
The structure of this template (for each package) is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set up the test environment - loading of packages, setting working
  directory and options;
\item
  Summary of tested datasets;
\item
  Loop over datasets:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    setting parameters for a specific dataset,
  \item
    selecting benchmark options,
  \item
    training a neural network with a tuned functions for each package,
  \item
    calculation of convergence metrics (RMSE, MAE, WAE),
  \item
    plot each training over one initial graph, then plot the best
    result,
  \item
    add results to the appropriate existing record (*.csv file) and
  \item
    clear the environment for next loop.
  \end{enumerate}
\item
  Clearing up the environment for the next package. It is optional to
  print warnings.
\end{enumerate}

To simplify this process, we developed tools in the NNbenchmark package,
of which the first version was created as part of GSoC 2019. In GSoC
2020, 3 functions encapsulating the template, that had been made generic
with an extensive use of the incredible \code{do.call} function, were
added:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \code{trainPredict\_1mth1data} a neural network is trained on one
  dataset and then used for predictions, with several utilities. Then,
  the performance of the neural network is exported, plotted and/or
  summarized.
\item
  \code{trainPredict\_1data} serves as a wrapper function for
  trainPredict\_1mth1data for multiple methods.
\item
  \code{trainPredict\_1pkg} serves as a wrapper function for
  trainPredict\_1mth1data for multiple datasets.
\end{enumerate}

A function for the summary of accuracy and speed, \code{NNsummary}, was
also added. The package repository is
\url{https://github.com/pkR-pkR/NNbenchmark}, with package templates in
\url{https://github.com/pkR-pkR/NNbenchmarkTemplates}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  summarizing or re-reviewing the tested packages utility functions \&
  documentation
\end{enumerate}

\textbf{Ease of Use Scoring}

We define an ease-of-use measure based on what we considered a user
would need when using a neural network package for nonlinear regression,
namely, utility functions and sufficient documentation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Utilities (1 star)

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    a predict function exists
  \item
    scaling capabilities exist
  \end{enumerate}
\item
  Sufficient documentation (2 stars)

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    the existence of useful example/vignette = (1 star)

    \begin{itemize}
    \tightlist
    \item
      clear, with regression = 2 points
    \item
      unclear, examples use iris or are for classification only = 1
      point
    \item
      no examples = 0 points
    \end{itemize}
  \item
    input/output is clearly documented, e.g., what values are expected
    and returned by a function = (1 star)

    \begin{itemize}
    \tightlist
    \item
      clear input and output = 2 points
    \item
      only one is clear = 1 point
    \item
      both are not documented = 0 points
    \end{itemize}
  \end{enumerate}
\end{enumerate}

The ease-of-use measure ranges from 0 to 3 stars.

\hypertarget{phase-3---collection-of-and-analysis-of-results}{%
\subsection{Phase 3 - Collection of and analysis of
results}\label{phase-3---collection-of-and-analysis-of-results}}

\hypertarget{results-collection}{%
\subsubsection{Results collection}\label{results-collection}}

Looping over the datasets using each package template, we collected
results in the relevant package directories in the templates repository.

\hypertarget{analysis}{%
\subsubsection{Analysis}\label{analysis}}

To rank how well a package converged and its speed, we developed the
following method:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The results datasets are loaded into the R environment as one large
  list. The dataset names, package:algorithm names and all 10 run
  numbers, durations, and RMSE are extracted from that list
\item
  For the duration score (DUR), the duration is averaged by dataset. 3
  criteria for the RMSE score by dataset are calculated:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The minimum value of RMSE for each package:algorithm as a measure of
    their best performance
  \item
    The median value of RMSE for each package:algorithm as a measure of
    their average performance, without the influence of outliers
  \item
    The spread of the RMSE values for each package which is measured by
    the difference between the median and the minimum RMSE (d51)
  \end{enumerate}
\item
  Then, the ranks are calculated for every dataset and the results are
  merged into one wide dataframe.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The duration rank only depends on the duration.
  \item
    For minimum RMSE values, ties are decided by duration mean, then the
    RMSE median
  \item
    For median RMSE values, ties are decided by the RMSE minimum, then
    the duration mean
  \item
    The d51 rank only depends on itself
  \end{enumerate}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  A global score for all datasets is found by a sum of the ranks (of
  duration, minimum RMSE, median RMSE, d51 RMSE) of each
  package:algorithm for each dataset
\item
  The final table is the result of ranking by the global minimum RMSE
  scores for each package:algorithm
\end{enumerate}

\hypertarget{results}{%
\section{Results}\label{results}}

Table 1 gives the RMSE ranks, time ranks, and scores for ease of use for
each package::algorithm pair. A more complete list of metric scores and
hyperparameters is given in Table 2 in Appendix C.

\textbf{Tables}

\begin{Schunk}
\begin{table}

\caption{\label{tab:unnamed-chunk-2}Result from Tested Packages}
\centering
\fontsize{7}{9}\selectfont
\begin{tabular}[t]{>{\bfseries}lcclcc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Individual score} & \multicolumn{1}{c}{ } & \multicolumn{2}{c}{Global score} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){5-6}
Package & Util & Doc & Algorithm & Time & RMSE\\
\midrule
 & 1 & 3.0 & ADAPTgd & 10 & 34\\

 & 1 & 3.0 & ADAPTgdwm & 17 & 25\\

 & 1 & 3.0 & BATCHgd & 39 & 40\\

\multirow{-4}{*}{\raggedright\arraybackslash AMORE} & 1 & 3.0 & BATCHgdwm & 40 & 39\\
\cmidrule{1-6}
 & 2 & 3.0 & adam & 16 & 33\\

 & 2 & 3.0 & rmsprop & 14 & 28\\

\multirow{-3}{*}{\raggedright\arraybackslash ANN2} & 2 & 3.0 & sgd & 12 & 41\\
\cmidrule{1-6}
 & 1 & 3.0 & trainwgrad\_adam & 50 & 18\\

 & 1 & 3.0 & trainwgrad\_RMSprop & 47 & 26\\

\multirow{-3}{*}{\raggedright\arraybackslash automl} & 1 & 3.0 & trainwpso & 57 & 42\\
\cmidrule{1-6}
brnn & 2 & 4.0 & Gauss-Newton & 8 & 13\\
\cmidrule{1-6}
 & 2 & 3.0 & optim(BFGS) & 46 & 10\\

 & 2 & 3.0 & pso\_psoptim & 54 & 54\\

\multirow{-3}{*}{\raggedright\arraybackslash CaDENCE} & 2 & 3.0 & Rprop & 56 & 51\\
\cmidrule{1-6}
caret & 2 & 3.0 & avNNet\_nnet\_optim(BFGS) & 9 & 22\\
\cmidrule{1-6}
 & 2 & 3.0 & adam & 32 & 45\\

 & 2 & 3.0 & gradientDescent & 52 & 58\\

 & 2 & 3.0 & momentum & 53 & 56\\

\multirow{-4}{*}{\raggedright\arraybackslash deepdive} & 2 & 3.0 & rmsProp & 34 & 53\\
\cmidrule{1-6}
deepnet & 1 & 3.0 & BP & 23 & 18\\
\cmidrule{1-6}
elmNNRcpp & 2 & 3.0 & ELM & 1 & 59\\
\cmidrule{1-6}
ELMR & 2 & 3.0 & ELM & 2 & 60\\
\cmidrule{1-6}
EnsembleBase & 1 & 1.0 & nnet\_optim(BFGS) & 5 & 12\\
\cmidrule{1-6}
h2o & 2 & 2.0 & first-order & 51 & 11\\
\cmidrule{1-6}
 & 2 & 0.0 & adadelta & 60 & 47\\

 & 2 & 0.0 & adagrad & 58 & 36\\

 & 2 & 0.0 & adam & 42 & 35\\

 & 2 & 0.0 & adamax & 48 & 23\\

 & 2 & 0.0 & nadam & 44 & 36\\

 & 2 & 0.0 & rmsprop & 37 & 52\\

\multirow{-7}{*}{\raggedright\arraybackslash keras} & 2 & 0.0 & sgd & 48 & 43\\
\cmidrule{1-6}
MachineShop & 1 & 3.0 & nnet\_optim(BFGS) & 6 & 4\\
\cmidrule{1-6}
minpack.lm & 1 & 3.5 & Levenberg-Marquardt & 13 & 24\\
\cmidrule{1-6}
 & 2 & 3.5 & optimx(BFGS) & 26 & 9\\

\multirow{-2}{*}{\raggedright\arraybackslash monmlp} & 2 & 3.5 & optimx(Nelder-Mead) & 32 & 46\\
\cmidrule{1-6}
 & 1 & 3.0 & backprop & 37 & 48\\

 & 1 & 3.0 & rprop- & 21 & 21\\

 & 1 & 3.0 & rprop+ & 18 & 20\\

 & 1 & 3.0 & sag & 41 & 38\\

\multirow{-5}{*}{\raggedright\arraybackslash neuralnet} & 1 & 3.0 & slr & 31 & 30\\
\cmidrule{1-6}
nlsr & 1 & 4.0 & NashLM & 18 & 1\\
\cmidrule{1-6}
nnet & 1 & 3.0 & optim (BFGS) & 3 & 3\\
\cmidrule{1-6}
qrnn & 2 & 3.0 & nlm() & 28 & 15\\
\cmidrule{1-6}
radiant.model & 2 & 2.0 & nnet\_optim(BFGS) & 11 & 7\\
\cmidrule{1-6}
rminer & 2 & 3.5 & nnet\_optim(BFGS) & 14 & 2\\
\cmidrule{1-6}
 & 2 & 3.0 & BackpropBatch & 43 & 49\\

 & 2 & 3.0 & BackpropChunk & 26 & 29\\

 & 2 & 3.0 & BackpropMomentum & 25 & 30\\

 & 2 & 3.0 & BackpropWeightDecay & 29 & 32\\

 & 2 & 3.0 & Quickprop & 45 & 57\\

 & 2 & 3.0 & Rprop & 24 & 16\\

 & 2 & 3.0 & SCG & 30 & 17\\

\multirow{-8}{*}{\raggedright\arraybackslash RSNNS} & 2 & 3.0 & Std\_Backpropagation & 22 & 27\\
\cmidrule{1-6}
snnR & 2 & 2.0 & SemiSmoothNewton & 7 & 49\\
\cmidrule{1-6}
traineR & 1 & 2.5 & nnet\_optim(BFGS) & 4 & 6\\
\cmidrule{1-6}
 & 1 & 4.0 & optim(BFGS) & 35 & 4\\

 & 1 & 4.0 & optim(CG) & 59 & 8\\

 & 1 & 4.0 & optim(L-BFGS-B) & 36 & 14\\

 & 1 & 4.0 & optim(Nelder-Mead) & 55 & 44\\

\multirow{-5}{*}{\raggedright\arraybackslash validann} & 1 & 4.0 & optim(SANN) & 20 & 55\\
\bottomrule
\end{tabular}
\end{table}

\end{Schunk}

\hypertarget{discussion-and-recommendations}{%
\subsection{Discussion and
Recommendations}\label{discussion-and-recommendations}}

The following is a list of packages we included in this study, with
brief descriptions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \CRANpkg{AMORE} \citep{R-AMORE},
\item
  \CRANpkg{ANN2} \citep{R-ANN2},
\item
  \CRANpkg{appnn} \citep{R-appnn},
\item
  \CRANpkg{autoencoder} \citep{R-autoencoder},
\item
  \CRANpkg{automl} \citep{R-automl},
\item
  \CRANpkg{BNN} \citep{R-BNN},
\item
  \CRANpkg{brnn} \citep{R-brnn},
\item
  \CRANpkg{Buddle} \citep{R-Buddle},
\item
  \CRANpkg{CaDENCE} \citep{R-CaDENCE},
\item
  \CRANpkg{cld2} \citep{R-cld2},
\item
  \CRANpkg{cld3} \citep{R-cld3},
\item
  \CRANpkg{condmixt} \citep{R-condmixt},
\item
  \CRANpkg{DamiaNN} \citep{R-DamiaNN},
\item
  \CRANpkg{deep} \citep{R-deep},
\item
  \CRANpkg{deepdive} \citep{R-deepdive},
\item
  \CRANpkg{deepnet} \citep{R-deepnet},
\item
  \CRANpkg{deepNN} \citep{R-deepNN},
\item
  \CRANpkg{DNMF} \citep{R-DNMF},
\item
  \CRANpkg{elmNNRcpp} \citep{R-elmNNRcpp},
\item
  \CRANpkg{ELMR} \citep{R-ELMR},
\item
  \CRANpkg{EnsembleBase} \citep{R-EnsembleBase},
\item
  \CRANpkg{evclass} \citep{R-evclass},
\item
  \CRANpkg{gamlss.add} \citep{R-gamlss.add},
\item
  \CRANpkg{gcForest} \citep{R-gcForest},
\item
  \CRANpkg{GMDH} \citep{R-GMDH},
\item
  \CRANpkg{GMDH2} \citep{R-GMDH2},
\item
  \CRANpkg{GMDHreg} \citep{R-GMDHreg},
\item
  \CRANpkg{gnn} \citep{R-gnn},
\item
  \CRANpkg{grnn} \citep{R-grnn},
\item
  \CRANpkg{h2o} \citep{R-h2o},
\item
  \CRANpkg{hybridEnsemble} \citep{R-hybridEnsemble},
\item
  \CRANpkg{isingLenzMC} \citep{R-isingLenzMC},
\item
  \CRANpkg{keras} \citep{R-keras},
\item
  \CRANpkg{kerasR} \citep{R-kerasR},
\item
  \CRANpkg{leabRa} \citep{R-leabRa},
\item
  \CRANpkg{learNN} \citep{R-learNN},
\item
  \CRANpkg{LilRhino} \citep{R-LilRhino},
\item
  \CRANpkg{minpack.lm} \citep{R-minpack.lm},
\item
  \CRANpkg{MachineShop} \citep{R-MachineShop},
\item
  \CRANpkg{monmlp} \citep{R-monmlp},
\item
  \CRANpkg{neural} \citep{R-neural},
\item
  \CRANpkg{neuralnet} \citep{R-neuralnet},
\item
  \CRANpkg{NeuralNetTools} \citep{R-NeuralNetTools},
\item
  \CRANpkg{NeuralSens} \citep{R-NeuralSens},
\item
  \CRANpkg{NlinTS} \citep{R-NlinTS},
\item
  \CRANpkg{nlsr} \citep{R-nlsr},
\item
  \CRANpkg{nnet} \citep{R-nnet},
\item
  \CRANpkg{nnetpredint} \citep{R-nnetpredint},
\item
  \CRANpkg{nnfor} \citep{R-nnfor},
\item
  \CRANpkg{nntrf} \citep{R-nntrf},
\item
  \CRANpkg{nnli2bRcpp} \citep{R-nnlib2Rcpp},
\item
  \CRANpkg{onnx} \citep{R-onnx},
\item
  \CRANpkg{OptimClassifier} \citep{R-OptimClassifier},
\item
  \CRANpkg{OSTSC} \citep{R-OSTSC},
\item
  \CRANpkg{pnn} \citep{R-pnn},
\item
  \CRANpkg{polyreg} \citep{R-polyreg},
\item
  \CRANpkg{predictoR} \citep{R-predictoR},
\item
  \CRANpkg{qrnn} \citep{R-qrnn},
\item
  \CRANpkg{QuantumOps} \citep{R-QuantumOps},
\item
  \CRANpkg{quarrint} \citep{R-quarrint},
\item
  \CRANpkg{radiant.model} \citep{R-radiant.model},
\item
  \CRANpkg{rasclass} \citep{R-rasclass},
\item
  \CRANpkg{rcane} \citep{R-rcane},
\item
  \CRANpkg{regressoR} \citep{R-regressoR},
\item
  \CRANpkg{rminer} \citep{R-rminer},
\item
  \CRANpkg{rnn} \citep{R-rnn},
\item
  \CRANpkg{RSNNS} \citep{R-RSNNS},
\item
  \CRANpkg{ruta} \citep{R-ruta},
\item
  \CRANpkg{simpleNeural} \citep{R-simpleNeural},
\item
  \CRANpkg{snnR} \citep{R-snnR},
\item
  \CRANpkg{softmaxreg} \citep{R-softmaxreg},
\item
  \CRANpkg{Sojourn.Data} \citep{R-Sojourn.Data},
\item
  \CRANpkg{spnn} \citep{R-spnn},
\item
  \CRANpkg{TeachNet} \citep{R-TeachNet},
\item
  \CRANpkg{tensorflow} \citep{R-tensorflow},
\item
  \CRANpkg{tfestimators} \citep{R-tfestimators},
\item
  \CRANpkg{trackdem} \citep{R-trackdem},
\item
  \CRANpkg{TrafficBDE} \citep{R-TrafficBDE},
\item
  \CRANpkg{tsensembler} \citep{R-tsensembler},
\item
  \CRANpkg{validann} \citep{R-validann},
\item
  \CRANpkg{zFactor} \citep{R-zFactor}.
\end{enumerate}

\hypertarget{nd-order-algorithms}{%
\subsubsection{2nd order algorithms}\label{nd-order-algorithms}}

Out of all the algorithms, the following second algorithms generally
performed better in terms of convergence despite being set to a much
lower number of iterations, to be precise a fifth or even less, than the
first-order algorithms.

An important finding is that 11 out of 15 of these package::algorithms
use the algorithms included in \code{optim} from \CRANpkg{stats}. 2 of
them, \CRANpkg{CaDENCE}'s BFGS \citep{R-CaDENCE} and
\CRANpkg{validann}'s BFGS and L-BFGS-B \citep{R-validann}, do so with no
intermediate package. However, it is not clearly stated in CaDENCE's
documentation that optim's BFGS algorithm is used and not one of the
other algorithms. Furthermore, the mention of Nelder-Mead in the
documentation might lead users to believe that optim's Nelder-Mead is
used instead. Speed and variation between results are also not as good
as other package's that use optim. This could be because CaDENCE is
intended for probabilistic nonlinear models with a full title of
``Conditional Density Estimation Network Construction and Evaluation''.
On the other hand, validann is clearly a package that allows a user to
use all optim's algorithms. validann::L-BFGS-B ranks lower than
validann::BFGS in just about everything for most runs, despite the
former being more sophisticated. This is probably due to our efforts to
harmonize parameters under-utilizing the possibilities of the L-BFGS-B
algorithm. Both CaDENCE and validann's BFGS are outperformed by nnet,
especially in terms of speed.

\CRANpkg{nnet} \citep{R-nnet} differs from the two packages because it
uses the C code from optim (converted earlier from Fortran) instead of
calling optim from R. It also only implements the BFGS algorithm. This
could be what allows it to be faster. nnet is only beaten by the Extreme
Learning Machine (ELM) algorithms in terms of speed. However, there is a
larger variation between results (see the RMSEd51.score in Appendix C)
in comparison to validann::BFGS. Most likely, the different default
values are the cause of this. For instance, nnet uses a range of initial
random weights of 0.7 while validann uses a value of 0.5. In spite of
these results, the real reason most authors or users are likely to
choose nnet is because it ships with base R and is even mentioned as the
very first package in CRAN's task view for machine learning.

Our research found that 6 of the 11 packages that use optim do so
through nnet. Moreover, 8 packages for neural networks, though not
tested, use nnet. The total number of nnet dependencies found through a
search through the offline database of CRAN with RWsearch came up with
136 packages, although some might be using nnet for the multinomial
log-linear models, not neural networks. As for the ones we tested, there
were several similarities and differences. The packages that use nnet
for neuralnetworks are often meta packages with a host of other machine
learning algorithms. \CRANpkg{caret} \citep{R-caret}, also mentioned in
the taskview, boasts 238 methods with around 13 different neural network
packages with somewhat deceivingly simple name of ``Classification and
Regression Training''. It has many pre-processing utilities available,
as well as other tools.

\CRANpkg{EnsembleBase} \citep{R-EnsembleBase} maybe useful for those who
wish to make ensembles and test a grid of parameters although the
documentation. \CRANpkg{MachineShop} \citep{R-MachineShop} has 51
algorithms, with some additional information about the response variable
types in the second vignette, functions for preprocessing and tuning,
performance assesment, and presentation of results.
\CRANpkg{radiant.model} \citep{R-radiant.model} has an unchangeable
maxit of 10000 in the original package. Perhaps the author thought this
was reasonable as source of the algorithm, nnet, is quite fast. We
changed this to harmonize the parameters. \CRANpkg{rminer}
\citep{R-rminer} is the only package dependant on nnet that ranks above
nnet at number 2 for minimum RMSE, and even number 1 in some runs. It
also ranks number 1 on the other accuracy measures (median RMSE, minimum
MAE, minimum WAE) and is only behind deepdive and minpack.lm in terms of
results that are consistent and do not vary (RMSEd51). The difference is
probably from the change of maximum allowable weights in rminer to 10000
from 1000 in nnet, which is also probably the reason it its fits are
slower. \CRANpkg{traineR} \citep{R-traineR} claims to unify the
different methods of creating models between several learning
algorithms.

Something worth noting is that nnet and validann do not have external
normalization, and it is especially recommended for validann . However,
some of the packages dependent on nnet do have this utility and it is
included in the scoring for ease of use. With NNbenchmark, this is done
through setting scale = TRUE in the function \code{prepare.ZZ}. Note
that scaling might lead to complicating the constraints which is not
always worth it. Regardless, users might want to have the utility and
most likely want a clear explanation of the method chosen to center the
variables. Scaling is one of the things that \CRANpkg{optimx}
\citep{R-optimx} incorporates in an attempt to make a more useful
version of optim that only allows for changing the sign of the function
which might not even be considered as scaling \citep{Nash-nlpor14}.

optimx itself still has several algorithms yet to be used by neural
network packages, although it does have 45 packages dependent on it at
the moment. \CRANpkg{monmlp} \citep{R-monmlp} of all the packages, only
monmlp uses optimx for its 2 algorithms: BFGS and Nelder-Mead. It's
implementation of BFGS does not particularly stand out in comparison to
the ones from optim. Note however, that the author, Alex J. Cannon who
is also the author of CaDENCE, has once again created a package meant to
fill a certain niche. This package is intended for multi-layer
perceptrons with optional partial monotonicity constraints. GAM-style
effect plots are also an interesting utility. Another package by Cannon
is \CRANpkg{qrnn} \citep{R-qrnn} which uses yet another algorithm:
nlm(), a ``Newton-type'' algorithm from stats. Although it's performance
is at the bottom of second order algorithms, sometimes even being beaten
by first order algorithms, this could also be because of what the
package is intended for. qrnn is designed for quantile regression neural
networks, with several options. Cannon has included automatic scaling
for all 3 of his packages.

\hypertarget{recommended-1st-order-algorithms}{%
\subsubsection{Recommended: 1st order
algorithms}\label{recommended-1st-order-algorithms}}

validann optim CG -slow RSNNS SCG h2o back-propagation RSNNS Rprop ANN2
adam CaDENCE Rprop -SLOW deepnet BP AMORE ADAPTgdwm AMORE ADAPTgd ANN2
sgd automl trainwgrad ANN2 rmsprop RSNNS BackpropChunk RSNNS
BackWeightDecay RSNNS Std\_Backpropagation RSNNS BackpropMomentum automl
trainwpso validann optim NelderMead snnR Semi Smooth Newton RSNNS
BackpropBatch validann optim SANN monmlp optimx Nelder Mead

\hypertarget{not-recommended-1st-order-algorithms---discuss-cutoff}{%
\subsubsection{Not recommended: 1st order algorithms \textless- DISCUSS
CUTOFF}\label{not-recommended-1st-order-algorithms---discuss-cutoff}}

By package ELMR, elmNNRcpp - fast ELM algorithms. Unfortunately, can't
finetune, does not converge well. neuralnet: a large ammount of
iterations, slow, erratic failures tensorflow: NOT EASY TO USE,
subsequently keras, tfestimators, ruta \ldots{} user needs to understand
the language However, advanced users might be able to highly specify a
neural network to their needs (customization?)

By algorithm: neuralnet rprop+ neuralnet rprop- neuralnet slr - once
ranked well with 100000 iterations AMORE BATCHgd CaDENCE pso psoptim -
need to reconfigure? elmNNRcpp - fast, no iterations RSNNS Quickprop (?)
AMORE BATCHgdwm tensorflow MomentumOptimizer tensorflow AdamOptimizer
ELMR - fast, no iterations tensorflow GradientDescentOptimizer keras
rmsprop keras adagrad keras sgd keras adadelta tensorflow
AdagradOptimizer keras adam tensorflow FtrlOptimizer neuralnetwork sag
tensorflow AdadeltaOptimizer neuralnet backprop - note, might not
actually reflect standings, somehow from template to template the
learning rate disappeared. Will fix this in future runs

\hypertarget{untested-to-do---list}{%
\subsubsection{Untested =\textgreater{} TO DO -
LIST}\label{untested-to-do---list}}

\hypertarget{conclusion-and-perspective}{%
\section{Conclusion and perspective}\label{conclusion-and-perspective}}

\hypertarget{positives}{%
\subsection{Positives}\label{positives}}

\begin{itemize}
\tightlist
\item
  We are happy to note the existence of neural network packages in R
  with algorithms that converge well.
\item
  \pkg{nnet}, which uses ?? need font choice?? optim's BFGS method, is
  already often chosen to represent neural networks for packages that
  are either a collection of independent machine learning algorithms,
  ensembles, or even applications in a field such as \ldots{} ?? need to
  complete sentence??. JN??: Why is this positive? ==\textgreater{} B:
  its meant to be part of the above, because nnet's algorithm converged
  well in our tests, thus the fact that it is being used by other
  packages is a plus? See the part about nnet \& packages that depend on
  it in the results
\item
  R users have access to a wide variety of neural network methods,
  including from libraries of other programming languages and using many
  different types of algorithms. ?? have we defined hyperparameters??
  hyperparameters, and uses 
\end{itemize}

\hypertarget{negatives}{%
\subsection{Negatives}\label{negatives}}

\begin{itemize}
\tightlist
\item
  We are disappointed that many of the packages we reviewed had poor
  documentation.
\item
  It would be helpful if there were more packages with (different)
  second order algorithms. A number of the 
\item
  We often found it difficult to discover what default starting values
  were used for model parameters, or 
\end{itemize}

\hypertarget{future-work}{%
\subsection{Future work}\label{future-work}}

As the field of neural networks continue to grow, there will always be
more algorithms to validate. For current packages available for R, we
believe our research should be extended to encompass more types of
neural networks and their data formats (classifier neural networks,
recurrent neural networks, and so on). Different rating schemes and
different settings for package functions should also be tried out,
though such work suffers from the curse of too many dimensions of
effort.

We also aspire to the possibility of an idealized neural network
package, in particular for R. From the work in this paper, we believe
that such a package should offer

\begin{itemize}
\tightlist
\item
  ease of use in setting up computations and attaching data
\item
  the incorporation of second-order optimization methods in training the
  model, with appropriate computation of derivative information and
  appropriate internal steps, e.g., Levenberg-Marquardt stabilization
\item
  good documentation and controls, especially of initial values for
  model parameters
\end{itemize}

\hypertarget{acknowledgements}{%
\subsection{Acknowledgements}\label{acknowledgements}}

This work was possible due to the support of the Google Summer of Code
initiative for R during years 2019 and 2020. Students Salsabila Mahdi
(2019 and 2020) and Akshaj Verma (2019) are grateful to Google for the
financial support.

\bibliography{RJreferences}

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{appendix-a}{%
\subsection{Appendix A}\label{appendix-a}}

Consider a set of observations \(y_i\) and its corresponding predictions
\(\hat y_i\) for \(i=1,\dots,n\). The three metrics used were: \[
MAE = \frac1n\sum_{i=1}^n|y_i - \hat y_i|,~
RMSE = \frac1n\sqrt{\sum_{i=1}^n(y_i - \hat y_i)^2},~
WAE = \frac1n\max_{i=1,\dots,n}|y_i - \hat y_i|.
\] These values represent the absolute, the squared and the maximum norm
of residual vectors.

\hypertarget{appendix-b}{%
\subsection{Appendix B}\label{appendix-b}}

We define three smooth functions for Simon Wood's test dataset \[
f_0=5*\sin(2\pi x),~
f_1=exp(3*x)-7
f_2=0.5 x^{11}*(10(1 - x))^6 - 10 (10*x)^3*(1 - x)^{10},~
\] \[
f_3=15 \exp(-5 |x-1/2|)-6,~
f_4=2-1_{(x <= 1/3)}(6*x)^3 - 1_{(x >= 2/3)} (6-6*x)^3 - 
1_{(2/3 > x > 1/3)}(8+2\sin(9*(x-1/3)\pi)).
\]

\hypertarget{appendix-c}{%
\subsection{Appendix C}\label{appendix-c}}

\begin{Schunk}
\begin{table}

\caption{\label{tab:unnamed-chunk-3}All convergence scores per package:algorithm}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{rlllrrrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Input parameter} & \multicolumn{4}{c}{Score} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-8}
Num & Input format & Maxit & Learn. rate & RMSE median & RMSE d51 & MAE & WAE\\
\midrule
1 & x \& y & 1000 & 0.01 & 24 & 8 & 26 & 19\\
2 & x \& y & 1000 & 0.01 & 22 & 28 & 17 & 26\\
3 & x \& y & 10000 & 0.1 & 37 & 24 & 42 & 31\\
4 & x \& y & 10000 & 0.1 & 33 & 14 & 36 & 27\\
5 & x \& y & 1000 & 0.01 & 27 & 26 & 28 & 23\\
\addlinespace
6 & x \& y & 1000 & 0.01 & 26 & 33 & 27 & 22\\
7 & x \& y & 1000 & 0.01 & 36 & 20 & 35 & 28\\
8 & x \& y & 1000 & 0.01 & 20 & 35 & 16 & 19\\
9 & x \& y & 1000 & 0.01 & 31 & 50 & 29 & 38\\
10 & x \& y & 1000 & - & 40 & 49 & 40 & 37\\
\addlinespace
11 & x \& y & 200 & - & 11 & 9 & 12 & 11\\
12 & x \& y & 200 & - & 28 & 48 & 23 & 39\\
13 & x \& y & 1000 & - & 56 & 56 & 54 & 56\\
14 & x \& y & 1000 & 0.01 & 54 & 60 & 52 & 58\\
15 & x \& y & 200 & - & 13 & 30 & 14 & 12\\
\addlinespace
16 & x \& y & 10000 & 0.4 & 41 & 1 & 37 & 44\\
17 & x \& y & 10000 & 0.8 & 57 & 2 & 57 & 53\\
18 & x \& y & 1000 & 0.8 & 52 & 3 & 53 & 51\\
19 & x \& y & 1000 & 0.8 & 45 & 4 & 47 & 50\\
20 & x \& y & 1000 & 0.8 & 18 & 38 & 24 & 17\\
\addlinespace
21 & x \& y & - & - & 59 & 55 & 59 & 59\\
22 & fmla \& data & - & - & 60 & 54 & 60 & 60\\
23 & x \& y & 200 & - & 15 & 33 & 15 & 15\\
24 & "y" \& data & 10000 & 0.01 & 7 & 7 & 8 & 8\\
25 & x \& y & 10000 & 0.1 & 48 & 27 & 51 & 41\\
\addlinespace
26 & x \& y & 10000 & 0.1 & 42 & 51 & 41 & 33\\
27 & x \& y & 10000 & 0.1 & 28 & 44 & 30 & 25\\
28 & x \& y & 10000 & 0.1 & 16 & 19 & 20 & 16\\
29 & x \& y & 10000 & 0.1 & 38 & 58 & 39 & 40\\
30 & x \& y & 10000 & 0.1 & 54 & 57 & 55 & 54\\
\addlinespace
31 & x \& y & 10000 & 0.1 & 44 & 47 & 43 & 43\\
32 & fmla \& data & 200 & - & 9 & 20 & 10 & 7\\
33 & full fmla \& data & 200 & - & 18 & 5 & 19 & 14\\
34 & x \& y & 200 & - & 10 & 17 & 9 & 10\\
35 & x \& y & 10000 & - & 46 & 46 & 43 & 47\\
\addlinespace
36 & fmla \& data & 100000 & 0.001 & 50 & 11 & 48 & 45\\
37 & fmla \& data & 100000 & - & 21 & 41 & 22 & 18\\
38 & fmla \& data & 100000 & - & 23 & 40 & 21 & 24\\
39 & fmla \& data & 100000 & - & 50 & 59 & 46 & 52\\
40 & fmla \& data & 100000 & - & 38 & 37 & 38 & 46\\
\addlinespace
41 & full fmla \& data & 200 & - & 3 & 16 & 3 & 6\\
42 & x \& y & 200 & - & 2 & 17 & 2 & 3\\
43 & x \& y & 200 & - & 14 & 22 & 7 & 35\\
44 & "y" \& data & 200 & - & 8 & 32 & 11 & 9\\
45 & fmla \& data & 200 & - & 1 & 6 & 1 & 1\\
\addlinespace
46 & x \& y & 10000 & 0.1 & 47 & 25 & 50 & 49\\
47 & x \& y & 1000 & - & 34 & 41 & 32 & 34\\
48 & x \& y & 1000 & - & 35 & 38 & 34 & 30\\
49 & x \& y & 1000 & - & 30 & 43 & 33 & 32\\
50 & x \& y & 10000 & - & 58 & 36 & 58 & 57\\
\addlinespace
51 & x \& y & 1000 & - & 24 & 51 & 25 & 29\\
52 & x \& y & 1000 & - & 16 & 22 & 18 & 19\\
53 & x \& y & 1000 & 0.1 & 32 & 30 & 31 & 36\\
54 & x \& y & 200 & - & 48 & 13 & 49 & 47\\
55 & fmla \& data & 200 & - & 5 & 15 & 6 & 2\\
\addlinespace
56 & x \& y & 200 & - & 4 & 11 & 4 & 5\\
57 & x \& y & 1000 & - & 6 & 10 & 5 & 4\\
58 & x \& y & 200 & - & 12 & 29 & 13 & 12\\
59 & x \& y & 10000 & - & 43 & 45 & 45 & 41\\
60 & x \& y & 1000 & - & 53 & 53 & 56 & 55\\
\bottomrule
\end{tabular}
\end{table}

\end{Schunk}

\hypertarget{appendix-d}{%
\subsection{Appendix D}\label{appendix-d}}

\begin{table}[htb!]
\begin{center}
\caption{\textbf{Review of Ommitted Packages}}
\scriptsize

\begin{tabular}{l l l l}
  \toprule
  No & Name (package)         & Category & Comment \\
  \midrule
  1  &\pkg{appnn}             & AP        & This package provides a feed forward neural network to predict\\
     &                        &           & the amyloidogenicity propensity of polypeptide sequences      \\
  2  &\pkg{autoencoder}       & AP        & This package provides a sparse autoencoder, an unsupervised   \\
     &                        &           & algorithm that learns useful features from the data its given \\
  3  &\pkg{BNN}               & RE*       & This package uses a feed forward neural network to perform    \\
     &                        &           & regression as provided in the examples, however, it is unclear\\      &                        &           & whether it fits the form of perceptron that is the scope of   \\
     &                        &           & our research. Moreover, it states that it is intended for     \\      &                        &           & variable selection. Although how exactly the package would be \\
     &                        &           & used to do so isn't accessible in the package, especially     \\
     &                        &           & considering the source code is based on .c code that users of \\
     &                        &           & R might not understand. It's performance is slow, which may   \\
     &                        &           & have to do with the 100.000 iterations it needs, although     \\
     &                        &           & quite accurate for simple datasets.                           \\
  4  &\pkg{Buddle}            & RE**      & (errors)\\
  5  &\pkg{cld2}              & 00        & \\
  6  &\pkg{cld3}              & AP        & \\
  7  &\pkg{condmixt}          & AP        & \\
  8  &\pkg{deep}              & CL        & \\
  9  &\pkg{DALEX2}            & 00        & removed keyword, included in 2019 \\
  10 &\pkg{DamiaNN}           & RE**      & (errors) exported functions, still doesn't work \\
  11 &\pkg{DChaos}            & ??        & removed keyword for some reason, need to check out! \\
  12 &\pkg{deepNN}            & RE**      & (errors) I/O weird, ragged vector array \\
  13 &\pkg{DNMF}              & AP        & \\
  14 &\pkg{evclass}           & CL        & \\
  15 &\pkg{gamlss.add}        & RE        & there is some code but dist not appropriate \\
  16 &\pkg{gcForest}          & 00        & \\
  17 &\pkg{GMDH}              & TS        & \\
  18 &\pkg{GMDH2}             & CL        & \\
  19 &\pkg{GMDHreg}           & RE*       & \\
  20 &\pkg{grnn}              & RE**      & \\
  21 &\pkg{hybridEnsemble}    & ??        & \\ 
  22 &\pkg{isingLenzMC}       & AP        & \\
  23 &\pkg{leabRa}            & ??        & \\      
  24 &\pkg{learNN}            & ??        & \\     
  25 &\pkg{LilRhino}          & AP        & \\
  26 &\pkg{neural}            & CL        & \\
  27 &\pkg{NeuralNetTools}    & UT        & tools for neural networks           \\
  28 &\pkg{NeuralSens}        & UT        & tools for neural networks           \\
  29 &\pkg{NlinTS}            & TS        & Time Series                         \\
  30 &\pkg{nnetpredint}       & UT        & confidence intervals for NN          \\
  31 &\pkg{nnfor}             & TS        & Times Series, uses neuralnet         \\
  32 &\pkg{nntrf}             & UT        & \\
  33 &\pkg{onnx}              &           & provides an open source format       \\
  34 &\pkg{OptimClassifier}   &           & choose classifier parameters, nnet   \\
  35 &\pkg{OSTSC}             &           & solving oversampling classification  \\
  36 &\pkg{passt}             &           & \\
  36 &\pkg{pnn}               &           & Probabilistic                        \\
  37 &\pkg{polyreg}           &           & polyregression as alternative to NN  \\
  38 &\pkg{predictoR}         &           & shiny interface, neuralnet           \\
  39 &\pkg{ProcData}          &           & \\
  40 &\pkg{QuantumOps}        &           & classifies MNIST, Schuld (2018), removed keyword, in 2019 \\
  41 &\pkg{quarrint}          &           & specified classifier for quarry data \\
  42 &\pkg{rasclass}          &           & classifier for raster images, nnet?  \\
  43 &\pkg{rcane}             &           & \\
  44 &\pkg{regressoR}         &           & a manual rich version of predictoR   \\
  45 &\pkg{rnn}               &           & Recurrent                            \\
  46 &\pkg{RTextTools}        &           & \\
  47 &\pkg{ruta}              &           & \\
  48 &\pkg{simpleNeural}      &           & \\
  49 &\pkg{softmaxreg}        &           & \\
  50 &\pkg{Sojourn.Data}      &           & sojourn Accelerometer methods, nnet? \\
  51 &\pkg{spnn}              &           & classifier, probabilistic            \\
  52 &\pkg{studyStrap}        &           & \\
  53 &\pkg{TeachNet}          &           & classifier, selfbuilt, slow          \\
  54 &\pkg{tensorflow}        &           & \\
  55 &\pkg{tfestimators}      &           & \\
  56 &\pkg{trackdem}          &           & classifier for particle tracking     \\
  57 &\pkg{TrafficBDE}        & RE*       & \\
  58 &\pkg{tsfgrnn}           &           & \\
  59 &\pkg{yap}               &           & \\
  60 &\pkg{yager}             & RE*       & \\
  61 &\pkg{zFactor}           & AP        & 'compressibility' of hydrocarbon gas \\
\end{tabular}
\end{center}
\end{table}

\hypertarget{appendix-1}{%
\subsection{Appendix ??}\label{appendix-1}}

\begin{Schunk}
\begin{Sinput}
library(NNbenchmark)
nrep <- 5       
odir <- tempdir()

library(nnet)
nnet.method <- "BFGS"
hyperParams.nnet <- function(...) {
    return (list(iter=200, trace=FALSE))
}
NNtrain.nnet <- function(x, y, dataxy, formula, neur, method, hyperParams, ...) {
    
    hyper_params <- do.call(hyperParams, list(...))
    
    NNreg <- nnet::nnet(x, y, size = neur, linout = TRUE, maxit = hyper_params$iter, trace=hyper_params$trace)
    return(NNreg)
}
NNpredict.nnet  <- function(object, x, ...) { predict(object, newdata=x) }
NNclose.nnet    <- function() {  if("package:nnet" %in% search())
                                detach("package:nnet", unload=TRUE) }
nnet.prepareZZ  <- list(xdmv = "d", ydmv = "v", zdm = "d", scale = TRUE)

res <- trainPredict_1pkg(4:5, pkgname = "nnet", pkgfun = "nnet", nnet.method,
  prepareZZ.arg = nnet.prepareZZ, nrep = nrep, doplot = TRUE,
  csvfile = FALSE, rdafile = FALSE, odir = odir, echo = FALSE)
\end{Sinput}

\includegraphics{2020-rticle_files/figure-latex/unnamed-chunk-4-1} 
\includegraphics{2020-rticle_files/figure-latex/unnamed-chunk-4-2} \end{Schunk}


\address{%
Salsabila Mahdi\\
Universitas Syiah Kuala\\
JL. Syech Abdurrauf No.3, Aceh 23111, Indonesia\\
}
\href{mailto:bila.mahdi@mhs.unsyiah.ac.id}{\nolinkurl{bila.mahdi@mhs.unsyiah.ac.id}}

\address{%
Akshaj Verma\\
Manipal Institute of Technology\\
Manipal, Karnataka, 576104, India\\
}
\href{mailto:akshajverma7@gmail.com}{\nolinkurl{akshajverma7@gmail.com}}

\address{%
Christophe Dutang\\
University Paris-Dauphine, University PSL, CNRS, CEREMADE\\
Place du Maréchal de Lattre de Tassigny, 75016 Paris, France\\
}
\href{mailto:dutang@ceremade.dauphine.fr}{\nolinkurl{dutang@ceremade.dauphine.fr}}

\address{%
Patrice Kiener\\
InModelia\\
5 rue Malebranche, 75005 Paris, France\\
}
\href{mailto:patrice.kiener@inmodelia.com}{\nolinkurl{patrice.kiener@inmodelia.com}}

\address{%
John C. Nash\\
Telfer School of Management, University of Ottawa\\
55 Laurier Avenue East, Ottawa, Ontario K1N 6N5 Canada\\
}
\href{mailto:nashjc@uottawa.ca}{\nolinkurl{nashjc@uottawa.ca}}

