\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
 
\usepackage[english]{babel}

\usepackage{graphicx, subcaption, xcolor}

\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm]{geometry}

%reference hypertexte
\usepackage[pagebackref=false, hyperfootnotes=false]{hyperref}

\usepackage{amsmath,amssymb,amsfonts,amsthm}

\usepackage{enumitem}

\renewcommand{\vec}{\bm}
\newcommand{\R}{\mathbb R}
\newcommand{\calL}{\mathcal L}


%text style
\newcommand{\pkg}{\textbf}
\newcommand{\sigle}{\textsc}
\newcommand{\code}{\texttt}
\newcommand{\soft}{\textsf}
\newcommand{\expo}{\textsuperscript}
\newcommand{\boldit}[1]{ \textbf{\textit{#1}} }


\newcommand{\orange}[1]{{\color{orange}#1}}
\newcommand{\cyan}[1]{{\color{cyan}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}

\title{Revision of our submission 2021-19 entitled\\ \textsl{A Review of R Neural Network Packages (with NNbenchmark): Accuracy and Ease of Use}}

\author{Salsabila Mahdi, Akshaj Verma, Christophe Dutang, Patrice Kiener and John C. Nash}


\begin{document}

\maketitle

Dear Editor-in-Chief,\\

\noindent We are pleased to propose a revised version of the manuscript, 
\textsl{A Review of R Neural Network Packages (with NNbenchmark): Accuracy and Ease of Use} to R journal. 
We are grateful for the interesting and relevant comments of the referees.

\medskip
Below, we detail our responses to points raised by Referee \#1 in Section \ref{referee1},
by Referee \#2 in Section \ref{referee2} and by Referee \#3 in Section \ref{referee3}.
\medskip

In the pdf, changes for Referee  \#1 are put in \red{red}, for Referee \#2 in \blue{blue} and
for Referee  \#3 in \orange{orange}.  Other changes are in \cyan{cyan}.
Hypertext references are now used for tables and figures to ease the reading of the paper in pdf.

Yours sincerely
\bigskip


Christophe Dutang\\
the corresponding author

 
\newpage
\section{Referee \#1}\label{referee1}

In the pdf, changes for Referee  \#1 are put in \red{red}.  

\begin{enumerate}


\item \textit{Comments on Rubric 1: Utilities in R to deal with NN; 
a. predict function exists = 1 star;
 b. scaling capabilities exist = 1 star}
 
\textit{Some packages tested received 0 star for scaling, but do support scaling through integration with other packages, like recipes, that provide frameworks for data preprocessing.  In general, recipes could be used for scaling with any of the packages, and the packages do not necessarily deserve credit for the existence of recipes.  However, some are specifically designed to integrate with recipes (or other preprocessing packages) by supporting model fitting calls of the following general form.
}
\begin{verbatim}
library(recipes)
rec <- recipe(formula, data) %>% step_normalize(all_predictors())
fit(rec, ...)
\end{verbatim}
\textit{In essence, recipes is a part of their interfaces.  Recipes integration may require a bit more coding than built-in scaling, but does enable scaling and has the added advantage of enabling other types of preprocessing steps.  A case could be made that capabilities are greater with the latter.  Thus, integration with other data preprocessing packages warrants credit in the rating.  To ensure that scaling capabilities are accurately characterized in the ratings, consider contacting the package maintainers to ask about their software is support for scaling.
}

\textbf{We update the text to explain exactly how the utility rating was computed and also explain
that many \soft{R} packages provide preprocessing functions which can be used before the neural network
fitting process.
The \code{RWsearch} package lists 67 packages on CRAN to perform data preprocessing.}
\begin{verbatim}
> library(RWsearch)
> crandb_down()
> s_crandb("preprocessing", "data",
+          select="TD", mode="and")
 [1] "bdpar"                "benthos"              "biclust"             
 [4] "binst"                "bulletcp"             "ChIPtest"            
 [7] "CITAN"                "clickR"               "cobalt"              
[10] "dataprep"             "discretization"       "ebal"                
...    
[49] "rdwplus"              "recipes"              "RespirAnalyzer"      
[52] "RGCxGC"               "rminer"               "RobLoxBioC"          
[55] "shinyrecipes"         "sstModel"             "TDMR"                
[58] "torchaudio"           "torchvision"          "tosca"               
[61] "TSrepr"               "tsrobprep"            "vimpclust"           
[64] "VWPre"                "waves"                "wiseR"               
[67] "wvtool"  
\end{verbatim}
\textbf{In the utility rating, we could have give stars for packages providing generic functions such as \code{print},
\code{plot}, \code{summary}, as it is supposed the use of S3 objects. 
We do not contact the 25 package maintainers providing the 60 algorithms, as the description, the documentation 
(manual, vignettes,\dots) should be sufficiently clear and precise so that users find the \code{predict} function
or the \code{scale} function without contacting the maintainer.
}

\item \textit{
Comments on Rubric 3: User-friendly call to fit a NN\\
       a. simple one-line call or a single function = 2 star\\
       b. multiple-lines call to a single function = 1 star\\
       c. multiple-lines call to many function = 0 star}

       
 \textit{A 2 star rating for \code{nlsr} seems unwarranted because its usage is more involved than other packages, like \code{nnet}, given the same ranking.  In order to fit a NN with the \code{nlsr::nlxb} function, the mathematical form of the NN equation must be supplied as a formula.  This requires more complete knowledge of the underlying NN than, say, \code{nnet} which requires specification of a formula only in terms of the response and a linear combination of the predictor variables.  The operators (and functions) that appear in formulas are analogous to function calls.  Accordingly, the formula in \code{nlsr} consists of more calls than the formula in \code{nnet}.  Additionally, the testing code call to \code{nlsr::nlxb} is two lines and includes a call to the list function.  Therefore, \code{nlxb} calls appear to be multiple-lines call to many function, which is more in line with 0 stars than the 2 star rating given.}

\textbf{Indeed, there are some inconsistencies with this rating and in particular \code{nlsr} was badly rated.}

\item \textit{Rating packages on their ease-of-use is a worthwhile endeavor.  However, some of the subjective components of the current rubric and its application detract from the ratings as a measure of ease-of-use.  Consider the four examples below intended to illustrate how model fitting syntaxes might vary across different packages or users.
}

\begin{verbatim}
# Example Syntax 1
fit(formula, data, model = "model_name", param1 = value1, param2 = value2)
# Example Syntax 2
model <- model_name(param1 = value1, param2 = value2)
fit(formula, data, model = model)
# Example Syntax 3
fit(formula, data, model = model_name(param1 = value1, param2 = value2))
# Example Syntax 4
fit(formula, data, params = list(param1 = value1, param2 = value2))
\end{verbatim}

\textit{Syntax 1 is similar to that used in \code{caret (fit = train, model = method)}, Syntaxes 2 and 3 to \code{MachineShop}, and Syntax 4 to \code{nlsr (fit = nlxb, params = control)}.  
The number of stars awarded seems to differ across these types of syntaxes, and the reason for the differences is unclear for a number of reasons.  
First, according to the current rubric, Syntax 3 might be considered less user-friendly than Syntax 1 because \code{model\_name} 
appears as a second function call instead of as a character value.  
Staring the two differently would seem arbitrary given that they require the same knowledge and similar specification of the model 
and parameter names.  Second, Syntaxes 2 and 3 differ only in the user is choice to define the model on a line separate from the fit call.  
The rubric to award more stars for fewer lines seems arbitrary in the case of these two syntaxes given that the number of lines is 
a user choice and not a package requirement.  
Third, Syntax 2, which is equivalent to Syntax 3, received a different number of stars than Syntax 4 even though both can be written in one line and consist of two function calls.
}

\textit{
In summary, Rubric 3 has several subjective components.  The term *simple* in item (a) is non-specific.  
The distinctions between single and multiple lines of code can be arbitrary in cases where the number of lines is a product of user choices.  
Single lines can often be written as multiple lines, and multiple lines as a single one (particularly with use of the \%$>$\% pipe operator).  
Additionally, there appear to be some inconsistencies in counting function calls (counting of \code{model\_name}  but not list) 
and arbitrariness in not counting function names supplied as argument values. 
Rubric 3 should be revised with more objective rules that are clearly described and consistently applied.  
For example, it makes sense to award fewer stars to packages that require a formula specification for the full NN equation 
(e.g. \code{nlsr::nlxb}) and more stars to those that only require a linear combination of the predictors in the formula or that 
accept x and y data structures directly.  
More thought should be given to the awarding of stars based on number of lines or function calls.
}

\textbf{We rewrite the rating methodology more objectively as follows
\begin{enumerate}
    \item[a.] a single function with arguments passed as character, numeric, boolean or formula; and data as a data.frame or a matrix = 2 stars
    \item[b.] a single function with model specification passed as a list or via a dedicated function; or data converted in a dedicated S3/S4 object = 1 star
    \item[c.] multiple functions for initializing-converting-fitting = 0 star
\end{enumerate}
Hence the Call rating has been updated accordingly.
We also split Table 2 into two different tables : the new Table 2 contains only the RMSE score and the time per \code{package:algorithm}
while the new Table 3 contains ease of use scores per package where additional columns have been added to clarify how the fitting function can
be called.}

\end{enumerate}

\newpage
\section{Referee \#2}\label{referee2}

In the pdf, changes for Referee \#2 are in \blue{blue}.  

\subsection*{Major issues}

\begin{enumerate}
\item \textit{The manuscript is not set appropriately in relation to the literature, e.g., the introduction has no references at all. Moreover, I think I found very few references, which are not R packages or datasets, and most of these are from the 1990s.}

\textbf{Appropriate literature review added in the introduction as well 
as new references to recent books added in Section 2.
}


\item \textit{The restriction on single hidden-layer regression networks with tanh() activation does not reflect state-of-the-art neural networks used in practice. Although such a restriction is understandable from the author’s practical perspective, it renders the comparison to be not very useful for applications of neural networks.}

\textbf{We update the formal description of a neural network with a general activation function $f$.}

\item \textit{The description of neural networks on pages 2-3 is not up to date. For example, it is very common to use activation functions such as ReLU, which is not differentiable at 0 and not bounded.}

\textbf{We add a new comment on that point}


\item \textit{For a benchmark paper, the number of included datasets is quite low, and the chosen datasets are very simple. Given that collections such as OpenML100 are readily available, a benchmark should be based on more and more complex data.}

\textbf{We thank the referee for pointing out the OpenML100 database, now replaced by OpenML-CC18 database, 
which propose many datasets (3433) of which 1589 are for regression purposes.
But we do not need as many packages because our 12 datasets identify a large variety of situations where 
numerical difficulties occur for any training algorithm, such as nearly singular Jacobian, zero residual or divide by zero
situations.
Furthermore, the larger dataset \code{bWoodN1} has been tested on TOP10 packages (not only TOP5)
We observe that only 2 packages (\code{CaDENCE} and \code{traineR}) 
have a RMSE minimum close to the RMSE of TOP5 packages. This suggests packages outside TOP10
will have a bad performance on \code{bWoodN1}. 
Finally, it will be a tremendous task to perform an entirely new benchmark over 1589 datasets
but we mention it for future work in the conclusion.}



\item \textit{It is unclear how hyperparameters such as the learning rate were tuned. In a benchmark paper, hyperparameter tuning is essential to draw valid conclusions beyond the defaults.}

\textbf{Due to the high number of package:algorithm, we did not fully attempt to train all the hyperparameters of all packages. 
We justify this choice because of the tendency of users to use default hyperparameters. Moreover, not all packages offer the same hyperparameters. 
Based on GSoc 2019, we considered \code{maxit} to be the most important parameters, as some packages performed rather differently despite 
having the same algorithm, sometimes even the same code for the algorithm, because they had different \code{maxit} values. }

\textbf{It was harder to find a learning rate value to harmonize towards. 
But even the algorithms that were supposedly the same, at least in name, seemed to need different learning rates for the same level of convergence. 
We made a decision after a reasonably-time-consuming grid search between which \code{maxit} and what learning rate. }

\textbf{As for the top 10 packages, especially the ones that basically stem form \code{nnet}'s BFGS, we tried to make sure all the other hyperparameters 
were harmonized as well. 
For those we didn't harmonize completely, such as \code{rminer} which had a difference of maximum allowable weights than the default \code{nnet}, 
we mentioned them in the paper. 
Most of the trial \& error was for first-order algorithms while the focus for second-order algorithms the \code{maxit} is uniquely set to 200. }
\textbf{We add a comment on this in the paper.}


\item \textit{The framing of this paper is not precise. The introduction states a still scientifically interesting hypothesis ("we hypothesize that these second-order algorithms would perform better than the first-order methods for datasets that fit in memory"), but according to the title, abstract, and evaluation criteria, they want to present a more general comparison of R packages.}

\textbf{We clarify this point by adding a paragraph in the introduction.}



\item \textit{Despite a very nicely designed and organized website (NNbenchmarkWeb), the documentation on GitHub is quite messy and without clear guidance on how to use the package.}

\textbf{An update of the project website has been carried out to reorganized notebooks, results.
The website is now hosted at \url{https://theairbend3r.github.io/NNbenchmarkWeb/index.html}.}



\item \textit{Comparing the performance only on the training data is rather unrelated to common applications and does not indicate how well the network generalizes.}

\textbf{We only use a training set because the purpose of our study is to verify the ability to reach good minima, i.e., rating optimization
methods used in NN packages. This requirement is satisfied by using only a train set.}


\end{enumerate}

\subsection*{Text-specific issues}

\begin{enumerate}[resume]
\item \textit{Figure 1: The figure caption does not describe the figure sufficiently well.}

\textbf{We clarify the NN $a$-$b$-$c$ notation and also add words in the captions}

\item \textit{p. 2: It is unclear how Figure 1c relates to normalized inputs.}

\textbf{We clarify this point in the text}
\red{Patrice : could you explain (mathematically) in what this normalization consists? Does it mean to use the distribution
function and the quantile function of a Gaussian distribution $\mathcal N(0,0.1)$. We need to state in full mathematic
terms what happen to the equation.}

\item \textit{p. 2: Do not write large math equations inline. Use the display mode instead.}

\textbf{Done}

\end{enumerate}

\subsection*{Minor issues}

\begin{enumerate}[resume]
\item \textit{p. 1: "For regression and classification, the term multilayer perceptron is used interchangeably." - Multilayer perceptrons are a particular type of neural networks based on feedforward neural networks.}

\textbf{TODO}
\red{Patrice?}

\item \textit{The term "Neural networks" is spelled in different ways: "Neural Network", "neural network", "neural-network", etc.}

\textbf{Done}

 
\item \textit{Sometimes quite drastic or fuzzy wording: p. 1 "[…] poor packages are implemented on CRAN.", p. 2 "[…] perform better than [...]" or p. 2 "[…] we believe it is helpful to have relatively large gradients […]"}

\textbf{Done}
\red{John, could you validate?}

\end{enumerate}

\subsection*{Other remarks}

\begin{enumerate}[resume]
\item \textit{Regardless of the major issues in this paper mentioned above, the basic idea of reviewing existing R neural network packages is highly relevant in current research and should be pursued further.
}
\textbf{We are grateful to the referee for this comment.}
\end{enumerate}



%\newpage
\section{Referee \#3}\label{referee3}

In the pdf, changes for Referee  \#3 are in \orange{orange}.  

The minor changes are suggestions of aspects that can improve the manuscript:
\begin{enumerate}
\item \textit{the writing could be improved in several cases, such as: removal of "oral" language: "there’s" $->$ "there is";}

\textbf{I Ctrl+Find 's in the paper.}
\red{John, could you validate?}

\item \textit{change of title "Neural Networks: The Perceptron" $->$ "Multilayer Perceptron with a Single Hidden Layer";\\
 in Table 1 "nb." $->$ size.}
 
 \textbf{Done: I do not agree with Table 1 : parameter size is not correct to me}
\red{John}

\item \textit{the NN acronym in Fig.1 is not detailed, nor the notation 1-3-1 is explained.\\ 
Fig1 c) is not a single hidden layer networks.}

\textbf{We formally introduce the NN $a$-$b$-$c$ notation.}


\item \textit{the examples in page 2 assume the tanh and atan activation functions, why not use $f()$, where f is the activation function, which can include the logistic function?}

\textbf{Yes, we generalize the NN formulation in order to have a single equation.}


\item \textit{in Phase 2, it could be explained how the NNbenchmark was used, with one example (and its characteristics).}

\textbf{We add comments on this part, but the full example is left in Appendix C.}


\item \textit{some figures, such as Fig2, should have a x-axis with numbers and labels.}

\textbf{Done}


\item \textit{it should be explained what is a first order and second order algorithm.}

\textbf{Done, we add two sentences in the introduction.}


\end{enumerate}


\end{document}