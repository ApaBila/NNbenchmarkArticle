% !TeX root = RJwrapper.tex
\title{=\textgreater{} to be discussed (Option - Validating Neural Network
Packages in R with \pkg{NNbenchmark})}
\author{by Author One, Author Two}

\maketitle

\abstract{%
An abstract of less than 150 words. Possible format: 1) the overall
purpose of the study and the research problem(s) you investigated; 2)
the basic design of the study; 3) major findings or trends found as a
result of your analysis; and, 4) a brief summary of your interpretations
and conclusions.
}


\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

R Statistical Software, as any opensource platform, has relied on its
contributors to keep it up to date with the latest developments. Neural
networks is one of these advancements pertaining to a particular class
of models in machine learning. Formerly a theory with not much practical
implementation due to the complicated calculations of its algorithms,
neural networks are now one of the most actively explored in its field.
The reason it used to be so demanding is because neural networks use the
gradient of the cost to step into the direction of the optimal solution.
It updates the parameters of neurons accordingly to make better
predictions until a certain number of iterations. Manually computing
gradients for large, or even small datasets, was simply too hard. With
the help of computers, neural network algorithms can be calculated
through a few lines of code.

However, this capability of computers is often not used to the greatest
extent. Instead of calculating first order derivatives, and moving
incrementally forward by a predetermined learning rate, it is faster to
adjust the size of each step according to its curvature. Curvature can
be learned about through calculating the second order derivative. Some
algorithms take this further by numerical methods. The Hessian, a matrix
of the second derivatives, is approximated. Such methods are called
Quasi-Newton. Broyden-Fletcher-Goldfarb-Shanno (BFGS) is a popular
example of an algorithm from this class. We believed that these second
order algorithms are also better than first order algorithms in terms of
finding the optimal solution. Regardless of our belief, it was important
to conduct a thorough examination to assess the quality of these
training algorithms in R. There is much code, but barely enough
comparison. At the very least our research may be used as a framework
for future research.

In R, code for new neural network algorithms, better code for existing
ones, or code to import algorithms from other platforms/languages is
submitted and shared in the form of packages. As of August the 25th,
2019, there were 77 packages in CRAN with this keyword. Those are:\\
\CRANpkg{AMORE} \citep{R-AMORE}, \CRANpkg{ANN2} \citep{R-ANN2},
\CRANpkg{appnn} \citep{R-appnn}, \CRANpkg{autoencoder}
\citep{R-autoencoder}, \CRANpkg{automl} \citep{R-automl}, \CRANpkg{BNN}
\citep{R-BNN}, \CRANpkg{brnn} \citep{R-brnn}, \CRANpkg{Buddle}
\citep{R-Buddle}, \CRANpkg{CaDENCE} \citep{R-CaDENCE}, \CRANpkg{cld2}
\citep{R-cld2}, \CRANpkg{cld3} \citep{R-cld3}, \CRANpkg{condmixt}
\citep{R-condmixt}, \CRANpkg{DALEX2} \citep{R-DALEX2}, \CRANpkg{DamiaNN}
\citep{R-DamiaNN}, \CRANpkg{DChaos} \citep{R-DChaos}, \CRANpkg{deepnet}
\citep{R-deepnet}, \CRANpkg{deepNN} \citep{R-deepNN}, \CRANpkg{DNMF}
\citep{R-DNMF}, \CRANpkg{elmNNRcpp} \citep{R-elmNNRcpp}, \CRANpkg{ELMR}
\citep{R-ELMR}, \CRANpkg{EnsembleBase} \citep{R-EnsembleBase},
\CRANpkg{evclass} \citep{R-evclass}, \CRANpkg{gamlss.add}
\citep{R-gamlss.add}, \CRANpkg{gcForest} \citep{R-gcForest},
\CRANpkg{GMDH} \citep{R-GMDH}, \CRANpkg{GMDH2} \citep{R-GMDH2},
\CRANpkg{GMDHreg} \citep{R-GMDHreg}, \CRANpkg{grnn} \citep{R-grnn},
\CRANpkg{h2o} \citep{R-h2o}, \CRANpkg{hybridEnsemble}
\citep{R-hybridEnsemble}, \CRANpkg{isingLenzMC} \citep{R-isingLenzMC},
\CRANpkg{keras} \citep{R-keras}, \CRANpkg{kerasformula}
\citep{R-kerasformula}, \CRANpkg{kerasR} \citep{R-kerasR},
\CRANpkg{leabRa} \citep{R-leabRa}, \CRANpkg{learNN} \citep{R-learNN},
\CRANpkg{LilRhino} \citep{R-LilRhino}, \CRANpkg{minpack.lm}
\citep{R-minpack.lm}, \CRANpkg{MachineShop} \citep{R-MachineShop},
\CRANpkg{monmlp} \citep{R-monmlp}, \CRANpkg{neural} \citep{R-neural},
\CRANpkg{neuralnet} \citep{R-neuralnet}, \CRANpkg{NeuralNetTools}
\citep{R-NeuralNetTools}, \CRANpkg{NeuralSens} \citep{R-NeuralSens},
\CRANpkg{NlinTS} \citep{R-NlinTS}, \CRANpkg{nlsr} \citep{R-nlsr},
\CRANpkg{nnet} \citep{R-nnet}, \CRANpkg{nnetpredint}
\citep{R-nnetpredint}, \CRANpkg{nnfor} \citep{R-nnfor}, \CRANpkg{onnx}
\citep{R-onnx}, \CRANpkg{OptimClassifier} \citep{R-OptimClassifier},
\CRANpkg{OSTSC} \citep{R-OSTSC}, \CRANpkg{pnn} \citep{R-pnn},
\CRANpkg{polyreg} \citep{R-polyreg}, \CRANpkg{predictoR}
\citep{R-predictoR}, \CRANpkg{qrnn} \citep{R-qrnn}, \CRANpkg{QuantumOps}
\citep{R-QuantumOps}, \CRANpkg{quarrint} \citep{R-quarrint},
\CRANpkg{radiant.model} \citep{R-radiant.model}, \CRANpkg{rasclass}
\citep{R-rasclass}, \CRANpkg{rcane} \citep{R-rcane}, \CRANpkg{regressoR}
\citep{R-regressoR}, \CRANpkg{rminer} \citep{R-rminer}, \CRANpkg{rnn}
\citep{R-rnn}, \CRANpkg{RSNNS} \citep{R-RSNNS}, \CRANpkg{ruta}
\citep{R-ruta}, \CRANpkg{simpleNeural} \citep{R-simpleNeural},
\CRANpkg{snnR} \citep{R-snnR}, \CRANpkg{softmaxreg}
\citep{R-softmaxreg}, \CRANpkg{Sojourn.Data} \citep{R-Sojourn.Data},
\CRANpkg{spnn} \citep{R-spnn}, \CRANpkg{TeachNet} \citep{R-TeachNet},
\CRANpkg{tensorflow} \citep{R-tensorflow}, \CRANpkg{tfestimators}
\citep{R-tfestimators}, \CRANpkg{trackdem} \citep{R-trackdem},
\CRANpkg{TrafficBDE} \citep{R-TrafficBDE}, \CRANpkg{tsensembler}
\citep{R-tsensembler}, \CRANpkg{validann} \citep{R-validann},
\CRANpkg{zFactor} \citep{R-zFactor}.

In particular, packages that provide neural network of the perceptron
type (one input layer, one normalized layer, one hidden layer with a
nonlinear activation function that is usually tanh(), one normalized
layer, one output output layer) for regression purpose (i.e.~NN(X1, .,
Xn) = E{[}Y{]} were the focus of this research.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

Our research process can be divided into 3 phases: preparation,
exploration, and results. In practice, these three phases overlapped as
in any other research.

\hypertarget{phase-1---preparation}{%
\subsection{Phase 1 - Preparation}\label{phase-1---preparation}}

\textbf{Datasets}\\
All the datasets used were nonlinear. Linear data sets are more simple
and can even be solved with OLS regression. This is why we believe to
truly set apart the ability of neural networks we needed to go beyond
linear regression. Varying difficulties between data sets helped to
classify further package's algorithms accuracy. One site was used for 3
of the multivariate data sets. Sonja Surjanovic and Derek Bingham of
Simon Fraser University created this resourceful website to evaluate the
design and analysis of computer models. Links to each dataset and their
level of difficulty:\\
- \url{http://www.sfu.ca/~ssurjano/fried.html} (Friedman - average)\\
- \url{http://www.sfu.ca/~ssurjano/detpep10curv.html} (Dette - medium)\\
- \url{http://www.sfu.ca/~ssurjano/ishigami.html} (Ishigami - high)\\
The other multivariate dataset, Ref153, was taken from \ldots{} 3 of the
univariate datasets were taken from NIST at
\url{https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml}. Gauss1
and Gauss2 have a low level of difficulty to solve. Gauss3 is average.
Dmod1, Dmod2 Dreyfus1, Dreyfus2 NeuroOne from \ldots{}\\
\textbf{Packages}\\
Searching manually through the thousands of packages title or the
package description of thousands packages would have taken a long time.
With \CRANpkg{RWsearch} \citep{R-RWsearch} we were able to automate the
process. All packages that have ``neural network'' as a keyword in the
package title or in the package description, that are mentioned in the
introduction, were included.

\hypertarget{phase-2---exploration-of-each-package-and-development-of-template}{%
\subsection{Phase 2 - Exploration of each package and development of
template}\label{phase-2---exploration-of-each-package-and-development-of-template}}

\textbf{Exploration}\\
However, not all packages that had the keyword were fit for the scope of
our research. Some didn't have any functions to make neural networks.
They were simply meta-packages. Others were not regression neural
networks of the perceptron type or were only made for specific purposes.
We learned this through reading documentation and trying out example
code. \textbf{Template}\\
As we inspected the packages, we developed a template for benchmarking.
This template's structure is as follows:\\
(1) Set up of environment - loading packages, setting directory,
options;\\
(2) Summary of datasets;\\
(3) A loop over datasets which contained (a) setting parameters for a
specific dataset (b) selecting benchmark options (c) the training of a
neural network with a package's tuned functions (d) calculation of RMSE
and MAE (e) plot each training over one initial graph, then plot the
best result (f) adding results to the appropriate *.csv file and (g)
clearing up environment for next loop; and\\
(4) Clearing up the environment for the next package. (5) It is optional
to print warnings.\\
This process was made easier with tools from the NNbenchmark package. It
is not on CRAN yet and can instead be found at
\url{https://github.com/pkR-pkR/NNbenchmark}. Our templates for each
package can be found in the companion repository,
\url{https://github.com/pkR-pkR/NNbenchmarkTemplates}.

\hypertarget{phase-3---collection-of-and-analysis-of-results}{%
\subsection{Phase 3 - Collection of and analysis of
results}\label{phase-3---collection-of-and-analysis-of-results}}

\textbf{Collection} After the templates were finished, the packages were
looped on all datasets. Results were collected in the directory of the
templates repository. \textbf{Analysis} We manipulated the results with
the following rating scheme:

\hypertarget{results}{%
\section{Results}\label{results}}

The following is the final table of results. Further components of the
rating can be found at the end. \newpage

\begin{center}
\textbf{Table 1: Results of Benchmarking}

\begin{tabular}{l l l l}
  \toprule
  No & Name (package::algorithm)        & Rating & Comment \\
  \midrule
  1  &\pkg{AMORE}::train.ADAPTgd        & *      & \\
     &\pkg{AMORE}::train.ADAPTgdwm      & *      & \\
     &\pkg{AMORE}::train.BATCHgd        & rating & \\ 
     &\pkg{AMORE}::train.BATCHgdwm      & rating & \\
  2  &\pkg{automl}                      & rating & \\
  3  &\pkg{ANN2}::neuralnetwork.sgd     & rating & \\
     &\pkg{ANN2}::neuralnetwork.adam    & rating & \\
     &\pkg{ANN2}::neuralnetwork.rmsprop & rating & \\
  4  &\pkg{brnn}                        & ***    & \\
  5  &\pkg{CaDENCE}                     & rating & \\
  6  &\pkg{deepnet}::gradientdescent    & rating & \\
  7  &\pkg{elmNNRcpp}                   & rating & \\
  8  &\pkg{ELMR}                        & rating & \\
  9  &\pkg{h2o}::deeplearning           & rating & bad on univariate datasets            \\
  10 &\pkg{keras}                       & rating & high level API for tensorflow         \\
  11 &\pkg{kerasformula}                & rating & \\
  12 &\pkg{kerasR}                      & rating & \\
  13 &\pkg{minpack.lm}::nlsLM           & **     & Requires hand-made formulas + scaling \\
  14 &\pkg{MachineShop}::fit.NNetModel()& rating & uses nnet                            \\
  15 &\pkg{monmlp}::fit.BFGS            & rating & optimx, 200 iterations               \\
     &\pkg{monmlp}::fit.Nelder-Mead     & rating & optimx, 10000 iterations             \\
  16 &\pkg{neural}::mlptrain            & rating & more appropriate for classification  \\
  17 &\pkg{neuralnet}::backprop         & rating & not good, most NA's                  \\
     &\pkg{neuralnet}::rprop+           & rating & 100,000 iter                         \\
     &\pkg{neuralnet}::rprop-           & rating & above                                \\
     &\pkg{neuralnet}::sag              & rating & above                                \\
     &\pkg{neuralnet}::slr              & rating & above                                \\
  18 &\pkg{nlsr}::nlxb                  & **     & Requires hand-made formulas + scaling \\
  19 &\pkg{nnet}::nnet.BFGS             & rating & optim, 250 iterations, good          \\
  20 &\pkg{qrnn}::qrnn.fit              & rating & might be the best, 2000 iters?       \\
  21 &\pkg{radiant.model}::radiant.model& rating & uses nnet, 10000 iters               \\
  22 &\pkg{rcane}::rlm                  & rating?& linear, not appropriate              \\
  23 &\pkg{rminer}::fit                 & rating & uses nnet                            \\
  24 &\pkg{RSNNS}::BackpropBatch        & rating &                                      \\
     &\pkg{RSNNS}::BackpropChunk        & rating &                                      \\
     &\pkg{RSNNS}::BackpropMomentum     & rating &                                      \\
     &\pkg{RSNNS}::BackpropWeightDecay  & rating &                                      \\
     &\pkg{RSNNS}::Quickprop            & rating &                                      \\
     &\pkg{RSNNS}::Rprop                & rating &                                      \\
     &\pkg{RSNNS}::SCG                  & rating &                                      \\
     &\pkg{RSNNS}::Std-Backpropagation  & rating &                                      \\
  25 &\pkg{ruta}                        & rating &                                      \\
  26 &\pkg{simpleNeural}::sN.MLPtrain   & rating & NA's, works for Ref153-NeuroOne      \\ 
  27 &\pkg{snnR}                        & rating & classification                       \\
  28 &\pkg{softmaxreg}                  & rating & linear                               \\
  29 &\pkg{tensorflow}::AdadeltaOptmizer& rating &                                      \\
     &\pkg{tensorflow}::AdagradOptmizer & rating &                                      \\
     &\pkg{tensorflow}::AdamOptmizer    & rating &                                      \\
     &\pkg{tensorflow}::FtrlOptmizer    & rating &                                      \\
     &\pkg{tensorflow}::GradientDescent & rating &                                      \\
     &\pkg{tensorflow}::MomentumOptmizer& rating &                                      \\
  30 &\pkg{tfestimators}                & rating &                                      \\
  31 &\pkg{tsensembler}                 & rating & uses nnet, chooses "optimal" weights \\
  32 &\pkg{validann}::Nelder-Mead       & rating & 10000 iter                           \\
     &\pkg{validann}::BFGS              & rating & good                                 \\
     &\pkg{validann}::CG                & rating & good BUT slow, sys hangs             \\
     &\pkg{validann}::L-BFGS-B          & rating & good                                 \\
     &\pkg{validann}::SANN              & rating & 10000 iter                           \\  
     &\pkg{validann}::Brent             & rating & for one dimension, not looped        \\
\end{tabular}

\end{center}

\newpage
\begin{center}
\textbf{Table 2: Review of Ommitted Packages}

\begin{tabular}{l l l l}
  \toprule
  No & Name (package)            & Category & Comment \\
  \midrule
  1  &\pkg{appnn}                & -        & \\
  2  &\pkg{autoencoder}          & -        & \\     
  3  &\pkg{BNN}                  & -        & \\
  4  &\pkg{Buddle}               & -        & \\
  5  &\pkg{cld2}                 & -        & \\
  6  &\pkg{cld3}                 & -        & \\
  7  &\pkg{condmixt}             & -        & \\
  8  &\pkg{DALEX2}               & -        & \\
  9  &\pkg{DamiaNN}              & -        & \\
  10 &\pkg{DChaos}               & -        & \\
  11 &\pkg{deepNN}               & -        & \\
  12 &\pkg{DNMF}                 & -        & \\
  13 &\pkg{EnsembleBase}         & -        & \\
  14 &\pkg{evclass}              & -        & \\
  15 &\pkg{gamlss.add}           & -        & \\
  16 &\pkg{gcForest}             & -        & \\
  17 &\pkg{GMDH}                 & -        & \\
  18 &\pkg{GMDH2}                & -        & \\
  19 &\pkg{GMDHreg}              & -        & \\
  20 &\pkg{grnn}                 & -        & \\
  21 &\pkg{hybridEnsemble}       & -        & \\ 
  22 &\pkg{isingLenzMC}          & -        & \\
  23 &\pkg{leabRa}               & -        & \\      
  24 &\pkg{learNN}               & -        & \\     
  25 &\pkg{LilRhino}             & -        & \\
  26 &\pkg{NeuralNetTools}       & -        & tools for neural networks           \\
  27 &\pkg{NeuralSens}           & -        & tools for neural networks           \\
  28 &\pkg{NlinTS}               & NA       & Time Series                         \\
  29 &\pkg{nnetpredint}          & -        & confidence intervals for NN          \\
  30 &\pkg{nnfor}                & NA       & Times Series, uses neuralnet         \\
  31 &\pkg{onnx}                 & -        & provides an open source format       \\
  32 &\pkg{OptimClassifier}      & NA       & choose classifier parameters, nnet   \\
  33 &\pkg{OSTSC}                & -        & solving oversampling classification  \\
  34 &\pkg{pnn}                  & NA       & Probabilistic                        \\
  35 &\pkg{polyreg}              & -        & polyregression ALT to NN             \\
  36 &\pkg{predictoR}            & NA       & shiny interface, neuralnet           \\
  37 &\pkg{QuantumOps}           & NA       & classifies MNIST, Schuld (2018)      \\
  38 &\pkg{quarrint}             & NA       & specified classifier for quarry data \\
  39 &\pkg{rasclass}             & NA       & classifier for raster images, nnet?  \\
  40 &\pkg{regressoR}            & NA       & a manual rich version of predictoR   \\
  41 &\pkg{rnn}                  & NA       & Recurrent                            \\
  42 &\pkg{Sojourn.Data}         & NA       & sojourn Accelerometer methods, nnet? \\
  43 &\pkg{spnn}                 & NA       & classifier, probabilistic            \\
  44 &\pkg{TeachNet}             & NA       & classifier, selfbuilt, slow          \\
  45 &\pkg{trackdem}             & NA       & classifier for particle tracking     \\
  46 &\pkg{TrafficBDE}           & NA       & specific reg, predicting traffic     \\
  47 &\pkg{zFactor}              & NA       & 'compressibility' of hydrocarbon gas \\
\end{tabular}

\end{center}

Possible things to talk about: What is running under the packages?
Dependencies, base functions / optimization algorithms nnet, neuralnet
optim() \textbar{} \CRANpkg{optimx} \citep{R-optimx}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\hypertarget{future-work}{%
\subsection{Future work}\label{future-work}}

As the alogrithms for neural networks continue to grow, there will
always be more to validate. For current algorithms in R, our research
should be extended to encompass more types of neural networks and their
data formats (classifiers NN's, recurrent NN's, and so on). Different
rating schemes and tunings of package functions can also be tried out.

\hypertarget{acknowledgements}{%
\subsection{Acknowledgements}\label{acknowledgements}}

This work was possible due to the support of Google during Summer of
Code 2019.

\bibliography{RJreferences}

\newpage

\textbf{Supplementary Materials}

\begin{center}
\textbf{Table 3: Value of Components from Each Rating}
\begin{tabular}{l l l l l l l}
  \toprule
  No & Name (package::algorithm)        & Docs & UF & CQ & Time & Final   \\
  \midrule
  1  &\pkg{AMORE}::train.ADAPTgd        &      &    &    &      &         \\
     &\pkg{AMORE}::train.ADAPTgdwm      &      &    &    &      &         \\
     &\pkg{AMORE}::train.BATCHgd        &      &    &    &      &         \\ 
     &\pkg{AMORE}::train.BATCHgdwm      &      &    &    &      &         \\
  2  &\pkg{automl}                      &      &    &    &      &         \\
  3  &\pkg{ANN2}::neuralnetwork.sgd     &      &    &    &      &         \\
     &\pkg{ANN2}::neuralnetwork.adam    &      &    &    &      &         \\
     &\pkg{ANN2}::neuralnetwork.rmsprop &      &    &    &      &         \\
  4  &\pkg{brnn}                        &      &    &    &      &         \\
  5  &\pkg{CaDENCE}                     &      &    &    &      &         \\
  6  &\pkg{deepnet}::gradientdescent    &      &    &    &      &         \\
  7  &\pkg{elmNNRcpp}                   &      &    &    &      &         \\
  8  &\pkg{ELMR}                        &      &    &    &      &         \\
  9  &\pkg{h2o}::deeplearning           &      &    &    &      &         \\
  10 &\pkg{keras}                       &      &    &    &      &         \\
  11 &\pkg{kerasformula}                &      &    &    &      &         \\
  12 &\pkg{kerasR}                      &      &    &    &      &         \\
  13 &\pkg{minpack.lm}::nlsLM           &      &    &    &      &         \\
  14 &\pkg{MachineShop}::fit.NNetModel()&      &    &    &      &         \\
  15 &\pkg{monmlp}::fit.BFGS            &      &    &    &      &         \\
     &\pkg{monmlp}::fit.Nelder-Mead     &      &    &    &      &         \\
  16 &\pkg{neural}::mlptrain            &      &    &    &      &         \\
  17 &\pkg{neuralnet}::backprop         &      &    &    &      &         \\
     &\pkg{neuralnet}::rprop+           &      &    &    &      &         \\
     &\pkg{neuralnet}::rprop-           &      &    &    &      &         \\
     &\pkg{neuralnet}::sag              &      &    &    &      &         \\
     &\pkg{neuralnet}::slr              &      &    &    &      &         \\
  18 &\pkg{nlsr}::nlxb                  &      &    &    &      &         \\
  19 &\pkg{nnet}::nnet.BFGS             &      &    &    &      &         \\
  20 &\pkg{qrnn}::qrnn.fit              &      &    &    &      &         \\
  21 &\pkg{radiant.model}::radiant.model&      &    &    &      &         \\
  22 &\pkg{rcane}::rlm                  &      &    &    &      &         \\
  23 &\pkg{rminer}::fit                 &      &    &    &      &         \\
  24 &\pkg{RSNNS}::BackpropBatch        &      &    &    &      &         \\
     &\pkg{RSNNS}::BackpropChunk        &      &    &    &      &         \\
     &\pkg{RSNNS}::BackpropMomentum     &      &    &    &      &         \\
     &\pkg{RSNNS}::BackpropWeightDecay  &      &    &    &      &         \\
     &\pkg{RSNNS}::Quickprop            &      &    &    &      &         \\
     &\pkg{RSNNS}::Rprop                &      &    &    &      &         \\
     &\pkg{RSNNS}::SCG                  &      &    &    &      &         \\
     &\pkg{RSNNS}::Std-Backpropagation  &      &    &    &      &         \\
  25 &\pkg{ruta}                        &      &    &    &      &         \\
  26 &\pkg{simpleNeural}::sN.MLPtrain   &      &    &    &      &         \\ 
  27 &\pkg{snnR}                        &      &    &    &      &         \\
  28 &\pkg{softmaxreg}                  &      &    &    &      &         \\
  29 &\pkg{tensorflow}::AdadeltaOptmizer&      &    &    &      &         \\
     &\pkg{tensorflow}::AdagradOptmizer &      &    &    &      &         \\
     &\pkg{tensorflow}::AdamOptmizer    &      &    &    &      &         \\
     &\pkg{tensorflow}::FtrlOptmizer    &      &    &    &      &         \\
     &\pkg{tensorflow}::GradientDescent &      &    &    &      &         \\
     &\pkg{tensorflow}::MomentumOptmizer&      &    &    &      &         \\
  30 &\pkg{tfestimators}                &      &    &    &      &         \\
  31 &\pkg{tsensembler}                 &      &    &    &      &         \\
  32 &\pkg{validann}::Nelder-Mead       &      &    &    &      &         \\
     &\pkg{validann}::BFGS              &      &    &    &      &         \\
     &\pkg{validann}::CG                &      &    &    &      &         \\
     &\pkg{validann}::L-BFGS-B          &      &    &    &      &         \\
     &\pkg{validann}::SANN              &      &    &    &      &         \\  
     &\pkg{validann}::Brent             &      &    &    &      &         \\
  \end{tabular}
\end{center}

note: Documentation = Docs, Utility Functions = UF, Convergence Quality
= CQ, Convergence Time = Time, Final Rating = Final.


\address{%
Author One\\
\\
line 1\\ line 2\\
}


\address{%
Author Two\\
Affiliation\\
line 1\\ line 2\\
}


