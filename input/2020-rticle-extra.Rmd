---
title: Supplementary Materials for a Review of R Neural Network Packages (with NNbenchmark)$:$
  Accuracy and Ease of Use
author:
- Salsabila Mahdi^[Universitas Syiah Kuala, Indonesia]
- Akshaj Verma^[Manipal Institute of Technology, India]
- Christophe Dutang^[Universit√© Paris-Dauphine, University PSL, France]
- Patrice Kiener^[InModelia, France]
- John C. Nash^[Telfer School of Management, University of Ottawa, Canada]
preamble: 
  includes: 
    \usepackage{float}
    \usepackage{multirow}
    \usepackage{subcaption}
    \usepackage{longtable}
    \newcommand{\code}{\texttt}
    \newcommand{\pkg}{\textbf}
    \newcommand{\soft}{\textsf}
output:
  pdf_document: 
    toc: true
    number_sections: true
---

<!-- if we want html output, we may use https://rstudio.github.io/distill/ package -->


```{r, echo=FALSE}
library(knitr)
library(kableExtra)
library(NNbenchmark)
```



<!-- Table final result -->

```{r echo=FALSE, message=FALSE}
options(knitr.kable.NA = '')

if(file.exists("./tables/Table1.csv")) {  
  Table1 <- read.table("./tables/Table1-final.csv", sep = ";", dec=".",
                       header=TRUE)
  Table1 <- Table1[Table1[,2] != "", ]
}else 
   Table1 <- rbind(rep("missing",6))

colnames(Table1) <- c("Package", "Algorithm", "Time", "RMSE",
                    "Util", "Doc", "Call")

pkg.name <- Table1$Package[Table1$Package != ""]
idx.pkg.name <- (1+0:NROW(Table1))[Table1$Package != ""]
Table1$Doc <- floor(Table1$Doc)

#repeat value
Table1$Package <- rep(pkg.name, times=diff(idx.pkg.name))
Table1$Util <- rep(Table1$Util[!is.na(Table1$Util)], times=diff(idx.pkg.name))
Table1$Doc <- rep(Table1$Doc[!is.na(Table1$Doc)], times=diff(idx.pkg.name))
Table1$Call <- rep(Table1$Call[!is.na(Table1$Call)], times=diff(idx.pkg.name))

#convert to stars
cvt2star <- function(j)
{
  switch(as.character(j), "0" = " ", "1"="*", "2"="**", "3"="***", "4"="****")
}
Table1$Doc <- unlist(sapply(Table1$Doc, cvt2star))
Table1$Util <- unlist(sapply(Table1$Util, cvt2star))
Table1$Call <- unlist(sapply(Table1$Call, cvt2star))

#reorder columns
Table1 <- Table1[, c(1, 5:7, 2:4)]
#get the min RMSE by pkg
pkgRkbyRMSE <- sort(tapply(Table1$RMSE, Table1$Package, min))
#create new index
n <- NROW(Table1)
getorderbypkg <- function(p)
{
  idx <- (1:n)[Table1$Package == p]
  if(length(idx) > 1)
    return(idx[order(Table1[idx, "RMSE"])])
  else
    return(idx)
}
idxRk <- unlist(sapply(names(pkgRkbyRMSE), getorderbypkg))
Table1rk <- Table1[idxRk, ]
rownames(Table1rk) <- 1:n

```

<!-- Table of discarded packages -->
```{r echo=FALSE, message=FALSE}
options(knitr.kable.NA = '')

if(file.exists("./tables/Table2-UTF8-cleaned.csv")) {  
#  Table2 <- read.csv("./tables/Table2.csv", sep = ";", encoding="latin1")
  Table2 <- read.csv("./tables/Table2-UTF8-cleaned.csv")
  Table2 <- Table2[Table2[,3] != "", ]
  Table2 <- Table2[,c("Package", "Category", "Reason.to.Discard..Where.")]
}else
 Table2 <- rep("missing", 3)
colnames(Table2) <- c("Package", "Category", "Reason to Discard (File(s) and/or function(s))")


```


<!-- Additionnal Table for final result -->

```{r echo=FALSE, message=FALSE}
options(knitr.kable.NA = '')

if(file.exists("./tables/Table3.csv")) {  
  Table3 <- read.csv("./tables/Table3.csv")
}else
 Table3 <- rep("missing", 7)
colnames(Table3) <- c("Package::algorithm", "RMSE min", "RMSE median", "RMSE D51", 
                      "MAE median", "WAE median", "Time median")

TOP5algo <- subset(Table1rk, RMSE %in% 1:5)[, c("Package", "Algorithm")]
rownames(TOP5algo)  <- subset(Table1rk, RMSE %in% 1:5)[, "Package"]

Table3$Package <- sapply(strsplit(Table3[, "Package::algorithm"], "::"), head, n=1)
Table3$Algorithm <- TOP5algo[Table3$Package, "Algorithm"]
idx2shorten <- grep("(BFGS)", Table3$Algorithm)
Table3$Algorithm[idx2shorten] <- substr(Table3$Algorithm[idx2shorten], 1, nchar(Table3$Algorithm[idx2shorten])-6)
Table3 <- Table3[, c("Package", "Algorithm", "RMSE min", "RMSE median", 
                     "RMSE D51", "MAE median", "WAE median", "Time median")]
Table3[, c("RMSE min", "RMSE median", "RMSE D51", "MAE median", "WAE median", "Time median")] <- apply(Table3[, c("RMSE min", "RMSE median", 
                     "RMSE D51", "MAE median", "WAE median", "Time median")],
      2, signif, digits=4)

```


<!-- Parameter Table -->

```{r echo=FALSE, message=FALSE}
if(file.exists("./tables/TableAppendixC.csv")) {  
  Table1SupplApp <- read.csv("./tables/TableAppendixC.csv", sep = ";") 
}else 
  Table1SupplApp <- rep("missing",8)
colnames(Table1SupplApp) <- c("Num", "Input format", "Maxit", "Learn. rate", "median",
                    "d51", "MAE", "WAE")
#Table1SupplApp[, "Package:Algorithm"] <- paste0(Table1[Table1SupplApp$Num, "Package"], ":",
#                        substr(Table1[Table1SupplApp$Num, "Algorithm"], 4, nchar(Table1[Table1SupplApp$Num, "Algorithm"])))
#Table1SupplApp <- Table1SupplApp[, c("Num", "Package:Algorithm", "Input format", "Maxit", "Learn. rate", "median",
#                    "d51", "MAE", "WAE")]

Table1SupplApp[, "Package"] <- Table1[Table1SupplApp$Num, "Package"]
Table1SupplApp[, "Algorithm"] <- Table1[Table1SupplApp$Num, "Algorithm"]

Table1SupplApp <- Table1SupplApp[, c("Package","Algorithm", "Input format", "Maxit", "Learn. rate", "median",
                    "d51", "MAE", "WAE")]

#use idxRk as for Table1rk
if(all(Table1SupplApp[, "Algorithm"] == Table1$Algorithm))
{
  Table1SupplApp <- Table1SupplApp[idxRk, ]
  rownames(Table1SupplApp) <- 1:NROW(Table1SupplApp)
}

TOP5pkg <- head(unique(Table1rk$Package), 5)
TOP5algo <- head(Table1rk$Algorithm[order(Table1rk$RMSE)], 5)
TOP5algo <- substr(TOP5algo, 5, nchar(TOP5algo))
TOP5sentence <- paste0("TOP5 are ", paste(paste(TOP5pkg, TOP5algo, sep=":"), collapse=", "), ".")      

```

# Additionnal materials for all packages 
## Score probabilities


\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{img/scoreprobperpkgBnW.png}
        \label{fig:scoreprob}
        \caption{Score probabilities of \code{package:algorithm}}
\end{figure}

<!--## Appendix E-->


# Additionnal materials for top-5 packages

## Graphics for top-5 packages

Figures below provides some insights where a package performs reasonably well 
with respect to one explanatory variable and where the fit misses the correct behavior of
an explanatory variable.
It displays the average response per rounded explanatory variable for the predicted, 
the empirical and the theoretical values.
That is, the empirical value and the predicted value for the $j$th explanatory 
variable are respectively computed at x-value $x$ as
$$
\bar y^{emp}_j(x) = \frac1{n_x}  \sum_{i=1}^n y_i 1_{r(x_{i,j}) = x},~
\bar y^{pred}_j(x) =  \frac1{n_x} \sum_{i=1}^n \hat y_i 1_{r(x_{i,j}) = x},~~
n_x=\sum_{i=1}^n 1_{r(x_{i,j}) = x},
$$
where $r()$ denotes the round function with two decimal places and $y_i, \hat y_i$ 
stand respectively for the $i$th observed response and the $i$th predicted response.
For instance, \pkg{MachineShop}, \pkg{nnet}, \pkg{nlsr} do not correctly capture the sinusoidal 
aspect of explanatory variable $x_5$ on the expected response, whereas \pkg{rminer}, \pkg{validann}
miss the increasing non-linear trend of explanatory variable $x_1$ on the expected response.


\begin{figure}
    \centering    
    \includegraphics[width=\textwidth]{img/MachineShop.png}
        \label{fig:MachineShop}
        \caption{Average predicted mean per explanatory variable for \code{MachineShop}}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/nnet.png}
        \label{fig:nnet}
        \caption{Average predicted mean per explanatory variable for \code{nnet}}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/rminer.png}
        \label{fig:rminer}
        \caption{Average predicted mean per explanatory variable for \code{rminer}}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/nlsr.png}
        \label{fig:nlsr}
        \caption{Average predicted mean per explanatory variable for \code{nlsr}}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/validann.png}
        \label{fig:validann}
        \caption{Average predicted mean per explanatory variable for \code{validann}}
\end{figure}



