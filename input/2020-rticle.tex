% !TeX root = RJwrapper.tex
\title{A review of R neural network packages (with NNbenchmark)\(:\) accuracy
and ease of use}
\author{by Salsabila Mahdi, Akshaj Verma, Christophe Dutang, Patrice Kiener, John C. Nash}

\maketitle


\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

In the last three decades, neural networks (NN) have evolved from an
academic topic to a common scientific computing tool. CRAN currently
hosts approximately 80 packages in May 2020 involving neural network
modeling, some offering more than one algorithm. However, to our
knowledge, there is no comprehensive study which checks the accuracy,
the reliability and the ease-of-use of those NN packages.

In this paper, we attempted to test this rather large number of packages
against the common set of datasets with different levels of complexity,
and to benchmark and rank them with certain metrics.

Restricting our evaluation to regression algorithms applied on the
one-hidden layer perceptron and ignoring those for classification or
other specialized purposes, there were approximately 60
package::algorithm pairs left to test. The criteria used in our
benchmark were: (i) the accuracy, i.e.~the ability to find the global
minima on 13 datasets, measured by the Root Mean Square Error (RMSE) in
a limited number of iterations; (ii) the speed of the training
algorithm; (iii) the availability of helpful utilities; (iv) and the
quality of the documentation.

We have attempted to give a score for each evaluation criterion and to
rank each package::algorithm pair in a global table. Overall, 15 pairs
are considered accurate and reliable and can be recommended for daily
usage. Most others should be avoided as they are either less accurate,
too slow, too difficult to handle, or have poor or no documentation.

To carry out this work, we developed various codes and templates, as
well as the NNbenchmark package used for testing. This material is
available at \url{https://akshajverma.com/NNbenchmarkWeb/index.html} and
\url{https://github.com/pkR-pkR/NNbenchmark}, and can be used to verify
our work and, we hope, improve both packages and their evaluation.
Finally, we provide some hints and features to guide the development of
an idealized neural network package for R.

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The R Project for Statistical Computing (\url{www.r-project.org}), as
any opensource platform, relies on its contributors to keep it up to
date. Neural networks (NN), inspired on the brain's own connections
system, are a class of models in the growing field of machine learning
forwhich R has a number of tools. During the last 30 years, neural
networks have evolved from an academic topic to a common tool in
scientific computing. Previously, neural networks were considered more
theory than practice, partly because the algorithms used were
computationally demanding.

As a convenience in the general conversation, the same term is used in a
generic manner for different model structures and applications:
multilayer perceptron for regression, multilayer perceptron for
classification, multilayer perceptron for specialized applications,
recurrent neural network for autoregressive time series, convolutional
neural networks for dimension reduction and pattern recognition, deep
neural networks for image or voice recognition. Most of the above types
of neural networks can be found in R packages hosted on CRAN but without
any warranty about the accuracy or the speed of computation. This is an
issue as many poor algorithms are available in the literature and hence
poor packages implemented on CRAN.

A neural network algorithm requires complicated calculations to improve
the model control parameters. As with other optimization problems, the
gradient of the chosen cost function that indicates the lack of
suitability of the model is sought. This lets us improve the model by
changing the parameters in the negative gradient direction. Parameters
for the model are generally obtained using part of the available data (a
training set) and tested on the remaining data. Modern software allows
much of this work, including approximation of the gradient, to be
carried out without a large effort by the the user.

The training process can generally be made more efficient if we can also
approximate second derivatives of the cost function, allowing us to use
its curvature via the Hessian matrix. There are a large number of
approaches, of which quasi-Newton algorithms are perhaps the most common
and useful. Within this group, methods based on the
Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for updating the
(inverse) Hessian approximation provide several well-known examples. In
conducting this study, we believed that these second-order algorithms
would perform better than first-order methods for fit-in-memory
datasets.

Regardless of our belief, we wished to be able to conduct a thorough
examination of these training algorithms in R. There are many packages,
but barely any information to allow comparison. Our work, reported here,
aims to provide a framework for benchmarking neural network packages. We
restrict our examination to packages for R, and in this report focus on
those that provide neural networks of the perceptron type, that is, one
input layer, one normalized layer, one hidden layer with a nonlinear
activation function that is usually the hyperbolic tangent tanh(), and
one output output layer. The criteria used in our benchmark were: (i)
the accuracy, i.e.~the ability to find the global minima on 13 datasets
in a limited number of iterations; (ii) the speed of the training
algorithm; (iii) the availability of helpful utilities; (iv) and the
quality of the documentation. We restricted our evaluation to regression
algorithms applied on the one-hidden layer perceptron and ignored those
for classification or other specialized purposes.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

??JN: ******************************

In working on material below, I think we need to provide some
explanation of goals

\begin{itemize}
\tightlist
\item
  What do we mean by RMSE, other measures? Should define here for later
  use. ??JN perhaps?
\end{itemize}

We measure the quality of our model by how small the RMSE is of the
trained (??is this right?) model and the training data. This is defined
in Appendix A.

\begin{itemize}
\tightlist
\item
  What does ``convergence'' mean in our context? ??JN: Something like
\end{itemize}

In training neural network models we attempt to adjust a set of
parameters so that an objective function is minimized. When our method
for such adjustment can no longer reduce the objective, we say that we
have converged. However, practical methods require that we terminate the
optimization process in exceptional situations (e.g., a divide by zero)
or a pre-set limit on the number of steps or elapsed time is reached.

\begin{itemize}
\tightlist
\item
  What do we mean by ``performance''? Other goals? ??JN: perhaps?
\end{itemize}

We measure performance primarily by relative computing time between
methods on a particular computing platform. We could also count measures
of iterations, function evaluations or similar quantities that indicate
the computing effort. We note that differences in machine architecture
and in the attached libraries (e.g., BLAS choices for R) will modify our
measures. We are putting our tools on a Github repository so that
further evaluation can be made by ourselves and others as hardware and
software evolves.

As a reminder, RMSE and other convergence metrics are defined in
Appendix A.

The number of iterations we limited the algorithms to is in Appendix C.
Second order algorithms are all set to a maximum of 200 iterations. On
the other hand, first order algorithms were set to several values,
depending on how well they converged. maxit1storderA 1000 iterations,
maxit1storderB 10000 iterations, and maxit1storderC 100000 iterations.

We tried to harmonize the parameters.
*************************************

Our research process was divided into 3 phases.

\hypertarget{phase-1---preparation-of-benchmark-datasets}{%
\subsection{Phase 1 - Preparation of benchmark
datasets}\label{phase-1---preparation-of-benchmark-datasets}}

\hypertarget{datasets-need-to-be-finished}{%
\subsubsection{Datasets =\textgreater{} NEED TO BE
FINISHED??}\label{datasets-need-to-be-finished}}

All the datasets we use cannot generally be modeled using a
non-iterative calculation such as Ordinary Least Squares. Varying levels
of difficulty in modeling the different data sets are intended to allow
us to further classify different algorithms and the packages that
implement them. Sonja Surjanovic and Derek Bingham of Simon Fraser
University created a useful website from which three of the multivariate
datasets were drawn. We note the link, name and difficulty level of the
three datasets:\\
- \url{http://www.sfu.ca/~ssurjano/fried.html} (Friedman - average)\\
- \url{http://www.sfu.ca/~ssurjano/detpep10curv.html} (Dette - medium)\\
- \url{http://www.sfu.ca/~ssurjano/ishigami.html} (Ishigami - high)\\
The other multivariate dataset, Ref153, was taken from \ldots{}

Three of the univariate datasets we used were taken from a website of
the US National Institute for Standards and Technology (NIST):
\url{https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml}. (Gauss1 -
low; Gauss2 - low; Gauss3 - average)

Univariate datasets Dmod1, Dmod2 are from \ldots{}

Dreyfus1 is a pure neural network which has no error. This can make it
difficult for algorithms that assume an error exists. Dreyfus2 is
Dreyfus1 with errors. NeuroOne from \ldots{}

Finally, we also consider a Simon Wood test dataset, used in
\citep{wood2011fast} for benchmarking generalized additive models.
Precisely, we consider a generation of Gaussian random variates \(Y_i\),
\(i=1,\dots,n\) with the mean \(\mu_i\) defined as \[
\mu_i = 1+ f_0(x_{i,0})+f_1(x_{i,1})+f_2(x_{i,2})+f_3(x_{i,3})
+f_4(x_{i,4})+f_0(x_{i,5})
\] and standard deviation \(\sigma=1/4\) where \(f_j\) are Simon Wood's
smooth functions defined in Appendix B, \(x_{i,j}\) are uniform variates
and \(n=20,000\).

\hypertarget{packages}{%
\subsubsection{Packages}\label{packages}}

Using \CRANpkg{RWsearch} \citep{R-RWsearch} we sought to automate the
process of searching for neural network packages. All packages that have
``neural network'' as a keyword in the package title or in the package
description were included. In May 2020, around 80 packages falls into
this category. Packages \pkg{nlsr}, \pkg{minpack.lm}, \pkg{caret} were
added because the former 2 are important implementations of second-order
algorithms while the latter is the first cited meta package in the
CRAN's task view for machine learning,
\url{https://CRAN.R-project.org/view=MachineLearning}, as well as the
dependency for some of the other packages tested. Restricting to
regression analysis left us with 49 package::algorithm pairs in 2019 and
60 package::algorithm pairs in 2020.

\hypertarget{phase-2---review-of-packages-and-development-of-a-benchmarking-template}{%
\subsection{Phase 2 - Review of packages and development of a
benchmarking
template}\label{phase-2---review-of-packages-and-development-of-a-benchmarking-template}}

From documentation and example code, we learned that not all packages
selected by the automated search fit the scope of our research. Some
have no function to generate neural networks. Others were not regression
neural networks of the perceptron type or were only intended for very
specific purposes.

\textbf{Templates for Testing Accuracy and Speed}

As we inspected the packages, we developed a template for benchmarking.
The structure of this template (for each package) is as follows:\\
(1) Set up the test environment - loading of packages, setting working
directory and options;\\
(2) Summary of datasets;\\
(3) Loop over datasets: (a) setting parameters for a specific dataset
(b) selecting benchmark options (c) training a neural network with a
tuned functions for each package (d) calculation of RMSE and MAE
(??definition, reference) (e) plot each training over one initial graph,
then plot the best result (f) add results to the appropriate existing
record (*.csv file) and (g) clear the environment for next loop; and\\
(4) Clearing up the environment for the next package. (5) It is optional
to print warnings.

To simplify this process, we developed tools in the NNbenchmark package,
of which the first version was created as part of GSoC 2019. In GSoC
2020, 3 functions encapsulating the template that had been made generic
with \code{do.call} were added:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In trainPredict\_1mth1data a neural network is trained on one dataset
  and then used for predictions, with several utilities. Then, the
  performance of the neural network is summarized.
\item
  trainPredict\_1data serves as a wrapper function for
  trainPredict\_1mth1data for multiple methods.
\item
  trainPredict\_1pkg serves as a wrapper function for
  trainPredict\_1mth1data for multiple datasets.
\end{enumerate}

A function for the summary of accuracy and speed, NNsummary, was also
added. The package repository is
\url{https://github.com/pkR-pkR/NNbenchmark}, with package templates in
\url{https://github.com/pkR-pkR/NNbenchmarkTemplates}).

\textbf{Ease of Use Scoring}

We decided ease of use based on what we considered a user would need
when using a neural network package for nonlinear regression, namely,
utility functions and sufficient documentation.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Utilities (1 star)

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    a predict function exists\\
  \item
    scaling capabilities exist
  \end{enumerate}
\item
  Sufficient documentation (2 stars)

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    the existence of useful example/vignette = (1 star)

    \begin{itemize}
    \tightlist
    \item
      clear, with regression = 2 points
    \item
      unclear, examples use iris or are for classification only = 1
      point
    \item
      no examples = 0 points
    \end{itemize}
  \item
    input/output is clearly documented, e.g., what values are expected
    and returned by a function = (1 star)

    \begin{itemize}
    \tightlist
    \item
      clear input and output = 2 points
    \item
      only one is clear = 1 point
    \item
      both are not documented = 0 points
    \end{itemize}
  \end{enumerate}
\end{enumerate}

For a total of 0 to 3 stars.

\hypertarget{phase-3---collection-of-and-analysis-of-results}{%
\subsection{Phase 3 - Collection of and analysis of
results}\label{phase-3---collection-of-and-analysis-of-results}}

\hypertarget{results-collection}{%
\subsubsection{Results collection}\label{results-collection}}

Looping over the datasets using each package template, we collected
results in the relevant package directories in the templates repository.

\hypertarget{analysis}{%
\subsubsection{Analysis}\label{analysis}}

To rank how well a package converged and its speed, we developed the
following method:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The results datasets are loaded into the R environment as one large
  list. The dataset names, package:algorithm names and all 10 run
  numbers, durations, and RMSE are extracted from that list
\item
  For the duration score (DUR), the duration is averaged by dataset. 3
  criteria for the RMSE score by dataset are calculated:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The minimum value of RMSE for each package:algorithm as a measure of
  their best performance
\item
  The median value of RMSE for each package:algorithm as a measure of
  their average performance, without the influence of outliers
\item
  The spread of the RMSE values for each package which is measured by
  the difference between the median and the minimum RMSE (d51)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Then, the ranks are calculated for every dataset and the results are
  merged into one wide dataframe.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The duration rank only depends on the duration.
\item
  For minimum RMSE values, ties are decided by duration mean, then the
  RMSE median
\item
  For median RMSE values, ties are decided by the RMSE minimum, then the
  duration mean
\item
  The d51 rank only depends on itself
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  A global score for all datasets is found by a sum of the ranks (of
  duration, minimum RMSE, median RMSE, d51 RMSE) of each
  package:algorithm for each dataset
\item
  The final table is the result of ranking by the global minimum RMSE
  scores for each package:algorithm
\item
  In addition to the previous metrics, two other convergence metrics
  have been considered: the Mean Absolute Error (MAE) and the Worst
  Absolute Error (WAE), see Appendix A. The ranking on those two metrics
  may help distinguish packages with close RMSE values. However, we do
  not choose the MAE for overall ranking as there is no consensus in the
  literature, see e.g.~\citep{willmott2005advantages,chai2014root}.
\end{enumerate}

To rank how easy or not a package was to use (TO BE DISCUSSED FURTHER):
- Functionality (util): scaling, input, output, trace - Documentation
(docs): examples, structure/functions, vignettes

\hypertarget{results}{%
\section{Results}\label{results}}

\textbf{Tables} (NOTE: FINAL MEASURE FOR CONVERGENCE - RMSE RANKS? OR A
COMBINATION OF OTHER MEASURES? As in Christophe's recent email: L1
MAE(), L2 RMSE(), Linfinity (WAE)) --\textgreater{} see Appendix

(ALSO: THE FOLLOWING IS SIMPLY ALPHABETIC LIST FOR ALL TESTED, I WILL
DIVIDE THE TABLE INTO 4: 2nd ORDER always recommended, 1st ORDER
recommended, 1st ORDER not recommended, untested packages)

\begin{Schunk}
\begin{table}

\caption{\label{tab:unnamed-chunk-1}Results from Tested Packages}
\centering
\begin{tabular}[t]{llrrrr}
\toprule
package & algorithm & time.score & RMSE.score & UT & Docs\\
\midrule
AMORE & ADAPTgd & 10 & 34 & 1 & 3.0\\
 & ADAPTgdwm & 17 & 25 &  & \\
 & BATCHgd & 39 & 40 &  & \\
 & BATCHgdwm & 40 & 39 &  & \\
ANN2 & adam & 16 & 33 & 2 & 3.0\\
\addlinespace
 & rmsprop & 14 & 28 &  & \\
 & sgd & 12 & 41 &  & \\
automl & trainwgrad\_adam & 50 & 18 & 1 & 3.0\\
 & trainwgrad\_RMSprop & 47 & 26 &  & \\
 & trainwpso & 57 & 42 &  & \\
\addlinespace
brnn & Gauss-Newton & 8 & 13 & 2 & 4.0\\
CaDENCE & optim(BFGS) & 46 & 10 & 2 & 3.0\\
 & pso\_psoptim & 54 & 54 &  & \\
 & Rprop & 56 & 51 &  & \\
caret & avNNet\_nnet\_optim(BFGS) & 9 & 22 & 2 & 3.0\\
\addlinespace
deepdive & adam & 32 & 45 & 2 & 3.0\\
 & gradientDescent & 52 & 58 &  & \\
 & momentum & 53 & 56 &  & \\
 & rmsProp & 34 & 53 &  & \\
deepnet & BP & 23 & 18 & 1 & 3.0\\
\addlinespace
elmNNRcpp & ELM & 1 & 59 & 2 & 3.0\\
ELMR & ELM & 2 & 60 & 2 & 3.0\\
EnsembleBase & nnet\_optim(BFGS) & 5 & 12 & 1 & 1.0\\
h2o & first-order & 51 & 11 & 2 & 2.0\\
keras & adadelta & 60 & 47 & 2 & 0.0\\
\addlinespace
 & adagrad & 58 & 36 &  & \\
 & adam & 42 & 35 &  & \\
 & adamax & 48 & 23 &  & \\
 & nadam & 44 & 36 &  & \\
 & rmsprop & 37 & 52 &  & \\
\addlinespace
 & sgd & 48 & 43 &  & \\
MachineShop & nnet\_optim(BFGS) & 6 & 4 & 1 & 3.0\\
minpack.lm & Levenberg-Marquardt & 13 & 24 & 1 & 3.5\\
monmlp & optimx(BFGS) & 26 & 9 & 2 & 3.5\\
 & optimx(Nelder-Mead) & 32 & 46 &  & \\
\addlinespace
neuralnet & backprop & 37 & 48 & 1 & 3.0\\
 & rprop- & 21 & 21 &  & \\
 & rprop+ & 18 & 20 &  & \\
 & sag & 41 & 38 &  & \\
 & slr & 31 & 30 &  & \\
\addlinespace
nlsr & NashLM & 18 & 1 & 1 & 4.0\\
nnet & optim (BFGS) & 3 & 3 & 1 & 3.0\\
qrnn & nlm() & 28 & 15 & 2 & 3.0\\
radiant.model & nnet\_optim(BFGS) & 11 & 7 & 2 & 2.0\\
rminer & nnet\_optim(BFGS) & 14 & 2 & 2 & 3.5\\
\addlinespace
RSNNS & BackpropBatch & 43 & 49 & 2 & 3.0\\
 & BackpropChunk & 26 & 29 &  & \\
 & BackpropMomentum & 25 & 30 &  & \\
 & BackpropWeightDecay & 29 & 32 &  & \\
 & Quickprop & 45 & 57 &  & \\
\addlinespace
 & Rprop & 24 & 16 &  & \\
 & SCG & 30 & 17 &  & \\
 & Std\_Backpropagation & 22 & 27 &  & \\
snnR & SemiSmoothNewton & 7 & 49 & 2 & 2.0\\
traineR & nnet\_optim(BFGS) & 4 & 6 & 1 & 2.5\\
\addlinespace
validann & optim(BFGS) & 35 & 4 & 1 & 4.0\\
 & optim(CG) & 59 & 8 &  & \\
 & optim(L-BFGS-B) & 36 & 14 &  & \\
 & optim(Nelder-Mead) & 55 & 44 &  & \\
 & optim(SANN) & 20 & 55 &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
 &  &  &  &  & \\
\addlinespace
 &  &  &  &  & \\
\bottomrule
\end{tabular}
\end{table}

\end{Schunk}

(THE FOLLOWING IS JUST AN ALPHABETICALLY ORDERED LIST OF CURRENTLY
UNTESTED PACKAGES)

\begin{center}
<!-- \textbf{Table 2: Review of Ommitted Packages} -->

\begin{tabular}{l l l l}
  \toprule
  No & Name (package)         & Category & Comment \\
  \midrule
  1  &\pkg{appnn}             & AP        & This package provides a feed forward neural network to predict\\
     &                        &           & the amyloidogenicity propensity of polypeptide sequences      \\
  2  &\pkg{autoencoder}       & AP        & This package provides a sparse autoencoder, an unsupervised   \\
     &                        &           & algorithm that learns useful features from the data its given \\
  3  &\pkg{BNN}               & RE*       & This package uses a feed forward neural network to perform    \\
     &                        &           & regression as provided in the examples, however, it is unclear\\      &                        &           & whether it fits the form of perceptron that is the scope of   \\
     &                        &           & our research. Moreover, it states that it is intended for     \\      &                        &           & variable selection. Although how exactly the package would be \\
     &                        &           & used to do so isn't accessible in the package, especially     \\
     &                        &           & considering the source code is based on .c code that users of \\
     &                        &           & R might not understand. It's performance is slow, which may   \\
     &                        &           & have to do with the 100.000 iterations it needs, although     \\
     &                        &           & quite accurate for simple datasets.                           \\
  4  &\pkg{Buddle}            & RE**      & (errors)\\
  5  &\pkg{cld2}              & 00        & \\
  6  &\pkg{cld3}              & AP        & \\
  7  &\pkg{condmixt}          & AP        & \\
  8  &\pkg{deep}              & CL        & \\
  9  &\pkg{DALEX2}            & 00        & removed keyword, included in 2019 \\
  10 &\pkg{DamiaNN}           & RE**      & (errors) exported functions, still doesn't work \\
  11 &\pkg{DChaos}            & ??        & removed keyword for some reason, need to check out! \\
  12 &\pkg{deepNN}            & RE**      & (errors) I/O weird, ragged vector array \\
  13 &\pkg{DNMF}              & AP        & \\
  14 &\pkg{evclass}           & CL        & \\
  15 &\pkg{gamlss.add}        & RE        & there is some code but dist not appropriate \\
  16 &\pkg{gcForest}          & 00        & \\
  17 &\pkg{GMDH}              & TS        & \\
  18 &\pkg{GMDH2}             & CL        & \\
  19 &\pkg{GMDHreg}           & RE*       & \\
  20 &\pkg{grnn}              & RE**      & \\
  21 &\pkg{hybridEnsemble}    & ??        & \\ 
  22 &\pkg{isingLenzMC}       & AP        & \\
  23 &\pkg{leabRa}            & ??        & \\      
  24 &\pkg{learNN}            & ??        & \\     
  25 &\pkg{LilRhino}          & AP        & \\
  26 &\pkg{neural}            & CL        & \\
  27 &\pkg{NeuralNetTools}    & UT        & tools for neural networks           \\
  28 &\pkg{NeuralSens}        & UT        & tools for neural networks           \\
  29 &\pkg{NlinTS}            & TS        & Time Series                         \\
  30 &\pkg{nnetpredint}       & UT        & confidence intervals for NN          \\
  31 &\pkg{nnfor}             & TS        & Times Series, uses neuralnet         \\
  32 &\pkg{nntrf}             & UT        & \\
  33 &\pkg{onnx}              &           & provides an open source format       \\
  34 &\pkg{OptimClassifier}   &           & choose classifier parameters, nnet   \\
  35 &\pkg{OSTSC}             &           & solving oversampling classification  \\
  36 &\pkg{passt}             &           & \\
  36 &\pkg{pnn}               &           & Probabilistic                        \\
  37 &\pkg{polyreg}           &           & polyregression as alternative to NN  \\
  38 &\pkg{predictoR}         &           & shiny interface, neuralnet           \\
  39 &\pkg{ProcData}          &           & \\
  40 &\pkg{QuantumOps}        &           & classifies MNIST, Schuld (2018), removed keyword, in 2019 \\
  41 &\pkg{quarrint}          &           & specified classifier for quarry data \\
  42 &\pkg{rasclass}          &           & classifier for raster images, nnet?  \\
  43 &\pkg{rcane}             &           & \\
  44 &\pkg{regressoR}         &           & a manual rich version of predictoR   \\
  45 &\pkg{rnn}               &           & Recurrent                            \\
  46 &\pkg{RTextTools}        &           & \\
  47 &\pkg{ruta}              &           & \\
  48 &\pkg{simpleNeural}      &           & \\
  49 &\pkg{softmaxreg}        &           & \\
  50 &\pkg{Sojourn.Data}      &           & sojourn Accelerometer methods, nnet? \\
  51 &\pkg{spnn}              &           & classifier, probabilistic            \\
  52 &\pkg{studyStrap}        &           & \\
  53 &\pkg{TeachNet}          &           & classifier, selfbuilt, slow          \\
  54 &\pkg{tensorflow}        &           & \\
  55 &\pkg{tfestimators}      &           & \\
  56 &\pkg{trackdem}          &           & classifier for particle tracking     \\
  57 &\pkg{TrafficBDE}        & RE*       & \\
  58 &\pkg{tsfgrnn}           &           & \\
  59 &\pkg{yap}               &           & \\
  60 &\pkg{yager}             & RE*       & \\
  61 &\pkg{zFactor}           & AP        & 'compressibility' of hydrocarbon gas \\
\end{tabular}

\end{center}

\hypertarget{discussion-and-recommendations}{%
\subsection{Discussion and
Recommendations}\label{discussion-and-recommendations}}

The following is a list of packages we included in this study, with
brief descriptions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \CRANpkg{AMORE} \citep{R-AMORE},
\item
  \CRANpkg{ANN2} \citep{R-ANN2},
\item
  \CRANpkg{appnn} \citep{R-appnn},
\item
  \CRANpkg{autoencoder} \citep{R-autoencoder},
\item
  \CRANpkg{automl} \citep{R-automl},
\item
  \CRANpkg{BNN} \citep{R-BNN},
\item
  \CRANpkg{brnn} \citep{R-brnn},
\item
  \CRANpkg{Buddle} \citep{R-Buddle},
\item
  \CRANpkg{CaDENCE} \citep{R-CaDENCE},
\item
  \CRANpkg{cld2} \citep{R-cld2},
\item
  \CRANpkg{cld3} \citep{R-cld3},
\item
  \CRANpkg{condmixt} \citep{R-condmixt},
\item
  \CRANpkg{DamiaNN} \citep{R-DamiaNN},
\item
  \CRANpkg{deep} \citep{R-deep},
\item
  \CRANpkg{deepdive} \citep{R-deepdive},
\item
  \CRANpkg{deepnet} \citep{R-deepnet},
\item
  \CRANpkg{deepNN} \citep{R-deepNN},
\item
  \CRANpkg{DNMF} \citep{R-DNMF},
\item
  \CRANpkg{elmNNRcpp} \citep{R-elmNNRcpp},
\item
  \CRANpkg{ELMR} \citep{R-ELMR},
\item
  \CRANpkg{EnsembleBase} \citep{R-EnsembleBase},
\item
  \CRANpkg{evclass} \citep{R-evclass},
\item
  \CRANpkg{gamlss.add} \citep{R-gamlss.add},
\item
  \CRANpkg{gcForest} \citep{R-gcForest},
\item
  \CRANpkg{GMDH} \citep{R-GMDH},
\item
  \CRANpkg{GMDH2} \citep{R-GMDH2},
\item
  \CRANpkg{GMDHreg} \citep{R-GMDHreg},
\item
  \CRANpkg{gnn} \citep{R-gnn},
\item
  \CRANpkg{grnn} \citep{R-grnn},
\item
  \CRANpkg{h2o} \citep{R-h2o},
\item
  \CRANpkg{hybridEnsemble} \citep{R-hybridEnsemble},
\item
  \CRANpkg{isingLenzMC} \citep{R-isingLenzMC},
\item
  \CRANpkg{keras} \citep{R-keras},
\item
  \CRANpkg{kerasR} \citep{R-kerasR},
\item
  \CRANpkg{leabRa} \citep{R-leabRa},
\item
  \CRANpkg{learNN} \citep{R-learNN},
\item
  \CRANpkg{LilRhino} \citep{R-LilRhino},
\item
  \CRANpkg{minpack.lm} \citep{R-minpack.lm},
\item
  \CRANpkg{MachineShop} \citep{R-MachineShop},
\item
  \CRANpkg{monmlp} \citep{R-monmlp},
\item
  \CRANpkg{neural} \citep{R-neural},
\item
  \CRANpkg{neuralnet} \citep{R-neuralnet},
\item
  \CRANpkg{NeuralNetTools} \citep{R-NeuralNetTools},
\item
  \CRANpkg{NeuralSens} \citep{R-NeuralSens},
\item
  \CRANpkg{NlinTS} \citep{R-NlinTS},
\item
  \CRANpkg{nlsr} \citep{R-nlsr},
\item
  \CRANpkg{nnet} \citep{R-nnet},
\item
  \CRANpkg{nnetpredint} \citep{R-nnetpredint},
\item
  \CRANpkg{nnfor} \citep{R-nnfor},
\item
  \CRANpkg{nntrf} \citep{R-nntrf},
\item
  \CRANpkg{nnli2bRcpp} \citep{R-nnlib2Rcpp},
\item
  \CRANpkg{onnx} \citep{R-onnx},
\item
  \CRANpkg{OptimClassifier} \citep{R-OptimClassifier},
\item
  \CRANpkg{OSTSC} \citep{R-OSTSC},
\item
  \CRANpkg{pnn} \citep{R-pnn},
\item
  \CRANpkg{polyreg} \citep{R-polyreg},
\item
  \CRANpkg{predictoR} \citep{R-predictoR},
\item
  \CRANpkg{qrnn} \citep{R-qrnn},
\item
  \CRANpkg{QuantumOps} \citep{R-QuantumOps},
\item
  \CRANpkg{quarrint} \citep{R-quarrint},
\item
  \CRANpkg{radiant.model} \citep{R-radiant.model},
\item
  \CRANpkg{rasclass} \citep{R-rasclass},
\item
  \CRANpkg{rcane} \citep{R-rcane},
\item
  \CRANpkg{regressoR} \citep{R-regressoR},
\item
  \CRANpkg{rminer} \citep{R-rminer},
\item
  \CRANpkg{rnn} \citep{R-rnn},
\item
  \CRANpkg{RSNNS} \citep{R-RSNNS},
\item
  \CRANpkg{ruta} \citep{R-ruta},
\item
  \CRANpkg{simpleNeural} \citep{R-simpleNeural},
\item
  \CRANpkg{snnR} \citep{R-snnR},
\item
  \CRANpkg{softmaxreg} \citep{R-softmaxreg},
\item
  \CRANpkg{Sojourn.Data} \citep{R-Sojourn.Data},
\item
  \CRANpkg{spnn} \citep{R-spnn},
\item
  \CRANpkg{TeachNet} \citep{R-TeachNet},
\item
  \CRANpkg{tensorflow} \citep{R-tensorflow},
\item
  \CRANpkg{tfestimators} \citep{R-tfestimators},
\item
  \CRANpkg{trackdem} \citep{R-trackdem},
\item
  \CRANpkg{TrafficBDE} \citep{R-TrafficBDE},
\item
  \CRANpkg{tsensembler} \citep{R-tsensembler},
\item
  \CRANpkg{validann} \citep{R-validann},
\item
  \CRANpkg{zFactor} \citep{R-zFactor}.
\item
\item
\item
\item
\item
\item
\item
\end{enumerate}

A. Recommended: 2nd order algorithms Out of all the algorithms, these
second algorithms generally performed better in terms of convergence
despite being set to a much lower number of iterations, 200, than the
first-order algorithms. Moreover, they performed better in terms of
speed. The best in this class were. \CRANpkg{minpack.lm} and.
\CRANpkg{nlsr}, tied at rank number 1. The Levenberg-Marquardt (LM)
algorithm used is fast and converges well. stats::nls() is used.
However, these packages require a handwritten formula that may not be
ideal for certain situations. A more popular package for neural networks
is nnet. This might be because it is part of base R. It implements the
BFGS algorithm with stats::optim().

Ranked directly after are some packages that depend on nnet or use the
same functions. They differ in how well they decide initial parameters.
rminer (rank 4), MachineShop (rank 5), and radiant.model (rank 7) use
nnet. Note, radiant.model has its iterations set to 10000, which
originally made it slower yet converge better. We used a modified
version of the package. At rank 6 is validann's BFGS algorithm using
stats::optim(). Its use of optim's L-BFGS-B ranked at number 9 with
CaDENCE's use of optim's BFGS.. \CRANpkg{monmlp}, from the same author
as CaDENCE (Alex Cannon), uses the package. \CRANpkg{optimx}'s BFGS
\citep{R-optimx}.

Alex Cannon also implemented a quantile regression neural network in
qrnn with stats::nlm(). It requires more iterations and is not as fast
compared to the other second-order algorithms. However, it is a valuable
implementation of quantile regression. Last but not least is
\CRANpkg{brnn}'s Gauss Newton algorithm which ranks at number 8. brnn is
easy to use but does not converge as well due to a hidden constraint: a
missing first parameter. Furthermore, brnn's algorithm minimizes the sum
of squared errors and a penalty on parameters instead of just the sum of
squared errors. This may prevent parameters to get highly correlated,
especially with an almost degenerated Jacobian matrix.

B. Recommended: 1st order algorithms validann optim CG -slow RSNNS SCG
h2o back-propagation RSNNS Rprop ANN2 adam CaDENCE Rprop -SLOW deepnet
BP AMORE ADAPTgdwm AMORE ADAPTgd ANN2 sgd automl trainwgrad ANN2 rmsprop
RSNNS BackpropChunk RSNNS BackWeightDecay RSNNS Std\_Backpropagation
RSNNS BackpropMomentum automl trainwpso validann optim NelderMead snnR
Semi Smooth Newton RSNNS BackpropBatch validann optim SANN monmlp optimx
Nelder Mead

C. Not recommended: 1st order algorithms \textless- DISCUSS CUTOFF By
package ELMR, elmNNRcpp - fast ELM algorithms. Unfortunately, can't
finetune, does not converge well. neuralnet: a large ammount of
iterations, slow, erratic failures tensorflow: NOT EASY TO USE,
subsequently keras, tfestimators, ruta \ldots{} user needs to understand
the language However, advanced users might be able to highly specify a
neural network to their needs (customization?)

By algorithm: neuralnet rprop+ neuralnet rprop- neuralnet slr - once
ranked well with 100000 iterations AMORE BATCHgd CaDENCE pso psoptim -
need to reconfigure? elmNNRcpp - fast, no iterations RSNNS Quickprop (?)
AMORE BATCHgdwm tensorflow MomentumOptimizer tensorflow AdamOptimizer
ELMR - fast, no iterations tensorflow GradientDescentOptimizer keras
rmsprop keras adagrad keras sgd keras adadelta tensorflow
AdagradOptimizer keras adam tensorflow FtrlOptimizer neuralnetwork sag
tensorflow AdadeltaOptimizer neuralnet backprop - note, might not
actually reflect standings, somehow from template to template the
learning rate disappeared. Will fix this in future runs

D. Untested =\textgreater{} TO DO - LIST

\hypertarget{conclusion-and-perspective}{%
\section{Conclusion and perspective}\label{conclusion-and-perspective}}

??JN: Can we start to put in some major findings? i.e., important
positive findings, big negatives?

\hypertarget{positives-no-particular-order}{%
\subsection{Positives (no particular
order)}\label{positives-no-particular-order}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the existence of algorithms that converge well
\item
  nnet, which uses optim's BFGS, is already often chosen to represent
  neural networks for packages that are either a collection of
  independent machine learning algorithms, ensembles, or even
  applications in a field such as \ldots{}
\item
  the wide variety of neural networks available to users of R, from
  libraries of other programming languages to many different types of
  algorithms, hyperparameters, and uses
\end{enumerate}

\hypertarget{negatives-no-particular-order}{%
\subsection{Negatives (no particular
order)}\label{negatives-no-particular-order}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  bad documentation
\item
  the lack of packages that expand the number of unique second order
  algorithms. (Perhaps even the existence of what can be considered as
  repetitive packages?)
\item
  the lack of clear default values, or bad default values
\end{enumerate}

\hypertarget{future-work}{%
\subsection{Future work}\label{future-work}}

As the field of neural networks continue to grow, there will always be
more algorithms to validate. For current algorithms in R, our research
should be extended to encompass more types of neural networks and their
data formats (classifier neural networks, recurrent neural networks, and
so on). Different rating schemes and different parameters for package
functions can also be tried out.

\begin{itemize}
\tightlist
\item
  The dreamed NN package: Recommendation to package authors
\item
  Conclusion
\end{itemize}

\hypertarget{acknowledgements}{%
\subsection{Acknowledgements}\label{acknowledgements}}

This work was possible due to the support of the Google Summer of Code
initiative for R during years 2019 and 2020. Students Salsabila Mahdi
(2019 and 2020) and Akshaj Verma (2019) are grateful to Google for the
financial support.

\bibliography{RJreferences}

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{appendix-a}{%
\subsection{Appendix A}\label{appendix-a}}

Consider a set of observations \(y_i\) and its corresponding predictions
\(\hat y_i\) for \(i=1,\dots,n\). The three metrics used were: \[
MAE = \frac1n\sum_{i=1}^n|y_i - \hat y_i|,~
RMSE = \frac1n\sqrt{\sum_{i=1}^n(y_i - \hat y_i)^2},~
WAE = \frac1n\max_{i=1,\dots,n}|y_i - \hat y_i|.
\] These values represent the absolute, the squared and the maximum norm
of residual vectors.

\hypertarget{appendix-b}{%
\subsection{Appendix B}\label{appendix-b}}

We define three smooth functions for Simon Wood's test dataset \[
f_0=5*\sin(2\pi x),~
f_1=exp(3*x)-7
f_2=0.5 x^{11}*(10(1 - x))^6 - 10 (10*x)^3*(1 - x)^{10},~
\] \[
f_3=15 \exp(-5 |x-1/2|)-6,~
f_4=2-1_{(x <= 1/3)}(6*x)^3 - 1_{(x >= 2/3)} (6-6*x)^3 - 
1_{(2/3 > x > 1/3)}(8+2\sin(9*(x-1/3)\pi)).
\]

\hypertarget{appendix-c}{%
\subsection{Appendix C}\label{appendix-c}}

\begin{Schunk}

\begin{tabular}{rlllrrrr}
\toprule
No & input & maxit & lr & RMSEmed.score & RMSEd51.score & MAE.score & WAE.score\\
\midrule
1 & x \& y & 1000 & 0.01 & 24 & 8 & 26 & 19\\
2 & x \& y & 1000 & 0.01 & 22 & 28 & 17 & 26\\
3 & x \& y & 10000 & 0.1 & 37 & 24 & 42 & 31\\
4 & x \& y & 10000 & 0.1 & 33 & 14 & 36 & 27\\
5 & x \& y & 1000 & 0.01 & 27 & 26 & 28 & 23\\
\addlinespace
6 & x \& y & 1000 & 0.01 & 26 & 33 & 27 & 22\\
7 & x \& y & 1000 & 0.01 & 36 & 20 & 35 & 28\\
8 & x \& y & 1000 & 0.01 & 20 & 35 & 16 & 19\\
9 & x \& y & 1000 & 0.01 & 31 & 50 & 29 & 38\\
10 & x \& y & 1000 & - & 40 & 49 & 40 & 37\\
\addlinespace
11 & x \& y & 200 & - & 11 & 9 & 12 & 11\\
12 & x \& y & 200 & - & 28 & 48 & 23 & 39\\
13 & x \& y & 1000 & - & 56 & 56 & 54 & 56\\
14 & x \& y & 1000 & 0.01 & 54 & 60 & 52 & 58\\
15 & x \& y & 200 & - & 13 & 30 & 14 & 12\\
\addlinespace
16 & x \& y & 10000 & 0.4 & 41 & 1 & 37 & 44\\
17 & x \& y & 10000 & 0.8 & 57 & 2 & 57 & 53\\
18 & x \& y & 1000 & 0.8 & 52 & 3 & 53 & 51\\
19 & x \& y & 1000 & 0.8 & 45 & 4 & 47 & 50\\
20 & x \& y & 1000 & 0.8 & 18 & 38 & 24 & 17\\
\addlinespace
21 & x \& y & - & - & 59 & 55 & 59 & 59\\
22 & fmla \& data & - & - & 60 & 54 & 60 & 60\\
23 & x \& y & 200 & - & 15 & 33 & 15 & 15\\
24 & "y" \& data & 10000 & 0.01 & 7 & 7 & 8 & 8\\
25 & x \& y & 10000 & 0.1 & 48 & 27 & 51 & 41\\
\addlinespace
26 & x \& y & 10000 & 0.1 & 42 & 51 & 41 & 33\\
27 & x \& y & 10000 & 0.1 & 28 & 44 & 30 & 25\\
28 & x \& y & 10000 & 0.1 & 16 & 19 & 20 & 16\\
29 & x \& y & 10000 & 0.1 & 38 & 58 & 39 & 40\\
30 & x \& y & 10000 & 0.1 & 54 & 57 & 55 & 54\\
\addlinespace
31 & x \& y & 10000 & 0.1 & 44 & 47 & 43 & 43\\
32 & fmla \& data & 200 & - & 9 & 20 & 10 & 7\\
33 & full fmla \& data & 200 & - & 18 & 5 & 19 & 14\\
34 & x \& y & 200 & - & 10 & 17 & 9 & 10\\
35 & x \& y & 10000 & - & 46 & 46 & 43 & 47\\
\addlinespace
36 & fmla \& data & 100000 & 0.001 & 50 & 11 & 48 & 45\\
37 & fmla \& data & 100000 & - & 21 & 41 & 22 & 18\\
38 & fmla \& data & 100000 & - & 23 & 40 & 21 & 24\\
39 & fmla \& data & 100000 & - & 50 & 59 & 46 & 52\\
40 & fmla \& data & 100000 & - & 38 & 37 & 38 & 46\\
\addlinespace
41 & full fmla \& data & 200 & - & 3 & 16 & 3 & 6\\
42 & x \& y & 200 & - & 2 & 17 & 2 & 3\\
43 & x \& y & 200 & - & 14 & 22 & 7 & 35\\
44 & "y" \& data & 200 & - & 8 & 32 & 11 & 9\\
45 & fmla \& data & 200 & - & 1 & 6 & 1 & 1\\
\addlinespace
46 & x \& y & 10000 & 0.1 & 47 & 25 & 50 & 49\\
47 & x \& y & 1000 & - & 34 & 41 & 32 & 34\\
48 & x \& y & 1000 & - & 35 & 38 & 34 & 30\\
49 & x \& y & 1000 & - & 30 & 43 & 33 & 32\\
50 & x \& y & 10000 & - & 58 & 36 & 58 & 57\\
\addlinespace
51 & x \& y & 1000 & - & 24 & 51 & 25 & 29\\
52 & x \& y & 1000 & - & 16 & 22 & 18 & 19\\
53 & x \& y & 1000 & 0.1 & 32 & 30 & 31 & 36\\
54 & x \& y & 200 & - & 48 & 13 & 49 & 47\\
55 & fmla \& data & 200 & - & 5 & 15 & 6 & 2\\
\addlinespace
56 & x \& y & 200 & - & 4 & 11 & 4 & 5\\
57 & x \& y & 1000 & - & 6 & 10 & 5 & 4\\
58 & x \& y & 200 & - & 12 & 29 & 13 & 12\\
59 & x \& y & 10000 & - & 43 & 45 & 45 & 41\\
60 & x \& y & 1000 & - & 53 & 53 & 56 & 55\\
\bottomrule
\end{tabular}

\end{Schunk}


\address{%
Salsabila Mahdi\\
Universitas Syiah Kuala\\
JL. Syech Abdurrauf No.3, Aceh 23111, Indonesia\\
}
\href{mailto:bila.mahdi@mhs.unsyiah.ac.id}{\nolinkurl{bila.mahdi@mhs.unsyiah.ac.id}}

\address{%
Akshaj Verma\\
Manipal Institute of Technology\\
Manipal, Karnataka, 576104, India\\
}
\href{mailto:akshajverma7@gmail.com}{\nolinkurl{akshajverma7@gmail.com}}

\address{%
Christophe Dutang\\
University Paris-Dauphine, University PSL, CNRS, CEREMADE\\
Place du Marchal de Lattre de Tassigny, 75016 Paris, France\\
}
\href{mailto:dutang@ceremade.dauphine.fr}{\nolinkurl{dutang@ceremade.dauphine.fr}}

\address{%
Patrice Kiener\\
InModelia\\
5 rue Malebranche, 75005 Paris, France\\
}
\href{mailto:patrice.kiener@inmodelia.com}{\nolinkurl{patrice.kiener@inmodelia.com}}

\address{%
John C. Nash\\
Telfer School of Management, University of Ottawa\\
55 Laurier Avenue East, Ottawa, Ontario K1N 6N5 Canada\\
}
\href{mailto:nashjc@uottawa.ca}{\nolinkurl{nashjc@uottawa.ca}}

