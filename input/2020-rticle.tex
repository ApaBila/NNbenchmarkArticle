% !TeX root = RJwrapper.tex
\title{A review of R neural network packages with NNbenchmark\(:\) ease
of use and accuracy -- working title}
\author{by Salsabila Mahdi, Akshaj Verma, Christophe Dutang, Patrice
Kiener, John C. Nash}

\maketitle


\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

There are many R packages whose purpose is neural network modeling. With
support of the Google Summer of Code (GSoC) initiative for R, two of the
authors were supported, mentored by the other authors, to attempt to
benchmark and report on as many of these packages as possible. This
article presents the overall purpose of the study and the research
problems investigated, the basic design of the study, the major findings
or trends found as a result of our analysism a brief summary of our
interpretations and conclusions. These have been encapsulated in the
NNbenchmark package.

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The R Project for Statistical Computing (\url{www.r-project.org}), as
any opensource platform, relies on its contributors to keep it up to
date. Neural networks are a class of models, based on the brain's own
connections system, in the growing field of machine learning for which R
has a number of tools. Previously, neural networks could be considered
more theory than practise, partly because the algorithms used are
computationally demanding.

A neural network algorithm requires complicated calculations to improve
the model control parameters. As with other optimization problems, the
gradient of the chosen cost function that indicates the lack of
suitability of the model is sought. This lets us improve the model by
changing the parameters in the negative gradient direction. Parameters
for the model are generally obtained using part of our available data (a
training set) and tested on the remaining data. Modern software allows
the much of this work, including approximation of the gradient, to be
carried out without a large effort by the the user.

This process can generally be made more efficient if we also can
approximate second derivatives of the cost function, allowing us to use
its curvature via the Hessian matrix. There are a large number of
approaches, of which quasi-Newton algorithms are perhaps the most common
and useful. Within this group, methods based on the
Broyden-Fletcher-Goldfarb-Shanno (BFGS) ideas for updating the Hessian
approximation (or its inverse) provide several well-known examples. In
conducting this study, we believed that these second-order algorithms
would perform better than first-order methods.

Regardless of our belief, we wished to be able to conduct a thorough
examination of these training algorithms in R. There are many packages,
but barely any information to allow comparison. Our work, reported here,
aims to provide a framework for benchmarking neural network packages. We
restrict our examination to packages for R, and in this report focus on
those that provide neural networks of the perceptron type, that is, one
input layer, one normalized layer, one hidden layer with a nonlinear
activation function that is usually tanh(), and one output output layer.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

??JN: ******************************

In working on material below, I think we need to provide some
explanation of goals - What does ``convergence'' mean in our context? -
What do we mean by RMSE, other measures? Should define here for later
use. - What do we mean by ``performance''? Other goals?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Our research process was divided into 3 phases.

\hypertarget{phase-1---preparation}{%
\subsection{Phase 1 - Preparation}\label{phase-1---preparation}}

\hypertarget{datasets-need-to-be-finished}{%
\subsubsection{Datasets =\textgreater{} NEED TO BE
FINISHED}\label{datasets-need-to-be-finished}}

All the datasets used are nonlinear. ??JN. what do we mean by nonlinear
-- it has several different uses?? Linear data sets are more simple and
can even be solved with OLS (Ordinary Least Squares) regression.

?? Do we mean?

All the datasets we use are nonlinear in that they cannot be modelled
using a non-iterative calculation such as Ordinary Least Squares. In
this study, we wish to test the ability of neural networks to model
systems that are beyond the scope of linear regression. Varying levels
of difficulty in modelling the different data sets are intended to allow
us to further classify different algorithms and the packages that
implement them. Sonja Surjanovic and Derek Bingham of Simon Fraser
University created a useful website from which three of the multivariate
datasets were drawn. We note the link, name and difficulty level of the
three datasets:\\
- \url{http://www.sfu.ca/~ssurjano/fried.html} (Friedman - average)\\
- \url{http://www.sfu.ca/~ssurjano/detpep10curv.html} (Dette - medium)\\
- \url{http://www.sfu.ca/~ssurjano/ishigami.html} (Ishigami - high)\\
The other multivariate dataset, Ref153, was taken from \ldots{}

Three of the univariate datasets we used were taken from a webside of
the US National Institute for Standards and Technology (NIST):
\url{https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml}. (Gauss1 -
low; Gauss2 - low; Gauss3 - average)

Univariate datasets Dmod1, Dmod2 are from \ldots{}

Dreyfus1 is a pure neural network which has no error. This can make it
difficult for algorithms that assume an error exists. Dreyfus2 is
Dreyfus1 with errors. NeuroOne from \ldots{}\\
Wood \ldots{}

\hypertarget{packages}{%
\subsubsection{Packages}\label{packages}}

Using \CRANpkg{RWsearch} \citep{R-RWsearch} we sought all were able to
automate the process. All packages that have ``neural network'' as a
keyword in the package title or in the package description were
included.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \CRANpkg{AMORE} \citep{R-AMORE},
\item
  \CRANpkg{ANN2} \citep{R-ANN2},
\item
  \CRANpkg{appnn} \citep{R-appnn},
\item
  \CRANpkg{autoencoder} \citep{R-autoencoder},
\item
  \CRANpkg{automl} \citep{R-automl},
\item
  \CRANpkg{BNN} \citep{R-BNN},
\item
  \CRANpkg{brnn} \citep{R-brnn},
\item
  \CRANpkg{Buddle} \citep{R-Buddle},
\item
  \CRANpkg{CaDENCE} \citep{R-CaDENCE},
\item
  \CRANpkg{cld2} \citep{R-cld2},
\item
  \CRANpkg{cld3} \citep{R-cld3},
\item
  \CRANpkg{condmixt} \citep{R-condmixt},
\item
  \CRANpkg{DamiaNN} \citep{R-DamiaNN},
\item
  \CRANpkg{deep} \citep{R-deep},
\item
  \CRANpkg{deepdive} \citep{R-deepdive},
\item
  \CRANpkg{deepnet} \citep{R-deepnet},
\item
  \CRANpkg{deepNN} \citep{R-deepNN},
\item
  \CRANpkg{DNMF} \citep{R-DNMF},
\item
  \CRANpkg{elmNNRcpp} \citep{R-elmNNRcpp},
\item
  \CRANpkg{ELMR} \citep{R-ELMR},
\item
  \CRANpkg{EnsembleBase} \citep{R-EnsembleBase},
\item
  \CRANpkg{evclass} \citep{R-evclass},
\item
  \CRANpkg{gamlss.add} \citep{R-gamlss.add},
\item
  \CRANpkg{gcForest} \citep{R-gcForest},
\item
  \CRANpkg{GMDH} \citep{R-GMDH},
\item
  \CRANpkg{GMDH2} \citep{R-GMDH2},
\item
  \CRANpkg{GMDHreg} \citep{R-GMDHreg},
\item
  \CRANpkg{gnn} \citep{R-gnn},
\item
  \CRANpkg{grnn} \citep{R-grnn},
\item
  \CRANpkg{h2o} \citep{R-h2o},
\item
  \CRANpkg{hybridEnsemble} \citep{R-hybridEnsemble},
\item
  \CRANpkg{isingLenzMC} \citep{R-isingLenzMC},
\item
  \CRANpkg{keras} \citep{R-keras},
\item
  \CRANpkg{kerasR} \citep{R-kerasR},
\item
  \CRANpkg{leabRa} \citep{R-leabRa},
\item
  \CRANpkg{learNN} \citep{R-learNN},
\item
  \CRANpkg{LilRhino} \citep{R-LilRhino},
\item
  \CRANpkg{minpack.lm} \citep{R-minpack.lm},
\item
  \CRANpkg{MachineShop} \citep{R-MachineShop},
\item
  \CRANpkg{monmlp} \citep{R-monmlp},
\item
  \CRANpkg{neural} \citep{R-neural},
\item
  \CRANpkg{neuralnet} \citep{R-neuralnet},
\item
  \CRANpkg{NeuralNetTools} \citep{R-NeuralNetTools},
\item
  \CRANpkg{NeuralSens} \citep{R-NeuralSens},
\item
  \CRANpkg{NlinTS} \citep{R-NlinTS},
\item
  \CRANpkg{nlsr} \citep{R-nlsr},
\item
  \CRANpkg{nnet} \citep{R-nnet},
\item
  \CRANpkg{nnetpredint} \citep{R-nnetpredint},
\item
  \CRANpkg{nnfor} \citep{R-nnfor},
\item
  \CRANpkg{nntrf} \citep{R-nntrf},
\item
  \CRANpkg{nnli2bRcpp} \citep{R-nnlib2Rcpp},
\item
  \CRANpkg{onnx} \citep{R-onnx},
\item
  \CRANpkg{OptimClassifier} \citep{R-OptimClassifier},
\item
  \CRANpkg{OSTSC} \citep{R-OSTSC},
\item
  \CRANpkg{pnn} \citep{R-pnn},
\item
  \CRANpkg{polyreg} \citep{R-polyreg},
\item
  \CRANpkg{predictoR} \citep{R-predictoR},
\item
  \CRANpkg{qrnn} \citep{R-qrnn},
\item
  \CRANpkg{QuantumOps} \citep{R-QuantumOps},
\item
  \CRANpkg{quarrint} \citep{R-quarrint},
\item
  \CRANpkg{radiant.model} \citep{R-radiant.model},
\item
  \CRANpkg{rasclass} \citep{R-rasclass},
\item
  \CRANpkg{rcane} \citep{R-rcane},
\item
  \CRANpkg{regressoR} \citep{R-regressoR},
\item
  \CRANpkg{rminer} \citep{R-rminer},
\item
  \CRANpkg{rnn} \citep{R-rnn},
\item
  \CRANpkg{RSNNS} \citep{R-RSNNS},
\item
  \CRANpkg{ruta} \citep{R-ruta},
\item
  \CRANpkg{simpleNeural} \citep{R-simpleNeural},
\item
  \CRANpkg{snnR} \citep{R-snnR},
\item
  \CRANpkg{softmaxreg} \citep{R-softmaxreg},
\item
  \CRANpkg{Sojourn.Data} \citep{R-Sojourn.Data},
\item
  \CRANpkg{spnn} \citep{R-spnn},
\item
  \CRANpkg{TeachNet} \citep{R-TeachNet},
\item
  \CRANpkg{tensorflow} \citep{R-tensorflow},
\item
  \CRANpkg{tfestimators} \citep{R-tfestimators},
\item
  \CRANpkg{trackdem} \citep{R-trackdem},
\item
  \CRANpkg{TrafficBDE} \citep{R-TrafficBDE},
\item
  \CRANpkg{tsensembler} \citep{R-tsensembler},
\item
  \CRANpkg{validann} \citep{R-validann},
\item
  \CRANpkg{zFactor} \citep{R-zFactor}.
\end{enumerate}

\hypertarget{phase-2---review-of-packages-and-development-of-a-benchmarking-template}{%
\subsection{Phase 2 - Review of packages and development of a
benchmarking
template}\label{phase-2---review-of-packages-and-development-of-a-benchmarking-template}}

From documentation and example code, we learned that not all packages
selected by the automated search fit the scope of our research. Some
have no function to generate neural networks. They are simply
meta-packages. Others were not regression neural networks of the
perceptron type or were only intended for very specific purposes.

\textbf{Template =\textgreater{} TO REVISE AFTER 2020 CODE}

As we inspected the packages, we developed a template for benchmarking.
The structure of this template (for each package) is as follows:\\
(1) Set up the test environment - loading of packages, setting working
directory and options;\\
(2) Summary of datasets;\\
(3) Loop over datasets: (a) setting parameters for a specific dataset
(b) selecting benchmark options (c) training a neural network with a
tuned functions for each package (d) calculation of RMSE and MAE
(??definition, reference) (e) plot each training over one initial graph,
then plot the best result (f) add results to the appropriate existing
record (*.csv file) and (g) clear the environment for next loop; and\\
(4) Clearing up the environment for the next package. (5) It is optional
to print warnings.

To simplify this process, we developed tools in the NNbenchmark package,
of which the first version was created as part of the 2019 GSoC activity
and later refined in the 2020 initiative. The package repository is
\url{https://github.com/pkR-pkR/NNbenchmark}, with package templates in
\url{https://github.com/pkR-pkR/NNbenchmarkTemplates}).

\hypertarget{phase-3---collection-of-and-analysis-of-results}{%
\subsection{Phase 3 - Collection of and analysis of
results}\label{phase-3---collection-of-and-analysis-of-results}}

\hypertarget{results-collection}{%
\subsubsection{Results collection}\label{results-collection}}

Looping over the datasets using each package template, we collected
results in the relevant package directories in the tempplates
repository.

\hypertarget{analysis}{%
\subsubsection{Analysis}\label{analysis}}

To rank the how well a package converged and its speed, we developed the
following method: 1. The results datasets are loaded into the R
environment as one large list. The dataset names, package:algorithm
names and all 10 run numbers, durations, and RMSE are extracted from
that list 2. For the duration score (DUR), the duration is averaged by
dataset. 3 criteria for the RMSE score by dataset are calculated: a. The
minimum value of RMSE for each package:algorithm as a measure of their
best performance b. The median value of RMSE for each package:algorithm
as a measure of their average performance, without the influence of
outliers c.~The spread of the RMSE values for each package which is
measured by the difference between the median and the minimum RMSE (d51)
3. Then, the ranks are calculated for every dataset and the results are
merged into one wide dataframe. a. The duration rank only depends on the
duration. b. For minimum RMSE values, ties are decided by duration mean,
then the RMSE median c.~For median RMSE values, ties are decided by the
RMSE minimum, then the duration mean d.~The d51 rank only depends on
itself 4. A global score for all datasets is found by a sum of the ranks
(of duration, minimum RMSE, median RMSE, d51 RMSE) of each
package:algorithm for each dataset 5. The final table is the result of
ranking by the global minimum RMSE scores for each package:algorithm

To rank how easy or not a package was to use (TO BE DISCUSSED FURTHER):
- Functionality (util): scaling, input, output, trace - Documentation
(docs): examples, structure/functions, vignettes

\hypertarget{results}{%
\section{Results}\label{results}}

\textbf{Tables} (NOTE: FINAL MEASURE FOR CONVERGENCE - RMSE RANKS? OR A
COMBINATION OF OTHER MEASURES? As in Christophe's recent email: L1
MAE(), L2 RMSE(), Linfinity (WAE))

(ALSO: THE FOLLOWING IS SIMPLY ALPHABETIC LIST FOR ALL TESTED, I WILL
DIVIDE THE TABLE INTO 4: 2nd ORDER always recommended, 1st ORDER
recommended, 1st ORDER not recommended, untested packages)

\begin{center}
\textbf{Table X: Ratings}
\begin{tabular}{l l l l l l l}
  \toprule
  No & Name (package::algorithm)        & RMSE & DUR & UTIL & DOCS & OVERALL \\
  \midrule
  1  &\pkg{AMORE}::train.ADAPTgd        &      &     &      &      &         \\
     &\pkg{AMORE}::train.ADAPTgdwm      &      &     &      &      &         \\
     &\pkg{AMORE}::train.BATCHgd        &      &     &      &      &         \\ 
     &\pkg{AMORE}::train.BATCHgdwm      &      &     &      &      &         \\
  2  &\pkg{automl}                      &      &     &      &      &         \\
  3  &\pkg{ANN2}::neuralnetwork.sgd     &      &     &      &      &         \\
     &\pkg{ANN2}::neuralnetwork.adam    &      &     &      &      &         \\
     &\pkg{ANN2}::neuralnetwork.rmsprop &      &     &      &      &         \\
  4  &\pkg{brnn}                        &      &     &      &      &         \\
  5  &\pkg{CaDENCE}                     &      &     &      &      &         \\
  6  &\pkg{deepnet}::gradientdescent    &      &     &      &      &         \\
  7  &\pkg{elmNNRcpp}                   &      &     &      &      &         \\
  8  &\pkg{ELMR}                        &      &     &      &      &         \\
  9  &\pkg{h2o}::deeplearning           &      &     &      &      &         \\
  10 &\pkg{keras}                       &      &     &      &      &         \\
  11 &\pkg{kerasformula}                &      &     &      &      &         \\
  12 &\pkg{kerasR}                      &      &     &      &      &         \\
  13 &\pkg{minpack.lm}::nlsLM           &      &     &      &      &         \\
  14 &\pkg{MachineShop}::fit.NNetModel()&      &     &      &      &         \\
  15 &\pkg{monmlp}::fit.BFGS            &      &     &      &      &         \\
     &\pkg{monmlp}::fit.Nelder-Mead     &      &     &      &      &         \\
  16 &\pkg{neural}::mlptrain            &      &     &      &      &         \\
  17 &\pkg{neuralnet}::backprop         &      &     &      &      &         \\
     &\pkg{neuralnet}::rprop+           &      &     &      &      &         \\
     &\pkg{neuralnet}::rprop-           &      &     &      &      &         \\
     &\pkg{neuralnet}::sag              &      &     &      &      &         \\
     &\pkg{neuralnet}::slr              &      &     &      &      &         \\
  18 &\pkg{nlsr}::nlxb                  &      &     &      &      &         \\
  19 &\pkg{nnet}::nnet.BFGS             &      &     &      &      &         \\
  20 &\pkg{qrnn}::qrnn.fit              &      &     &      &      &         \\
  21 &\pkg{radiant.model}::radiant.model&      &     &      &      &         \\
  22 &\pkg{rcane}::rlm                  &      &     &      &      &         \\
  23 &\pkg{rminer}::fit                 &      &     &      &      &         \\
  24 &\pkg{RSNNS}::BackpropBatch        &      &     &      &      &         \\
     &\pkg{RSNNS}::BackpropChunk        &      &     &      &      &         \\
     &\pkg{RSNNS}::BackpropMomentum     &      &     &      &      &         \\
     &\pkg{RSNNS}::BackpropWeightDecay  &      &     &      &      &         \\
     &\pkg{RSNNS}::Quickprop            &      &     &      &      &         \\
     &\pkg{RSNNS}::Rprop                &      &     &      &      &         \\
     &\pkg{RSNNS}::SCG                  &      &     &      &      &         \\
     &\pkg{RSNNS}::Std-Backpropagation  &      &     &      &      &         \\
  25 &\pkg{ruta}                        &      &     &      &      &         \\
  26 &\pkg{simpleNeural}::sN.MLPtrain   &      &     &      &      &         \\ 
  27 &\pkg{snnR}                        &      &     &      &      &         \\
  28 &\pkg{softmaxreg}                  &      &     &      &      &         \\
  29 &\pkg{tensorflow}::AdadeltaOptmizer&      &     &      &      &         \\
     &\pkg{tensorflow}::AdagradOptmizer &      &     &      &      &         \\
     &\pkg{tensorflow}::AdamOptmizer    &      &     &      &      &         \\
     &\pkg{tensorflow}::FtrlOptmizer    &      &     &      &      &         \\
     &\pkg{tensorflow}::GradientDescent &      &     &      &      &         \\
     &\pkg{tensorflow}::MomentumOptmizer&      &     &      &      &         \\
  30 &\pkg{tfestimators}                &      &     &      &      &         \\
  31 &\pkg{tsensembler}                 &      &     &      &      &         \\
  32 &\pkg{validann}::Nelder-Mead       &      &     &      &      &         \\
     &\pkg{validann}::BFGS              &      &     &      &      &         \\
     &\pkg{validann}::CG                &      &     &      &      &         \\
     &\pkg{validann}::L-BFGS-B          &      &     &      &      &         \\
     &\pkg{validann}::SANN              &      &     &      &      &         \\  
     &\pkg{validann}::Brent             &      &     &      &      &         \\
  \end{tabular}
\end{center}

(THE FOLLOWING IS JUST AN ALPHABETICALLY ORDERED LIST OF CURRENTLY
UNTESTED PACKAGES)

\begin{center}
\textbf{Table 2: Review of Ommitted Packages}

\begin{tabular}{l l l l}
  \toprule
  No & Name (package)            & Category & Comment \\
  \midrule
  1  &\pkg{appnn}                & -        & \\
  2  &\pkg{autoencoder}          & -        & \\     
  3  &\pkg{BNN}                  & -        & \\
  4  &\pkg{Buddle}               & -        & \\
  5  &\pkg{cld2}                 & -        & \\
  6  &\pkg{cld3}                 & -        & \\
  7  &\pkg{condmixt}             & -        & \\
  8  &\pkg{DALEX2}               & -        & \\
  9  &\pkg{DamiaNN}              & -        & \\
  10 &\pkg{DChaos}               & -        & \\
  11 &\pkg{deepNN}               & -        & \\
  12 &\pkg{DNMF}                 & -        & \\
  13 &\pkg{EnsembleBase}         & -        & \\
  14 &\pkg{evclass}              & -        & \\
  15 &\pkg{gamlss.add}           & -        & \\
  16 &\pkg{gcForest}             & -        & \\
  17 &\pkg{GMDH}                 & -        & \\
  18 &\pkg{GMDH2}                & -        & \\
  19 &\pkg{GMDHreg}              & -        & \\
  20 &\pkg{grnn}                 & -        & \\
  21 &\pkg{hybridEnsemble}       & -        & \\ 
  22 &\pkg{isingLenzMC}          & -        & \\
  23 &\pkg{leabRa}               & -        & \\      
  24 &\pkg{learNN}               & -        & \\     
  25 &\pkg{LilRhino}             & -        & \\
  26 &\pkg{NeuralNetTools}       & -        & tools for neural networks           \\
  27 &\pkg{NeuralSens}           & -        & tools for neural networks           \\
  28 &\pkg{NlinTS}               & NA       & Time Series                         \\
  29 &\pkg{nnetpredint}          & -        & confidence intervals for NN          \\
  30 &\pkg{nnfor}                & NA       & Times Series, uses neuralnet         \\
  31 &\pkg{onnx}                 & -        & provides an open source format       \\
  32 &\pkg{OptimClassifier}      & NA       & choose classifier parameters, nnet   \\
  33 &\pkg{OSTSC}                & -        & solving oversampling classification  \\
  34 &\pkg{pnn}                  & NA       & Probabilistic                        \\
  35 &\pkg{polyreg}              & -        & polyregression ALT to NN             \\
  36 &\pkg{predictoR}            & NA       & shiny interface, neuralnet           \\
  37 &\pkg{QuantumOps}           & NA       & classifies MNIST, Schuld (2018)      \\
  38 &\pkg{quarrint}             & NA       & specified classifier for quarry data \\
  39 &\pkg{rasclass}             & NA       & classifier for raster images, nnet?  \\
  40 &\pkg{regressoR}            & NA       & a manual rich version of predictoR   \\
  41 &\pkg{rnn}                  & NA       & Recurrent                            \\
  42 &\pkg{Sojourn.Data}         & NA       & sojourn Accelerometer methods, nnet? \\
  43 &\pkg{spnn}                 & NA       & classifier, probabilistic            \\
  44 &\pkg{TeachNet}             & NA       & classifier, selfbuilt, slow          \\
  45 &\pkg{trackdem}             & NA       & classifier for particle tracking     \\
  46 &\pkg{TrafficBDE}           & NA       & specific reg, predicting traffic     \\
  47 &\pkg{zFactor}              & NA       & 'compressibility' of hydrocarbon gas \\
\end{tabular}

\end{center}

\hypertarget{discussion-and-recommandations}{%
\subsection{Discussion and
Recommandations}\label{discussion-and-recommandations}}

A. Recommended: 2nd order algorithms Out of all the algorithms, these
second algorithms generally performed better in terms of convergence
despite being set to a much lower number of iterations, 200, than the
first-order algorithms. Moreover, they performed better in terms of
speed. The best in this class were. \CRANpkg{minpack.lm} and.
\CRANpkg{nlsr}, tied at rank number 1. The Levenberg-Marquardt (LM)
algorithm used is fast and converges well. stats::nls() is used.
However, these packages require a handwritten formula that may not be
ideal for certain situations. A more popular package for neural networks
is nnet. This might be because it is part of base R. It implements the
BFGS algorithm with stats::optim().

Ranked directly after are some packages that depend on nnet or use the
same functions. They differ in how well they decide initial parameters.
rminer (rank 4), MachineShop (rank 5), and radiant.model (rank 7) use
nnet. Note, radiant.model has its iterations set to 10000, which
originally made it slower yet converge better. We used a modified
version of the package. At rank 6 is validann's BFGS algorithm using
stats::optim(). Its use of optim's L-BFGS-B ranked at number 9 with
CaDENCE's use of optim's BFGS.. \CRANpkg{monmlp}, from the same author
as CaDENCE (Alex Cannon), uses the package. \CRANpkg{optimx}'s BFGS
\citep{R-optimx}.

Alex Cannon also implemented a quantile regression neural network in
qrnn with stats::nlm(). It requires more iterations and is not as fast
compared to the other second-order algorithms. However, it is a valuable
implementation of quantile regression. Last but not least is
\CRANpkg{brnn}'s Gauss Newton algorithm which ranks at number 8. brnn is
easy to use but does not converge as well due to a hidden constraint: a
missing first parameter. Furthermore, brnn's algorithm minimizes the sum
of squared errors and a penalty on parameters instead of just the sum of
squared errors. This may prevent parameters to get highly correlated,
especially with an almost degenerated Jacobian matrix.

B. Recommended: 1st order algorithms validann optim CG RSNNS SCG h2o
back-propagation RSNNS Rprop ANN2 adam CaDENCE Rprop -SLOW deepnet BP
AMORE ADAPTgdwm AMORE ADAPTgd ANN2 sgd automl trainwgrad ANN2 rmsprop
RSNNS BackpropChunk RSNNS BackWeightDecay RSNNS Std\_Backpropagation
RSNNS BackpropMomentum automl trainwpso validann optim NelderMead snnR
Semi Smooth Newton RSNNS BackpropBatch validann optim SANN monmlp optimx
Nelder Mead

C. Not recommended: 1st order algorithms \textless- DISCUSS CUTOFF By
package ELMR, elmNNRcpp - fast ELM algorithms. Unfortunately, can't
finetune, does not converge well. neuralnet: a large ammount of
iterations, slow, erratic failures tensorflow: NOT EASY TO USE,
subsequently keras, tfestimators, ruta \ldots{} user needs to understand
the language However, advanced users might be able to highly specify a
neural network to their needs (customization?)

By algorithm: neuralnet rprop+ neuralnet rprop- neuralnet slr - once
ranked well with 100000 iterations AMORE BATCHgd CaDENCE pso psoptim -
need to reconfigure? elmNNRcpp - fast, no iterations RSNNS Quickprop (?)
AMORE BATCHgdwm tensorflow MomentumOptimizer tensorflow AdamOptimizer
ELMR - fast, no iterations tensorflow GradientDescentOptimizer keras
rmsprop keras adagrad keras sgd keras adadelta tensorflow
AdagradOptimizer keras adam tensorflow FtrlOptimizer neuralnetwork sag
tensorflow AdadeltaOptimizer neuralnet backprop - note, might not
actually reflect standings, somehow from template to template the
learning rate disappeared. Will fix this in future runs

D. Untested =\textgreater{} TO DO - LIST

\hypertarget{conclusion-to-do-after-2020-code}{%
\section{Conclusion =\textgreater{} TO DO AFTER 2020
CODE}\label{conclusion-to-do-after-2020-code}}

\hypertarget{future-work}{%
\subsection{Future work}\label{future-work}}

As the alogrithms for neural networks continue to grow, there will
always be more to validate. For current algorithms in R, our research
should be extended to encompass more types of neural networks and their
data formats (classifier neural networks, recurrent neural networks, and
so on). Different rating schemes and different parameters for package
functions can also be tried out.

\hypertarget{acknowledgements}{%
\subsection{Acknowledgements}\label{acknowledgements}}

This work was possible due to the support of the Google Summer of Code
initiative for R during years 2019 and 2020. Students Salsabila Madhi
(2019 and 2020) and Akshaj Verma (2019) are grateful to Google for the
financial support.

\bibliography{RJreferences}

\begin{itemize}
\tightlist
\item
  The dreamed NN package: Recommandation to package authors
\item
  Conclusion
\item
  Acknowledgments
\end{itemize}

For the acknowledgements, maybe : « » + later some aknowledgements to
the referees.

How do we proceed?


\address{%
Salsabila Mahdi\\
Universitas Syiah Kuala\\
JL. Syech Abdurrauf No.3, Aceh 23111, Indonesia\\
}
\href{mailto:bila.mahdi@mhs.unsyiah.ac.id}{\nolinkurl{bila.mahdi@mhs.unsyiah.ac.id}}

\address{%
Akshaj Verma\\
Manipal Institute of Technology\\
Navi Mumbai, Maharashtra, 400614, India\\
}
\href{mailto:akshajverma7@gmail.com}{\nolinkurl{akshajverma7@gmail.com}}

\address{%
Christophe Dutang\\
Univ. Paris-Dauphine, Univ. PSL, CNRS, CEREMADE\\
Place du Ml de Lattre de Tassigny, 75016 Paris, France\\
}
\href{mailto:dutang@ceremade.dauphine.fr}{\nolinkurl{dutang@ceremade.dauphine.fr}}

\address{%
Patrice Kiener\\
InModelia\\
5 rue Malebranche, 75005 Paris, France\\
}
\href{mailto:patrice.kiener@inmodelia.com}{\nolinkurl{patrice.kiener@inmodelia.com}}

\address{%
John C. Nash\\
Telfer School of Management, University of Ottawa\\
55 Laurier Avenue East, Ottawa, Ontario K1N 6N5 Canada\\
}
\href{mailto:nashjc@uottawa.ca}{\nolinkurl{nashjc@uottawa.ca}}
