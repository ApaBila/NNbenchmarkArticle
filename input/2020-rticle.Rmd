---
title: A review of R neural network packages (with NNbenchmark)$:$ accuracy and ease of use
author:
  - name: Salsabila Mahdi
    affiliation: Universitas Syiah Kuala
    address: JL. Syech Abdurrauf No.3, Aceh 23111, Indonesia 
    email:  bila.mahdi@mhs.unsyiah.ac.id
  - name: Akshaj Verma
    affiliation: Manipal Institute of Technology
    address: Manipal, Karnataka, 576104, India
    email:  akshajverma7@gmail.com
  - name: Christophe Dutang
    affiliation: University Paris-Dauphine, University PSL, CNRS, CEREMADE
    address: Place du Maréchal de Lattre de Tassigny, 75016 Paris, France
    email:  dutang@ceremade.dauphine.fr
  - name: Patrice Kiener
    affiliation: InModelia
    address: 5 rue Malebranche, 75005 Paris, France
    email:  patrice.kiener@inmodelia.com
  - name: John C. Nash
    affiliation: Telfer School of Management, University of Ottawa
    address: 55 Laurier Avenue East, Ottawa, Ontario K1N 6N5 Canada
    email:  nashjc@uottawa.ca
preamble: 
  includes: 
    \usepackage{float}
    \usepackage{multirow}
output: rticles::rjournal_article
---

```{r, echo=FALSE}
library(knitr)
library(kableExtra)
```


## Abstract

In the last three decades, neural networks (NN) have evolved from an academic topic to a common scientific computing tool. 
CRAN currently hosts approximately 80 packages in May 2020 involving neural network modeling, some offering more than one algorithm. 
However, to our knowledge, there is no comprehensive study which checks the accuracy, the reliability
and the ease-of-use of those NN packages.

<!-- JN??: changed "the" to "a" common set
==> B: agree -->

In this paper, we attempted to test this rather large number of packages against a common set of datasets with different levels of complexity, and to benchmark and rank them with certain metrics.

Restricting our evaluation to regression algorithms applied on the one-hidden layer perceptron and ignoring those for classification or other specialized purposes, there were approximately 60 package::algorithm pairs left to test. 
The criteria used in our benchmark were: (i) the accuracy, i.e. the ability to find the global minima on 13 datasets, measured by the Root Mean Square Error (RMSE) in a limited number of iterations; (ii) the speed of the training algorithm; (iii) the availability of helpful utilities; (iv) and the quality of the documentation.

We have attempted to give a score for each evaluation criterion and to rank each package::algorithm pair in a global table. Overall, 15 pairs are considered accurate and reliable and can be recommended for daily usage. 
Most others should be avoided as they are either less accurate, too slow, too difficult to handle, or have poor or no documentation.

To carry out this work, we developed various codes and templates, as well as the NNbenchmark package used for testing.
This material is available at https://akshajverma.com/NNbenchmarkWeb/index.html and https://github.com/pkR-pkR/NNbenchmark, and can be used to verify our work
and, we hope, improve both packages and their evaluation. Finally, we provide some hints and features to guide the 
development of an idealized neural network package for R.


## Introduction


The R Project for Statistical Computing (\url{www.r-project.org}), as any opensource platform, relies on its contributors to keep it up to date. 
Neural networks (NN), inspired on the brain's own connections system, are a class of models in the growing field of machine learning for which R has a number of tools.
During the last 30 years, neural networks have evolved from an academic topic to a common tool in scientific computing.
Previously, neural networks were considered more theory than practice, partly because the algorithms used were computationally demanding. 

As a convenience in the general conversation, the same term is used in a generic manner for different model structures and applications: multilayer perceptron for regression, multilayer perceptron for classification, multilayer perceptron for specialized applications, recurrent neural network for autoregressive time series, convolutional neural networks for dimension reduction and pattern recognition, deep neural networks for image or voice recognition.
Most of the above types of neural networks can be found in R packages hosted on CRAN but without any warranty about the accuracy or the speed of computation.
This is an issue as many poor algorithms are available in the literature and hence poor packages implemented on CRAN.

A neural network algorithm requires complicated calculations to improve the model control parameters. As with other optimization problems, the gradient of the chosen cost function that indicates the lack of suitability of the model is sought. 
This lets us improve the model by changing the parameters in the negative gradient direction. Parameters for the model are generally obtained using part of the available data (a training set) and tested on the remaining data. 
Modern software allows much of this work, including approximation of the gradient, to be carried out without a large effort by the the user. 

The training process can generally be made more efficient if we can also approximate second derivatives of the cost function, allowing us to use its curvature via the Hessian matrix. 
There are a large number of approaches, of which quasi-Newton algorithms are perhaps the most common and useful. 
Within this group, methods based on the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for updating the (inverse) Hessian approximation provide several well-known examples. 
In conducting this study, we believed that these second-order algorithms would perform better than first-order methods for fit-in-memory datasets. 

Regardless of our belief, we wished to be able to conduct a thorough examination of 
these training algorithms in R. 
There are many packages, but barely any information to allow comparison. 
Our work, reported here, aims to provide a framework for benchmarking neural network packages. 
We restrict our examination to packages for R, and in this report focus on those that provide neural networks of the perceptron type, that is, one input layer, one normalized layer, one hidden layer with a nonlinear activation function that is usually the hyperbolic tangent tanh(), and one output output layer. 
The criteria used in our benchmark were: (i) the accuracy, i.e. the ability to find the global minima on 13 datasets in a limited number of iterations; (ii) the speed of the training algorithm; (iii) the availability of helpful utilities; (iv) and the quality of the documentation.
We restricted our evaluation to regression algorithms applied on the one-hidden layer perceptron and ignored those for classification or other specialized purposes. 



# Neural Networks: the perceptron

Here, we give a short description of the one hidden layer perceptron. As the "layer" term suggests it, some terms come from the representation of graphs whereas some other terms come from the  traditional literature on nonlinear models. 

Using the graph description, a one-hidden layer neural network is made of 3 parts: (i) the layer of the input(s), (ii) the hidden layer which consists of independant neurons, each of them performing two operations: a linear combination of the inputs plus an offset, then a nonlinear function applied on this linear combination. (iii) the layer of the output(s) which is a linear combination of the output of the nonlinar functions in the hidden layer. 

<!-- JN??: differentiable rather than derivable -->

The nonlinear function used in the hidden layer must have the following four properties: continuous, differentiable, monotonic, bounded. The logistic function, the hyperbolic tangent function and the arctangent functions are the usual candidates. 

The above description has a simple mathematical equivalence. Let us give two examples. 

The model $y = a1 + a2*tanh(a3 + a4*x) + a5*tanh(a6 + a7*x) + a8*tanh(a9 + a10*x)$ describes a neural network with one input, three hidden neurons, one output model where x is the input, tanh() is the activation function, y is the output and $a1,..,a10$ are the parameters. 

The model $y = a1 + a2*atan(a3 + a4*x1 + a5*x2 + a6*x3 + a7*x4 + a8*x5) + a9*atan(a10 + a11*x1 + a12*x2 + a13*x3 + a14*x4 + a15*x5) + a16*atan(a17 + a18*x1 + a19*x2 + a20*x3 + a21*x4 + a22*x5)$ describes a neural network with five inputs, three hidden neurons, one output model where x is the input, atan() is the activation function, y is the output and $a1,..,a22$ are the parameters. 

In order to get large gradients at the first steps of the training algorithm, it is recommended to use normalized inputs, normalized outputs, odd functions like the hyperbolic tangent function or the arctangent function and small random values to initialize the parameters, for instance extracted from the N(0, 0.1) distribution. Such good practices help find good local minima and possibly the global minimum. 

The dataset used for the training is assumed to have a number of rows much larger than the number of parameters. While «much larger» is subject to discussion, values of 3 to 5 are generally accepted. (In experimental design, some iterative strategies start with a dataset having a number of experiments/lines equal to 1.8 times the number of parameters and then increase the number of experiments to finetune the model.)

It is rather clear from the mathematical formula above that neural networks of perceptron type are nonlinear models and require for their parameter estimation some training algorithms that can handle (highly) nonlinear models. Indeed, the intrinsic and parametric curvatures of such models are usually very high and, with so many parameters, the Jacobian matrix might exhibit some collinearities between its columns and become nearly singular. As a result, appropriate algorithms for such dataset::model pairs are rather limited and well-known. They are the second-order algorithms like BFGS and Levenberg-Marquardt (and Horsehoe?).

JN??: What is the Horseshoe? Probably I know it by a different name.
      Also Levenberg-Marquardt is a stabilization of the Jacobian that could be applied to several algorithms. Should we say 'Levenberg-Marquardt stabilized Gauss-Newton', which is what nlsr uses.
      
Unfortunately, due to some simple literature on the gradient and the hype around "deep neural networks" that manipulate ultra-large models with hundreds or thousands parameters and sometimes more parameters than examples in the datasets, many papers and many R packages emphasize the use of first-order gradient algorithms. In the case of the perceptron, this is an error and the goal of this paper is to demonstrate it. 

JN??: replace previous paragraph with ??

Unfortunately, there are widely-discussed articles concerning the gradient and "deep neural networks" that manipulate ultra-large models with hundreds or thousands parameters and sometimes more parameters than examples in the datasets.
These, along with some R packages, emphasize the use of first-order gradient algorithms. In the case of the perceptron, 
we contend this is an error, and provide evidence to that effect in this paper.



# Methodology


<!-- Considering regression problems, we measure the quality of our model by the -->
<!-- root mean squared error (RMSE): the smaller RMSE the better the fitted model.  -->
<!-- In addition to the RMSE, two other convergence metrics have been considered:  -->
<!-- the Mean Absolute Error (MAE) and the Worst Absolute Error (WAE). -->
<!-- We recall the definition of the RMSE, the MAE and the WAE in Appendix A. -->
<!-- The ranking on MAE and WAE may help distinguish packages with close RMSE values.  -->

<!-- B??: Christophe, I think John's explanation fits better. Can we erase the above? Also, I think this can be merged into or replace somewhat the part of Phase 2/point 2 further below-->

When training neural networks, we attempt to tune a set of hyperparameters so that the RMSE is minimized. 
When our method for such adjustment can no longer reduce the RMSE, we say that the given algorithm has converged.
In practice, some implementations of algorithms require that we stop the optimization process in 
exceptional situations (e.g., a divide by zero), or a pre-set limit on the number of steps or elapsed
time is reached. Thus the implementation may terminate even when the algorithm has not converged.
At termination, we desire that the RMSE be "small". Note that other measures could be used to judge
when to accept that the optimization is converged, for instance the Mean Absolute Error (MAE). 
However, the MAE is not used either in a convergence test nor in our overall ranking as there 
does not appear to be consensus on its use. See, e.g., \citep{willmott2005advantages,chai2014root}.

In our tests, a termination limit for second-order algorithms is 200 iterations. 
On the other hand, first-order algorithms were set to several values, depending on how well and how fast they converged: `maxit1storderA=1000` iterations, `maxit1storderB=10000` iterations, and `maxit1storderC=100000` iterations.
The full list of the maximum iteration number per package:algorithm is given in Appendix C. We were unable to completely harmonize the hyperparameters as an appropriate learning rate differed between package, despite the algorithm being similarly named.

We measure performance primarily by relative computing time between methods on a particular computing 
platform. We could also count measures of iterations, function evaluations or similar quantities that
indicate the computing effort. We note that differences in machine architecture and in the attached
libraries (e.g., BLAS choices for R) will modify our measures. We are putting our tools on a
Github repository so that further evaluation can be made by ourselves and others as hardware and software
evolves. 

The resulting files recording performance that are in our repository were mostly generated by one of us (SM)
on a Windows system build 10.0.18362.752 with an i7-8750H CPU, a Intel(R) UHD Graphics 630 and NVIDIA GeForce 
GTX 1060 chip, and 16 GB of RAM. The authors have multiple computers with different operating software and
configurations, and our experiences on those systems appear to be consistent with the results from the
test system described above.

<!-- JN??: Can I get some guidance of what to include/exclude in rest of this section? -->

<!-- B: Basically,
1. Introduce datasets, explain process of finding packages
2. Results of reviewing and testing - explain how we got the result files with metrics (the template) & ease of use scores
3. How we then ranked the results of speed and accuracy -> note, I didn't rank the ease of use and its per package, which might be why Christophe noted those as individual scores not global scores
-->
*************************************




Our research process was divided into 3 phases.

## Phase 1 - Preparation of benchmark datasets

### Datasets => NEED TO BE FINISHED??

All the datasets we use cannot generally be modeled using a non-iterative calculation
such as Ordinary Least Squares. Varying levels of difficulty in modeling the
different data sets are intended to allow us to further classify different algorithms and the packages
that implement them. Sonja Surjanovic and Derek Bingham of Simon Fraser University created a useful website 
from which three of the multivariate datasets were drawn. We note the link, name and difficulty level of 
the three datasets:  
- http://www.sfu.ca/~ssurjano/fried.html        (Friedman - average)  
- http://www.sfu.ca/~ssurjano/detpep10curv.html (Dette - medium)  
- http://www.sfu.ca/~ssurjano/ishigami.html     (Ishigami - high)  
The other multivariate dataset, Ref153, was taken from ...

Three of the univariate datasets we used were taken from a website of the US National Institute
for Standards and Technology (NIST):
https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml. (Gauss1 - low; Gauss2 - low; Gauss3 - average)

Univariate datasets Dmod1, Dmod2 are from ...

Dreyfus1 is a pure neural network which has no error. This can make it difficult for algorithms 
that assume an error exists. Dreyfus2 is Dreyfus1 with errors.
NeuroOne  from ...  

Finally, we also consider a Simon Wood test dataset, used in 
\citep{wood2011fast} for benchmarking generalized additive models. 
Precisely, we consider a generation of Gaussian random variates $Y_i$, $i=1,\dots,n$ with the mean $\mu_i$ defined as
$$
\mu_i = 1+ f_0(x_{i,0})+f_1(x_{i,1})+f_2(x_{i,2})+f_3(x_{i,3})
+f_4(x_{i,4})+f_0(x_{i,5})
$$
and standard deviation $\sigma=1/4$ where $f_j$ are Simon Wood's smooth 
functions defined in Appendix B, $x_{i,j}$ are uniform variates and $n=20,000$.


### Packages

Using \CRANpkg{RWsearch} \citep{R-RWsearch} we sought to automate the process of searching for neural network packages. 
All packages that have "neural network" as a keyword in the package title or in the package description were included. 
In May 2020, around 80 packages falls into this category.
Packages \pkg{nlsr}, \pkg{minpack.lm}, \pkg{caret} were added because the former 2 are important implementations of second-order algorithms while the latter is the first cited meta package in 
the CRAN's task view for machine learning, 	https://CRAN.R-project.org/view=MachineLearning, as well as the dependency for some of the other packages tested. 
Restricting to regression analysis left us with 49 package::algorithm pairs in 2019 and 60 package::algorithm pairs in 2020.


## Phase 2 - Review of packages and development of a benchmarking template

From documentation and example code, we learned that not all packages selected by the automated 
search fit the scope of our research. Some have no function to generate neural networks. 
Others were not regression neural networks of the perceptron type or 
were only intended for very specific purposes. Basically, each package was inspected 3 times.

1. The discard/not discard phase: depending on the package, this could be decided as easily as looking at
the DESCRIPTION file or having to go through the process of making the code and seeing the results.

2. Benchmarking with template that was developed in 2019 and encapsulated in the functions of 2020, keeping
notes of whether or not the package was easy to use.

**Templates for Testing Accuracy and Speed** 

As we inspected the packages, we developed a template for benchmarking. The structure of this
template (for each package) is as follows:  

1. Set up the test environment - loading of packages, setting working directory and options; 
2. Summary of tested datasets; 
3. Loop over datasets: 
      a. setting parameters for a specific dataset, 
      b. selecting benchmark options, 
      c. training a neural network with a tuned functions for each package, 
      d. calculation of convergence metrics (RMSE, MAE, WAE),
      e. plot each training over one initial graph, then plot the best result, 
      f. add results to the appropriate existing record (*.csv file) and 
      g. clear the environment for next loop.
4. Clearing up the environment for the next package.
It is optional to print warnings.

To simplify this process, we developed tools in the NNbenchmark package, of which the first version 
was created as part of GSoC 2019. In GSoC 2020, 3 functions encapsulating the template, that had been 
made generic with an extensive use of the incredible \code{do.call} function, were added:

1.    In \code{trainPredict\_1mth1data} a neural network is trained on one dataset and then used for predictions, with several utilities. Then, the performance of the neural network is exported, plotted and/or summarized.
2.    \code{trainPredict\_1data} serves as a wrapper function for trainPredict_1mth1data for multiple methods.
3.    \code{trainPredict\_1pkg} serves as a wrapper function for trainPredict_1mth1data for multiple datasets.

A function for the summary of accuracy and speed, \code{NNsummary}, was also added. The package repository is https://github.com/pkR-pkR/NNbenchmark, with package templates in https://github.com/pkR-pkR/NNbenchmarkTemplates. 

<!-- B??: should we add some examples with the code? As some package authors who publish in the R Journal do to introduce their package. Or we could link to specific pages of the website here?
==> CD: maybe we can put one example in Appendix but not in the body 
==> B: added an example with nnet -->

3. summarizing or re-reviewing the tested packages utility functions & documentation

**Ease of Use Scoring**

We define an ease-of-use measure based on what we considered a user would need when using a neural network 
package for nonlinear regression, namely, utility functions and sufficient documentation. 

1. Utilities (1 star)
    a. a predict function exists 
    b. scaling capabilities exist
2. Sufficient documentation (2 stars)
    a. the existence of useful example/vignette = (1 star)
        - clear, with regression = 2 points
        - unclear, examples use iris or are for classification only = 1 point
        - no examples = 0 points
    b. input/output is clearly documented, e.g., what values are expected and returned 
    by a function = (1 star)
        - clear input and output = 2 points
        - only one is clear = 1 point
        - both are not documented = 0 points

The ease-of-use measure ranges from 0 to 3 stars.

## Phase 3 - Collection of and analysis of results

### Results collection

Looping over the datasets using each package template, we collected results in the relevant package 
directories in the templates repository. 


### Analysis

To rank how well a package converged and its speed, we developed the following method:

1.	The results datasets are loaded into the R environment as one large list. The dataset names, package:algorithm names and all 10 run numbers, durations, and RMSE are extracted from that list
2.	For the duration score (DUR), the duration is averaged by dataset. 3 criteria for the RMSE score by dataset are calculated: 
    a.	The minimum value of RMSE for each package:algorithm as a measure of their best performance
    b.	The median value of RMSE for each package:algorithm as a measure of their average performance, without the influence of outliers
    c.	The spread of the RMSE values for each package which is measured by the difference between the median and the minimum RMSE (d51)

3.	Then, the ranks are calculated for every dataset and the results are merged into one wide dataframe.
    a.	The duration rank only depends on the duration.
    b.	For minimum RMSE values, ties are decided by duration mean, then the RMSE median
    c.	For median RMSE values, ties are decided by the RMSE minimum, then the duration mean
    d.	The d51 rank only depends on itself
  
<!-- B: to be precise - dfrRMSE <- dfr[order(dfr$RMSE.min, dfr$time.mean, dfr$RMSE.med),], so the RMSE score is the results of ranking by RMSE.min, if there is a tie then by time, if there is a tie then by median RMSE. -->


4.	A global score for all datasets is found by a sum of the ranks (of duration, minimum RMSE, median RMSE,  d51 RMSE) of each package:algorithm for each dataset
5.	The final table is the result of ranking by the global minimum RMSE scores for each package:algorithm




# Results

Table 1 gives the RMSE ranks, time ranks, and scores for ease of use for  each package::algorithm pair. A more complete list of metric scores and hyperparameters is given in Table 2 in Appendix C.

**Tables**

<!-- B??: would you prefer this ranked by RMSE score or alphabetically as it is now? -->

```{r echo=FALSE, message=FALSE}
library(kableExtra)
options(knitr.kable.NA = '')

if(file.exists("./tables/Table1.csv")) {  
  Table1 <- read.csv("./tables/Table1.csv", sep = ";")
  Table1 <- Table1[Table1[,2] != "", ]
}


# else if(file.exists("./tables/Table1.csv"))
# {  
#   Table1 <- read.csv("D:/DevGSoC/Packages/NNbenchmarkArticle/input/tables/Table1.csv", sep = ";")
#   Table1 <- Table1[Table1[,2] != "", ]
# }else if(file.exists("~/Documents/recherche-enseignement/code/R/NNbenchmark-project/NNbenchmarkArticle/input/tables/Table1.csv"))
# {  
#   Table1 <- read.csv("~/Documents/recherche-enseignement/code/R/NNbenchmark-project/NNbenchmarkArticle/input/tables/Table1.csv", sep = ";")
#   Table1 <- Table1[Table1[,2] != "", ]
# }

# else 
#   Table1 <- rep("missing",2)

colnames(Table1) <- c("Package", "Algorithm", "Time", "RMSE",
                    "Util", "Doc")

pkg.name <- Table1$Package[Table1$Package != ""]
idx.pkg.name <- (1+0:NROW(Table1))[Table1$Package != ""]
#repeate value
Table1$Package <- rep(pkg.name, times=diff(idx.pkg.name))
Table1$Util <- rep(Table1$Util[!is.na(Table1$Util)], times=diff(idx.pkg.name))
Table1$Doc <- rep(Table1$Doc[!is.na(Table1$Doc)], times=diff(idx.pkg.name))
#reorder columns => is that OK?
Table1 <- Table1[, c(1, 5:6, 2:4)]

kableExtra::kable(Table1, format = "latex", booktabs = TRUE, 
                  centering = TRUE, align="lcclcc",
                  caption="Result from Tested Packages") %>%
  add_header_above(c(" "=1, "Individual score"=2, " "=1,  "Global score"=2)) %>%
  column_spec(1, bold = TRUE) %>%
  collapse_rows(columns = 1, latex_hline = "major", valign ="middle") %>%
  kable_styling(font_size=7)
```


## Discussion and Recommendations

The following is a list of packages we included in this study, with brief descriptions.

1. \CRANpkg{AMORE} \citep{R-AMORE},
2. \CRANpkg{ANN2} \citep{R-ANN2},
3. \CRANpkg{appnn} \citep{R-appnn},
4. \CRANpkg{autoencoder} \citep{R-autoencoder},
5. \CRANpkg{automl} \citep{R-automl},
6. \CRANpkg{BNN} \citep{R-BNN},
7. \CRANpkg{brnn} \citep{R-brnn},
8. \CRANpkg{Buddle} \citep{R-Buddle},
9. \CRANpkg{CaDENCE} \citep{R-CaDENCE},
10. \CRANpkg{cld2} \citep{R-cld2},
11. \CRANpkg{cld3} \citep{R-cld3},
12. \CRANpkg{condmixt} \citep{R-condmixt},
13. \CRANpkg{DamiaNN} \citep{R-DamiaNN},
14. \CRANpkg{deep} \citep{R-deep},
15. \CRANpkg{deepdive} \citep{R-deepdive},
16. \CRANpkg{deepnet} \citep{R-deepnet},
17. \CRANpkg{deepNN} \citep{R-deepNN},
18. \CRANpkg{DNMF} \citep{R-DNMF},
19. \CRANpkg{elmNNRcpp} \citep{R-elmNNRcpp},
20. \CRANpkg{ELMR} \citep{R-ELMR},
21. \CRANpkg{EnsembleBase} \citep{R-EnsembleBase},
22. \CRANpkg{evclass} \citep{R-evclass},
23. \CRANpkg{gamlss.add} \citep{R-gamlss.add},
24. \CRANpkg{gcForest} \citep{R-gcForest},
25. \CRANpkg{GMDH} \citep{R-GMDH},
26. \CRANpkg{GMDH2} \citep{R-GMDH2},
27. \CRANpkg{GMDHreg} \citep{R-GMDHreg},
28. \CRANpkg{gnn} \citep{R-gnn},
29. \CRANpkg{grnn} \citep{R-grnn},
30. \CRANpkg{h2o} \citep{R-h2o},
31. \CRANpkg{hybridEnsemble} \citep{R-hybridEnsemble},
32. \CRANpkg{isingLenzMC} \citep{R-isingLenzMC},
33. \CRANpkg{keras} \citep{R-keras},
34. \CRANpkg{kerasR} \citep{R-kerasR},
35. \CRANpkg{leabRa} \citep{R-leabRa},
36. \CRANpkg{learNN} \citep{R-learNN},
37. \CRANpkg{LilRhino} \citep{R-LilRhino},
38. \CRANpkg{minpack.lm} \citep{R-minpack.lm}, 
39. \CRANpkg{MachineShop} \citep{R-MachineShop},
40. \CRANpkg{monmlp} \citep{R-monmlp},
41. \CRANpkg{neural} \citep{R-neural},
42. \CRANpkg{neuralnet} \citep{R-neuralnet},
43. \CRANpkg{NeuralNetTools} \citep{R-NeuralNetTools},
44. \CRANpkg{NeuralSens} \citep{R-NeuralSens},
45. \CRANpkg{NlinTS} \citep{R-NlinTS},
46. \CRANpkg{nlsr} \citep{R-nlsr},
47. \CRANpkg{nnet} \citep{R-nnet},
48. \CRANpkg{nnetpredint} \citep{R-nnetpredint},
49. \CRANpkg{nnfor} \citep{R-nnfor},
50. \CRANpkg{nntrf} \citep{R-nntrf},
51. \CRANpkg{nnli2bRcpp} \citep{R-nnlib2Rcpp},
52. \CRANpkg{onnx} \citep{R-onnx},
53. \CRANpkg{OptimClassifier} \citep{R-OptimClassifier},
52. \CRANpkg{OSTSC} \citep{R-OSTSC},
53. \CRANpkg{pnn} \citep{R-pnn},
54. \CRANpkg{polyreg} \citep{R-polyreg},
55. \CRANpkg{predictoR} \citep{R-predictoR},
56. \CRANpkg{qrnn} \citep{R-qrnn},
57. \CRANpkg{QuantumOps} \citep{R-QuantumOps},
58. \CRANpkg{quarrint} \citep{R-quarrint},
59. \CRANpkg{radiant.model} \citep{R-radiant.model},
60. \CRANpkg{rasclass} \citep{R-rasclass},
61. \CRANpkg{rcane} \citep{R-rcane},
62. \CRANpkg{regressoR} \citep{R-regressoR},
63. \CRANpkg{rminer} \citep{R-rminer},
64. \CRANpkg{rnn} \citep{R-rnn},
65. \CRANpkg{RSNNS} \citep{R-RSNNS},
66. \CRANpkg{ruta} \citep{R-ruta},
67. \CRANpkg{simpleNeural} \citep{R-simpleNeural},
68. \CRANpkg{snnR} \citep{R-snnR},
69. \CRANpkg{softmaxreg} \citep{R-softmaxreg},
70. \CRANpkg{Sojourn.Data} \citep{R-Sojourn.Data},
71. \CRANpkg{spnn} \citep{R-spnn},
72. \CRANpkg{TeachNet} \citep{R-TeachNet},
73. \CRANpkg{tensorflow} \citep{R-tensorflow},
74. \CRANpkg{tfestimators} \citep{R-tfestimators},
75. \CRANpkg{trackdem} \citep{R-trackdem},
76. \CRANpkg{TrafficBDE} \citep{R-TrafficBDE},
77. \CRANpkg{tsensembler} \citep{R-tsensembler},
78. \CRANpkg{validann} \citep{R-validann},
80. \CRANpkg{zFactor} \citep{R-zFactor}.

<!-- I think was from Patrice: For the comments per package, I suggest one small text per package, at least for the best 12/15 packages, and some text for the significant bad or painful packages like tensorflow, keras, h2o. etc. -->


<!-- (NOTE TO MENTORS: based on latest 2020 Run 04 with corrections) -->

### 2nd order algorithms
Out of all the algorithms, the following second algorithms generally performed better in terms of convergence despite being set to a much lower number of iterations, to be precise a fifth or even less, than the first-order algorithms. 

An important finding is that 11 out of 15 of these package::algorithms use the algorithms included in \code{optim} from \CRANpkg{stats}. 2 of them, \CRANpkg{CaDENCE}'s BFGS \citep{R-CaDENCE} and \CRANpkg{validann}'s BFGS and L-BFGS-B \citep{R-validann}, do so with no intermediate package. However, it is not clearly stated in CaDENCE's documentation that optim's BFGS algorithm is used and not one of the other algorithms. Furthermore, the mention of Nelder-Mead in the documentation might lead users to believe that optim's Nelder-Mead is used instead. Speed and variation between results are also not as good as other package's that use optim. This could be because CaDENCE is intended for probabilistic nonlinear models with a full title of "Conditional Density Estimation Network Construction and Evaluation". On the other hand, validann is clearly a package that allows a user to use all optim's algorithms. validann::L-BFGS-B ranks lower than validann::BFGS in just about everything for most runs, despite the former being more sophisticated. This is probably due to our efforts to harmonize parameters under-utilizing the possibilities of the L-BFGS-B algorithm. Both CaDENCE and validann's BFGS are outperformed by nnet, especially in terms of speed.

\CRANpkg{nnet} \citep{R-nnet} differs from the two packages because it uses the C code  from optim (converted earlier from Fortran) instead of calling optim from R. It also only implements the BFGS algorithm. This could be what allows it to be faster. nnet is only beaten by the Extreme Learning Machine (ELM) algorithms in terms of speed. However, there is a larger variation between results (see the RMSEd51.score in Appendix C) in comparison to validann::BFGS. Most likely, the different default values are the cause of this. For instance, nnet uses a range of initial random weights of 0.7 while validann uses a value of 0.5. In spite of these results, the real reason most authors or users are likely to choose nnet is because it ships with base R and is even mentioned as the very first package in CRAN's task view for machine learning.

Our research found that 6 of the 11 packages that use optim do so through nnet. Moreover, 8 packages for neural networks, though not tested, use nnet. The total number of nnet dependencies found through a search through the offline database of CRAN with RWsearch came up with 136 packages, although some might be using nnet for the multinomial log-linear models, not neural networks. As for the ones we tested, there were several similarities and differences. The packages that use nnet for neuralnetworks are often meta packages with a host of other machine learning algorithms. \CRANpkg{caret} \citep{R-caret}, also mentioned in the taskview, boasts 238 methods with around 13 different neural network packages with somewhat deceivingly simple name of "Classification and Regression Training". It has many pre-processing utilities available, as well as other tools. 

<!-- purely alphabetically -->
\CRANpkg{EnsembleBase} \citep{R-EnsembleBase} maybe useful for those who wish to make ensembles and test a grid of parameters although the documentation. \CRANpkg{MachineShop} \citep{R-MachineShop} has 51 algorithms, with some additional information about the response variable types in the second vignette, functions for preprocessing and tuning, performance assesment, and presentation of results. \CRANpkg{radiant.model} \citep{R-radiant.model} has an unchangeable maxit of 10000 in the original package. Perhaps the author thought this was reasonable as source of the algorithm, nnet, is quite fast. We changed this to harmonize the parameters. \CRANpkg{rminer} \citep{R-rminer} is the only package dependant on nnet that ranks above nnet at number 2 for minimum RMSE, and even number 1 in some runs. It also ranks number 1 on the other accuracy measures (median RMSE, minimum MAE, minimum WAE)  and is only behind deepdive and minpack.lm in terms of results that are consistent and do not vary (RMSEd51). The difference is probably from the change of maximum allowable weights in rminer to 10000 from 1000 in nnet, which is also probably the reason it its fits are slower. \CRANpkg{traineR} \citep{R-traineR} claims to unify the different methods of creating models between several learning algorithms. 

Something worth noting is that nnet and validann do not have external normalization, and it is especially recommended for validann <!-- Patrice?? -->. However, some of the packages dependent on nnet do have this utility and it is included in the scoring for ease of use. With NNbenchmark, this is done through setting scale = TRUE in the function \code{prepare.ZZ}. Note that scaling might lead to complicating the constraints which is not always worth it. Regardless, users might want to have the utility and most likely want a clear explanation of the method chosen to center the variables. Scaling is one of the things that \CRANpkg{optimx} \citep{R-optimx} incorporates in an attempt to make a more useful version of optim that only allows for changing the sign of the function which might not even be considered as scaling \citep{Nash-nlpor14}.

<!-- TO DO
BFGS - monmlp
LM - nlsr & minpack.lm
Gauss Newton - brnn
nlm() - qrnn (probably should mention somewhere here that Alex J Cannon functions automatically includes/ offers scaling)
snnR
-->

<!-- TO REVISE & ADD

Ranked directly after are some packages that depend on nnet or use the same functions. They differ in 
how well they decide initial parameters. rminer (rank 4), MachineShop (rank 5), and radiant.model (rank 7) 
use nnet. Note, radiant.model has its iterations set to 10000, which originally made it slower yet converge
better. We used a modified version of the package. At rank 6 is validann's BFGS algorithm using stats::optim(). 
Its use of optim's L-BFGS-B ranked at number 9 with CaDENCE's use of optim's BFGS.. \CRANpkg{monmlp}, from 
the same author as CaDENCE (Alex Cannon), uses the package. \CRANpkg{optimx}'s BFGS \citep{R-optimx}. 

The best  in this class were. \CRANpkg{minpack.lm} and. \CRANpkg{nlsr}, tied at rank number 1. The Levenberg-Marquardt (LM) algorithm used is fast and converges well. stats::nls() 
is used. However, these packages require a handwritten formula that may not be ideal for certain 
situations. A more popular package for neural networks is nnet. This might be because it is part of base R. 
It implements the BFGS algorithm with stats::optim(). 

Alex Cannon also implemented a quantile regression neural network in qrnn with stats::nlm(). It requires  more iterations and is not as fast compared to the other second-order algorithms. However, it is avaluable implementation of quantile regression. 

Last but not least is \CRANpkg{brnn}'s Gauss Newton algorithm which ranks at number 8. brnn is easy to use but does not converge as well due to a hidden constraint: a missing first parameter. Furthermore, brnn's algorithm minimizes the sum of squared errors and a penalty on parameters instead of just the sum of squared errors. This may prevent parameters to get highly correlated, especially with an almost degenerated Jacobian matrix. -->


### Recommended: 1st order algorithms
validann optim CG -slow
RSNNS SCG
h2o back-propagation
RSNNS Rprop
ANN2 adam
CaDENCE Rprop -SLOW
deepnet BP
AMORE ADAPTgdwm
AMORE ADAPTgd
ANN2 sgd
automl trainwgrad
ANN2 rmsprop
RSNNS BackpropChunk
RSNNS BackWeightDecay
RSNNS Std_Backpropagation
RSNNS BackpropMomentum
automl trainwpso
validann optim NelderMead
snnR Semi Smooth Newton
RSNNS BackpropBatch
validann optim SANN
monmlp optimx Nelder Mead

### Not recommended: 1st order algorithms <- DISCUSS CUTOFF
By package
ELMR, elmNNRcpp - fast ELM algorithms. Unfortunately, can't finetune, does not converge well.
neuralnet: a large ammount of iterations, slow, erratic failures
tensorflow: NOT EASY TO USE, subsequently keras, tfestimators, ruta ...
user needs to understand the language
However, advanced users might be able to highly specify a neural network to their needs
(customization?)

By algorithm:
neuralnet rprop+
neuralnet rprop-
neuralnet slr - once ranked well with 100000 iterations
AMORE BATCHgd
CaDENCE pso psoptim - need to reconfigure?
elmNNRcpp - fast, no iterations
RSNNS Quickprop (?)
AMORE BATCHgdwm
tensorflow MomentumOptimizer
tensorflow AdamOptimizer
ELMR - fast, no iterations
tensorflow GradientDescentOptimizer
keras rmsprop
keras adagrad
keras sgd
keras adadelta
tensorflow AdagradOptimizer
keras adam
tensorflow FtrlOptimizer
neuralnetwork sag
tensorflow AdadeltaOptimizer
neuralnet backprop - note, might not actually reflect standings, somehow from template to template the learning rate disappeared. Will fix this in future runs

### Untested => TO DO - LIST
<!-- B??: Christophe, shouldn't Appendix D be here? Although not necessarily in the format of  a table -->


# Conclusion and perspective

## Positives

- We are happy to note the existence of neural network packages in R with  algorithms that converge well.
- \pkg{nnet}, which uses ?? need font choice?? optim's BFGS method, is already often chosen to represent neural networks for packages that are either a collection of independent machine learning algorithms, ensembles, or even applications in a field such as ... ?? need to complete sentence??. 
JN??: Why is this positive?
- R users have access to a wide variety of neural network methods, including from libraries of other programming languages
and using many different types of algorithms. ?? have we defined hyperparameters?? hyperparameters, and uses
<!-- JN??: this bullet point still clumsy, but when we decide content, I'll edit -->

## Negatives
- We are disappointed that many of the packages we reviewed had poor documentation.
- It would be helpful if there were more packages with (different) second order algorithms. A number of the
<!-- packages reviewed appear to offer essentially the same methods. JN??: Is that what we want to say? -->
- We often found it difficult to discover what default starting values were used for model parameters, or 
<!-- else bad choices. JN??: Again, is that our message? -->

## Future work

As the field of neural networks continue to grow, there will always be more algorithms to validate. 
For current packages available for R, we believe our research should be extended to encompass more types of neural networks 
and their data formats (classifier neural networks, recurrent neural networks, and so on). Different 
rating schemes and different settings for package functions should also be tried out, though such 
work suffers from the curse of too many dimensions of effort.

We also aspire to the possibility of an idealized neural network package, in particular for R. From the
work in this paper, we believe that such a package should offer

- ease of use in setting up computations and attaching data
- the incorporation of second-order optimization methods in training the model, with
appropriate computation of derivative information and appropriate internal
steps, e.g., Levenberg-Marquardt stabilization
- good documentation and controls, especially of initial values for model parameters


## Acknowledgements

This work was possible due to the support of the Google Summer of Code initiative for R
during years 2019 and 2020.
Students Salsabila Mahdi (2019 and 2020) and Akshaj Verma (2019) are grateful to 
Google for the financial support.
<!-- For the acknowledgments, maybe : «  » + later some acknowledgments to the referees. -->


\bibliography{RJreferences}


# Appendix

## Appendix A

Consider a set of observations $y_i$ and its corresponding predictions $\hat y_i$ for $i=1,\dots,n$.
The three metrics used were:
$$
MAE = \frac1n\sum_{i=1}^n|y_i - \hat y_i|,~
RMSE = \frac1n\sqrt{\sum_{i=1}^n(y_i - \hat y_i)^2},~
WAE = \frac1n\max_{i=1,\dots,n}|y_i - \hat y_i|.
$$
These values represent the absolute, the squared and the maximum norm of residual vectors.

## Appendix B

We define three smooth functions for Simon Wood's test dataset
$$
f_0=5*\sin(2\pi x),~
f_1=exp(3*x)-7
f_2=0.5 x^{11}*(10(1 - x))^6 - 10 (10*x)^3*(1 - x)^{10},~
$$
$$
f_3=15 \exp(-5 |x-1/2|)-6,~
f_4=2-1_{(x <= 1/3)}(6*x)^3 - 1_{(x >= 2/3)} (6-6*x)^3 - 
1_{(2/3 > x > 1/3)}(8+2\sin(9*(x-1/3)\pi)).
$$

## Appendix C

```{r echo=FALSE, message=FALSE}
library(kableExtra)
if(file.exists("./tables/TableAppendixC.csv")) {  
  supC <- read.csv("./tables/TableAppendixC.csv", sep = ";") 
}
# 
# }else if(file.exists("D:/DevGSoC/Packages/NNbenchmarkArticle/input/tables/TableAppendixC.csv"))
# {  supC <- read.csv("D:/DevGSoC/Packages/NNbenchmarkArticle/input/tables/TableAppendixC.csv", sep = ";")
# }else if(file.exists("~/Documents/recherche-enseignement/code/R/NNbenchmark-project/NNbenchmarkArticle/input/tables/TableAppendixC.csv"))
# {  supC <- read.csv("~/Documents/recherche-enseignement/code/R/NNbenchmark-project/NNbenchmarkArticle/input/tables/TableAppendixC.csv", sep = ";")
# }else 
#   supC <- rep("missing",2)
colnames(supC) <- c("Num", "Input format", "Maxit", "Learn. rate", "RMSE median",
                    "RMSE d51", "MAE", "WAE")
kableExtra::kable(supC, format = "latex", booktabs = TRUE, 
                  centering = TRUE, 
                  caption="All convergence scores per package:algorithm") %>%
  add_header_above(c(" ", "Input parameter"=3, "Score"=4)) %>%
  kable_styling(font_size=8)
```

## Appendix D

\begin{table}[htb!]
\begin{center}
\caption{\textbf{Review of Ommitted Packages}}
\scriptsize

\begin{tabular}{l l l l}
  \toprule
  No & Name (package)         & Category & Comment \\
  \midrule
  1  &\pkg{appnn}             & AP        & This package provides a feed forward neural network to predict\\
     &                        &           & the amyloidogenicity propensity of polypeptide sequences      \\
  2  &\pkg{autoencoder}       & AP        & This package provides a sparse autoencoder, an unsupervised   \\
     &                        &           & algorithm that learns useful features from the data its given \\
  3  &\pkg{BNN}               & RE*       & This package uses a feed forward neural network to perform    \\
     &                        &           & regression as provided in the examples, however, it is unclear\\      &                        &           & whether it fits the form of perceptron that is the scope of   \\
     &                        &           & our research. Moreover, it states that it is intended for     \\      &                        &           & variable selection. Although how exactly the package would be \\
     &                        &           & used to do so isn't accessible in the package, especially     \\
     &                        &           & considering the source code is based on .c code that users of \\
     &                        &           & R might not understand. It's performance is slow, which may   \\
     &                        &           & have to do with the 100.000 iterations it needs, although     \\
     &                        &           & quite accurate for simple datasets.                           \\
  4  &\pkg{Buddle}            & RE**      & (errors)\\
  5  &\pkg{cld2}              & 00        & \\
  6  &\pkg{cld3}              & AP        & \\
  7  &\pkg{condmixt}          & AP        & \\
  8  &\pkg{deep}              & CL        & \\
  9  &\pkg{DALEX2}            & 00        & removed keyword, included in 2019 \\
  10 &\pkg{DamiaNN}           & RE**      & (errors) exported functions, still doesn't work \\
  11 &\pkg{DChaos}            & ??        & removed keyword for some reason, need to check out! \\
  12 &\pkg{deepNN}            & RE**      & (errors) I/O weird, ragged vector array \\
  13 &\pkg{DNMF}              & AP        & \\
  14 &\pkg{evclass}           & CL        & \\
  15 &\pkg{gamlss.add}        & RE        & there is some code but dist not appropriate \\
  16 &\pkg{gcForest}          & 00        & \\
  17 &\pkg{GMDH}              & TS        & \\
  18 &\pkg{GMDH2}             & CL        & \\
  19 &\pkg{GMDHreg}           & RE*       & \\
  20 &\pkg{grnn}              & RE**      & \\
  21 &\pkg{hybridEnsemble}    & ??        & \\ 
  22 &\pkg{isingLenzMC}       & AP        & \\
  23 &\pkg{leabRa}            & ??        & \\      
  24 &\pkg{learNN}            & ??        & \\     
  25 &\pkg{LilRhino}          & AP        & \\
  26 &\pkg{neural}            & CL        & \\
  27 &\pkg{NeuralNetTools}    & UT        & tools for neural networks           \\
  28 &\pkg{NeuralSens}        & UT        & tools for neural networks           \\
  29 &\pkg{NlinTS}            & TS        & Time Series                         \\
  30 &\pkg{nnetpredint}       & UT        & confidence intervals for NN          \\
  31 &\pkg{nnfor}             & TS        & Times Series, uses neuralnet         \\
  32 &\pkg{nntrf}             & UT        & \\
  33 &\pkg{onnx}              &           & provides an open source format       \\
  34 &\pkg{OptimClassifier}   &           & choose classifier parameters, nnet   \\
  35 &\pkg{OSTSC}             &           & solving oversampling classification  \\
  36 &\pkg{passt}             &           & \\
  36 &\pkg{pnn}               &           & Probabilistic                        \\
  37 &\pkg{polyreg}           &           & polyregression as alternative to NN  \\
  38 &\pkg{predictoR}         &           & shiny interface, neuralnet           \\
  39 &\pkg{ProcData}          &           & \\
  40 &\pkg{QuantumOps}        &           & classifies MNIST, Schuld (2018), removed keyword, in 2019 \\
  41 &\pkg{quarrint}          &           & specified classifier for quarry data \\
  42 &\pkg{rasclass}          &           & classifier for raster images, nnet?  \\
  43 &\pkg{rcane}             &           & \\
  44 &\pkg{regressoR}         &           & a manual rich version of predictoR   \\
  45 &\pkg{rnn}               &           & Recurrent                            \\
  46 &\pkg{RTextTools}        &           & \\
  47 &\pkg{ruta}              &           & \\
  48 &\pkg{simpleNeural}      &           & \\
  49 &\pkg{softmaxreg}        &           & \\
  50 &\pkg{Sojourn.Data}      &           & sojourn Accelerometer methods, nnet? \\
  51 &\pkg{spnn}              &           & classifier, probabilistic            \\
  52 &\pkg{studyStrap}        &           & \\
  53 &\pkg{TeachNet}          &           & classifier, selfbuilt, slow          \\
  54 &\pkg{tensorflow}        &           & \\
  55 &\pkg{tfestimators}      &           & \\
  56 &\pkg{trackdem}          &           & classifier for particle tracking     \\
  57 &\pkg{TrafficBDE}        & RE*       & \\
  58 &\pkg{tsfgrnn}           &           & \\
  59 &\pkg{yap}               &           & \\
  60 &\pkg{yager}             & RE*       & \\
  61 &\pkg{zFactor}           & AP        & 'compressibility' of hydrocarbon gas \\
\end{tabular}
\end{center}
\end{table}



## Appendix ??

```{r}
library(NNbenchmark)
nrep <- 5       
odir <- tempdir()

library(nnet)
nnet.method <- "BFGS"
hyperParams.nnet <- function(...) {
    return (list(iter=200, trace=FALSE))
}
NNtrain.nnet <- function(x, y, dataxy, formula, neur, method, hyperParams, ...) {
    
    hyper_params <- do.call(hyperParams, list(...))
    
    NNreg <- nnet::nnet(x, y, size = neur, linout = TRUE, maxit = hyper_params$iter, trace=hyper_params$trace)
    return(NNreg)
}
NNpredict.nnet  <- function(object, x, ...) { predict(object, newdata=x) }
NNclose.nnet    <- function() {  if("package:nnet" %in% search())
                                detach("package:nnet", unload=TRUE) }
nnet.prepareZZ  <- list(xdmv = "d", ydmv = "v", zdm = "d", scale = TRUE)

res <- trainPredict_1pkg(4:5, pkgname = "nnet", pkgfun = "nnet", nnet.method,
  prepareZZ.arg = nnet.prepareZZ, nrep = nrep, doplot = TRUE,
  csvfile = FALSE, rdafile = FALSE, odir = odir, echo = FALSE)
```

