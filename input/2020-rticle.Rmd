---
title: A review of R neural network packages (with NNbenchmark)$:$ accuracy and ease of use
author:
  - name: Salsabila Mahdi
    affiliation: Universitas Syiah Kuala
    address: JL. Syech Abdurrauf No.3, Aceh 23111, Indonesia
    email:  bila.mahdi@mhs.unsyiah.ac.id
  - name: Akshaj Verma
    affiliation: Manipal Institute of Technology
    address: Manipal, Karnataka, 576104, India
    email:  akshajverma7@gmail.com
  - name: Christophe Dutang
    affiliation: University Paris-Dauphine, University PSL, CNRS, CEREMADE
    address: Place du Maréchal de Lattre de Tassigny, 75016 Paris, France
    email:  dutang@ceremade.dauphine.fr
  - name: Patrice Kiener
    affiliation: InModelia
    address: 5 rue Malebranche, 75005 Paris, France
    email:  patrice.kiener@inmodelia.com
  - name: John C. Nash
    affiliation: Telfer School of Management, University of Ottawa
    address: 55 Laurier Avenue East, Ottawa, Ontario K1N 6N5 Canada
    email:  nashjc@uottawa.ca
preamble: |
  % Any extra LaTeX you need in the preamble
output: rticles::rjournal_article
---
## Abstract

<!-- (JN) -->
<!-- There are many R packages whose purpose is neural network modeling. With support of the Google Summer of Code (GSoC) initiative for R, two of the authors were supported, mentored by the other authors, to attempt to benchmark and report on as many ofthese packages as possible. This article presents the overall purpose of the study and the research problems investigated, the basic design of the study, the major findings or trends found as a result of our analysism a brief summary of our interpretations and  conclusions. These have been encapsulated in the NNbenchmark package. -->

<!-- (PK)  -->
<!-- For the last 30 years, neural networks have evolved from an academic topic (exploratory field of research in the academic community) to a common word in scientific computing. Following this trend, CRAN has been accepting and hosts many packages that claim to provide neural network modeling. Our latest count in May 2020 is 70 (??) packages. This is fine but how accurate and reliable are these packages? Can we trust all them? Are there packages that perfom better and are easier to use than other packages? Are there packages better suited to beginners and other packages better suited to experts? Do these packages provide the (full set of) features that can be found in some older respected (trusted) proprietary software? -->

<!-- This paper is, to our knowledge, the first attempt to test this vast amount of packages on (with/against) a few datasets of (with) different levels of complexity, from simple to difficult (hard) to train, benchmark them with a certain metrics, and finally rank them. The evaluation was conducted by the first two authors during summers 2019 and 2020, as students enrolled in the Google Summer of Code program, with templates prepared by the last two authors. Due to time constraints, we restricted our evaluation to regression algorithms and ignore algorithms for classification and exotic purpose. This left us with 49 (in 2019, 60 in 2020?) pairs packages::algorithms. The criteria used in our benchmark are: (i) the accuracy, i.e. the ability to find the global minima on 13 datasets, measured by the Root Mean Square Error (RMSE), (ii) the speed of the training algorithm, (iii) the ease of use, (iv) the quality of the documentation.  -->

<!-- All packages::algorithms were given a score for each criterion and ranked in a global table. Overall, 12/15 packages::algorithms are considered accurate and reliable and can be recommended for a daily use. 45 packages should be avoided as they are not accurate, takes too much time, are difficult to handle or have poor or even no documentation. -->

<!-- Our package called NNbenchmark, the templates and the code used to test each package::algorithm are available on Github at the address github.com/??  and can be used by the package authors to verify their metrics and eventually improve their package. Finally, we provide some hints and features of a dreamed (??JN: an idealized) neural network package that could serve as a guidance for developing the next generation of R neural network packages. -->

For the last 30 years, neural networks have evolved from an academic topic to a common tool in scientific computing. Following this trend, CRAN has chosen to host many packages that claim to provide neural network modeling. Our count in May 2020 is roughly 70 packages. How accurate and reliable are these packages and their documentation? Are there packages that perform better in terms of accuracy? Are some better suited to beginners while others require expert knowledge? Do these packages provide the features found in some dedicated proprietary software?
<!-- ??B: ... roughly 70 packages - are we counting those based on the RWsearch or? -->

This paper is, to our knowledge, the first attempt to test this rather large number of packages against a few datasets with different levels of complexity, and to benchmark and rank them with certain metrics.  We have restricted our evaluation to regression algorithms applied on the one-hidden layer perceptron and ignored those for classification or other specialized purposes. This left us with 49 package::algorithm pairs in 2019 and 60 package::algorithm pairs in 2020. The criteria used in our benchmark were: (i) the accuracy, i.e. the ability to find the global minima on 13 datasets, measured by the Root Mean Square Error (RMSE) in a limited number of iterations; (ii) the speed of the training algorithm; (iii) the availability of helpful utilities; (iv) and the quality of the documentation.
<!-- ??B: is it a good idea to talk about 2019 results in the abstract? Wouldn't that be confusing? -->

All package::algorithm pairs were given a score for each criterion and ranked in a global table. Overall, 15 package::algorithm pairs are considered accurate and reliable and can be recommended for daily usage. 45 algorithms should be avoided as they are either less accurate, slow, difficult to handle, or have poor or no documentation.

Our codes, templates, and the NNbenchmark package used to test each package::algorithm pair are publicly available at the address https:://github.com/pkR-pkR/NNbenchmark. These can be used by the package authors and others to verify the metrics and eventually improve their own package or code. Finally, we provide some hints and features to guide the development of an idealized neural network package for R.
<!-- ??B: just making sure, since the templates are in the other repo, are we simply referring them to the repo that has links to other repos in its README? -->
<!-- ??B: also, I was thinking, should/could there be an ideal system for NN packages? Like, a certain format to follow - so it is easier for users to navigate through the packages -->


<!-- ??B: I think the abstract provided by Patrice has some pros over Josh's at the top. I think it's worth dicussing what should be said in the abstract, and what should be said in the intro/elsewhere -->


## Introduction


<!-- (PK)  -->
(PK) For the last 30 years, neural networks have evolved from an academic topic to a common tool in scientific computing. As a convenience in the general conversation, the same term is used in a generic manner (for a shortcut to) for different model structures and applications: multilayer perceptron for regression, multilayer perceptron for classification, multilayer perceptron for specialized applications (mixture of models, conditional distribution, etc), recurrent neural network for autoregressive time series (NARMAX models), convolutional neural networks for dimension reduction and pattern recognition, deep neural networks for image or voice recognition.

Most of the above types of neural networks can be found in R packages hosted on CRAN but without any warranty about the quality or the accuracy of the calculation, since CRAN has no tools to check them. Package users have to believe package authors who are assumed to have selected the best algorithms and tuned them with the appropriate hyperparameters. This is rather problematic with neural networks as many poor algorithms have been mentioned in the literature, and are probably implemented in a few packages.

The goal of this paper is to verify the accuracy and the ease of use (including the quality of the documentation) of the packages hosted on CRAN that supply the most common neural network structure + algorithm, i.e. the multilayer perceptron for regression purpose. 




<!-- (SB)  -->
(SB) The R Project for Statistical Computing (\url{www.r-project.org}), as any opensource platform, relies on its contributors to keep it up to date. Neural networks, based on the brain's own connections system, are a class of models in the growing field of machine learning forwhich R has a number of tools. Previously, neural networks could be considered more theory than practice, partly because the algorithms used are computationally demanding. 

A neural network algorithm requires complicated calculations to improve the model control parameters. As with other optimization problems, the gradient of the chosen cost function that indicates the lack of suitability of the model is sought. This lets us improve the model by changing the parameters in the negative gradient direction. Parameters for the model are generally obtained using part of the available data (a training set) and tested on the remaining data. Modern software allows much of this work, including approximation of the gradient, to be carried out without a large effort by the the user. 

This process can generally be made more efficient if we can also approximate second derivatives of the cost function, allowing us to use its curvature via the Hessian matrix. There are a large number of approaches, of which quasi-Newton algorithms are perhaps the most common and useful. Within this group, methods based on the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for updating the Hessian approximation (or its inverse) provide several well-known examples. In conducting this study, we believed that these second-order algorithms would perform better than first-order methods. 

Regardless of our belief, we wished to be able to conduct a thorough examination of 
these training algorithms in R. There are many packages, but barely any information to allow comparison. Our work, reported here, aims to provide a framework for benchmarking neural network packages. We restrict our examination to packages for R, and in this report focus on those that provide neural networks of the perceptron type, that is, one input layer, one normalized layer, one hidden layer with a nonlinear activation function that is usually tanh(), and one output output layer. 


# Methodology

??JN: ******************************

In working on material below, I think we need to provide some explanation of goals
- What does "convergence" mean in our context?
- What do we mean by RMSE, other measures? Should define here for later use.
- What do we mean by "performance"? Other goals?

*************************************


Our research process was divided into 3 phases.

## Phase 1 - Preparation

### Datasets => NEED TO BE FINISHED??

All the datasets we use cannot generally be modeled using a non-iterative calculation
such as Ordinary Least Squares. Varying levels of difficulty in modeling the
different data sets are intended to allow us to further classify different algorithms and the packages
that implement them. Sonja Surjanovic and Derek Bingham of Simon Fraser University created a useful website 
from which three of the multivariate datasets were drawn. We note the link, name and difficulty level of 
the three datasets:  
- http://www.sfu.ca/~ssurjano/fried.html        (Friedman - average)  
- http://www.sfu.ca/~ssurjano/detpep10curv.html (Dette - medium)  
- http://www.sfu.ca/~ssurjano/ishigami.html     (Ishigami - high)  
The other multivariate dataset, Ref153, was taken from ...

Three of the univariate datasets we used were taken from a website of the US National Institute
for Standards and Technology (NIST):
https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml. (Gauss1 - low; Gauss2 - low; Gauss3 - average)

Univariate datasets Dmod1, Dmod2 are from ...

Dreyfus1 is a pure neural network which has no error. This can make it difficult for algorithms 
that assume an error exists. Dreyfus2 is Dreyfus1 with errors.
NeuroOne  from ...  
Wood ...

### Packages

Using \CRANpkg{RWsearch} \citep{R-RWsearch} we sought all were able to automate the process. 
All packages that have "neural network" as a keyword in the package title or in the package description 
were included. Packages \pkg{nlsr}, \pkg{minpack.lm}, \pkg{caret} were added because ...

## Phase 2 - Review of packages and development of a benchmarking template

From documentation and example code, we learned that not all packages selected by the automated 
search fit the scope of our research. Some have no function to generate neural networks. They 
are simply meta-packages. Others were not regression neural networks of the perceptron type or 
were only intended for very specific purposes. 

**Template => TO REVISE AFTER 2020 CODE**  

As we inspected the packages, we developed a template for benchmarking. The structure of this
template (for each package) is as follows:  
(1) Set up the test environment - loading of packages, setting working directory and options;   
(2) Summary of datasets;  
(3) Loop over datasets:
    (a) setting parameters for a specific dataset 
    (b) selecting benchmark options 
    (c) training a neural network with a tuned functions for each package 
    (d) calculation of RMSE and MAE (??definition, reference)
    (e) plot each training over one initial graph, then plot the best result 
    (f) add results to the appropriate existing record (*.csv file) and 
    (g) clear the environment for next loop; and  
(4) Clearing up the environment for the next package.
(5) It is optional to print warnings.

To simplify this process, we developed tools in the NNbenchmark package, of which the first version 
was created as part of the 2019 GSoC activity and later refined in the 2020 initiative. The package 
repository is https://github.com/pkR-pkR/NNbenchmark, with package templates 
in https://github.com/pkR-pkR/NNbenchmarkTemplates).

## Phase 3 - Collection of and analysis of results

### Results collection

Looping over the datasets using each package template, we collected results in the relevant package 
directories in the templates repository. 


### Analysis

To rank the how well a package converged and its speed, we developed the following method:

1.	The results datasets are loaded into the R environment as one large list. The dataset names, package:algorithm names and all 10 run numbers, durations, and RMSE are extracted from that list
2.	For the duration score (DUR), the duration is averaged by dataset. 3 criteria for the RMSE score by dataset are calculated:

a.	The minimum value of RMSE for each package:algorithm as a measure of their best performance
b.	The median value of RMSE for each package:algorithm as a measure of their average performance, without the influence of outliers
c.	The spread of the RMSE values for each package which is measured by the difference between the median and the minimum RMSE (d51)

3.	Then, the ranks are calculated for every dataset and the results are merged into one wide dataframe.
a.	The duration rank only depends on the duration.
b.	For minimum RMSE values, ties are decided by duration mean, then the RMSE median
c.	For median RMSE values, ties are decided by the RMSE minimum, then the duration mean
d.	The d51 rank only depends on itself

4.	A global score for all datasets is found by a sum of the ranks (of duration, minimum RMSE, median RMSE,  d51 RMSE) of each package:algorithm for each dataset
5.	The final table is the result of ranking by the global minimum RMSE scores for each package:algorithm
6. In addition to the previous metrics, two other convergence metrics have been considered:
the Mean Absolute Error (MAE) and the Worst Absolute Error (WAE), see Appendix. The ranking on
those two metrics may help distinguish packages with close RMSE values. 
However, we do not choose the MAE for overall ranking as there is no consensus in the literature, 
see e.g. \citep{willmott2005advantages,chai2014root}.

To rank how easy or not a package was to use (TO BE DISCUSSED FURTHER):
- Functionality (util): scaling, input, output, trace
- Documentation (docs): examples, structure/functions, vignettes

# Results

**Tables**
(NOTE: FINAL MEASURE FOR CONVERGENCE - RMSE RANKS? OR A COMBINATION OF OTHER MEASURES?
As in Christophe's recent email: L1 MAE(), L2 RMSE(), Linfinity (WAE)) --> see Appendix

(ALSO: THE FOLLOWING IS SIMPLY ALPHABETIC LIST FOR ALL TESTED, I WILL DIVIDE THE TABLE INTO 4: 
2nd ORDER always recommended, 1st ORDER recommended, 1st ORDER not recommended, untested packages)

\begin{center}
\textbf{Table 1: Review of Tested Packages}
\begin{tabular}{ l l l l l l l}
  No & Package::Function                & Algorithm             & RMSE & DUR & UTIL & DOCS \\
  1  &\pkg{AMORE}                       & 1. ADAPTgd            &      &     &      &      \\
     &::train                           & 2. ADAPTgdwm          &      &     &      &      \\
     &                                  & 3. BATCHgd            &      &     &      &      \\ 
     &                                  & 4. BATCHgdwm          &      &     &      &      \\
  2  &\pkg{ANN2}                        & 5. adam               &      &     &      &      \\
     &::neuralnetwork                   & 6. rmsprop            &      &     &      &      \\
     &                                  & 7. sgd                &      &     &      &      \\
  3  &\pkg{automl}                      & 8. trainwgrad.adam    &      &     &      &      \\
     &::automl\_train\_manual           & 9. trainwgrad.RMSprop &      &     &      &      \\
     &                                  &10. trainwpso          &      &     &      &      \\
  4  &\pkg{brnn}::brnn                  &11. Gauss-Newton       &      &     &      &      \\
  5  &\pkg{CaDENCE}                     &12. optim(BFGS)        &      &     &      &      \\
     &::cadence.fit                     &13. pso::psoptim       &      &     &      &      \\
     &                                  &14. Rprop              &      &     &      &      \\
  6  &\pkg{caret}::avNNet               &15. nnet::optim(BFGS)  &      &     &      &      \\
  7  &\pkg{deepdive}                    &16. adam               &      &     &      &      \\
     &::deepnet                         &17. gradientDescent    &      &     &      &      \\
     &                                  &18. momentum           &      &     &      &      \\ 
     &                                  &19. rmsProp            &      &     &      &      \\
  8  &\pkg{deepnet}::nn.train           &20. BP                 &      &     &      &      \\
  9  &\pkg{elmNNRcpp}::elm\_train       &21. ELM                &      &     &      &      \\
  10 &\pkg{ELMR}::OSelm\_train.formula  &22. ELM                &      &     &      &      \\
  11 &\pkg{EnsembleBase}::Regression.Batch.Fit &23. nnet::optim(BFGS) &      &     &      &      \\
  12 &\pkg{h2o}::deeplearning           &24.                    &      &     &      &      \\
  13 &\pkg{keras}                       &25. adadelta           &      &     &      &      \\
     &::fit                             &26. adagrad            &      &     &      &      \\
     &                                  &27. adam               &      &     &      &      \\
     &                                  &28. adamax             &      &     &      &      \\ 
     &                                  &29. nadam              &      &     &      &      \\
     &                                  &30. rmsprop            &      &     &      &      \\ 
     &                                  &31. sgd                &      &     &      &      \\
  14 &\pkg{MachineShop}::fit            &32. nnet::optim(BFGS)  &      &     &      &      \\
  15 &\pkg{minpack.lm}::nlsLM           &33. LM                 &      &     &      &      \\
  16 &\pkg{monmlp}::fit                 &34. BFGS               &      &     &      &      \\
     &\pkg{monmlp}::fit                 &35. Nelder-Mead        &      &     &      &      \\
  17 &\pkg{neuralnet}::                 &36. backprop           &      &     &      &      \\
     &::neuralnet                       &37. rprop-             &      &     &      &      \\
     &                                  &38. rprop+             &      &     &      &      \\
     &                                  &39. sag                &      &     &      &      \\
     &                                  &40. slr                &      &     &      &      \\
  18 &\pkg{nlsr}::nlxb                  &41. NashLM             &      &     &      &      \\
  19 &\pkg{nnet}::nnet                  &42. optim(BFGS)        &      &     &      &      \\
  20 &\pkg{qrnn}::qrnn.fit              &43. nlm()              &      &     &      &      \\
  21 &\pkg{radiant.model}::radiant.model&45. nnet::optim(BFGS)  &      &     &      &      \\
  22 &\pkg{rminer}::fit                 &46. nnet::optim(BFGS)  &      &     &      &      \\
  23 &\pkg{RSNNS}::                     &47. BackpropBatch      &      &     &      &      \\
     &\pkg{RSNNS}::                     &48. BackpropChunk      &      &     &      &      \\
     &\pkg{RSNNS}::                     &49. BackpropMomentum   &      &     &      &      \\
     &\pkg{RSNNS}::                     &50. BackpropWeightDecay&      &     &      &      \\
     &\pkg{RSNNS}::                     &51. Quickprop          &      &     &      &      \\
     &\pkg{RSNNS}::                     &52. Rprop              &      &     &      &      \\
     &\pkg{RSNNS}::                     &53. SCG                &      &     &      &      \\
     &\pkg{RSNNS}::                     &54. Std-Backpropagation&      &     &      &      \\
  24 &\pkg{snnR}                        &55.                    &      &     &      &      \\
  25 &\pkg{traineR}                     &55.                    &      &     &      &      \\
  26 &\pkg{validann}::                  &56. BFGS               &      &     &      &      \\
     &\pkg{validann}::                  &57. CG                 &      &     &      &      \\
     &\pkg{validann}::                  &58. L-BFGS-B           &      &     &      &      \\
     &\pkg{validann}::                  &59. Nelder-Mead        &      &     &      &      \\
     &\pkg{validann}::                  &60. SANN               &      &     &      &      \\  
  \end{tabular}
\end{center}

(THE FOLLOWING IS JUST AN ALPHABETICALLY ORDERED LIST OF CURRENTLY UNTESTED PACKAGES)
\begin{center}
\textbf{Table 2: Review of Ommitted Packages}

\begin{tabular}{l l l l}
  \toprule
  No & Name (package)         & Category & Comment \\
  \midrule
  1  &\pkg{appnn}             & AP        & This package provides a feed forward neural network to predict\\
     &                        &           & the amyloidfogenicity propensity of polypeptide sequences     \\
  2  &\pkg{autoencoder}       & AP        & \\     
  3  &\pkg{BNN}               & RE*       & \\
  4  &\pkg{Buddle}            & RE**      & (errors) even performance on examples is bad \\
  5  &\pkg{cld2}              & 00        & \\
  6  &\pkg{cld3}              & AP        & \\
  7  &\pkg{condmixt}          & AP        & \\
  8  &\pkg{deep}              & CL        & \\
  9  &\pkg{DALEX2}            & 00        & removed keyword, included in 2019 \\
  10 &\pkg{DamiaNN}           & RE**      & (errors) exported functions, still doesn't work \\
  11 &\pkg{DChaos}            & ??        & removed keyword for some reason, need to check out! \\
  12 &\pkg{deepNN}            & RE**      & (errors) I/O weird, ragged vector array \\
  13 &\pkg{DNMF}              & AP        & \\
  14 &\pkg{evclass}           & CL        & \\
  15 &\pkg{gamlss.add}        & RE        & there is some code but dist not appropriate \\
  16 &\pkg{gcForest}          & 00        & \\
  17 &\pkg{GMDH}              & TS        & \\
  18 &\pkg{GMDH2}             & CL        & \\
  19 &\pkg{GMDHreg}           & RE*       & \\
  20 &\pkg{grnn}              & RE**      & \\
  21 &\pkg{hybridEnsemble}    & ??        & \\ 
  22 &\pkg{isingLenzMC}       & AP        & \\
  23 &\pkg{leabRa}            & ??        & \\      
  24 &\pkg{learNN}            & ??        & \\     
  25 &\pkg{LilRhino}          & AP        & \\
  26 &\pkg{neural}            & CL        & \\
  27 &\pkg{NeuralNetTools}    & UT        & tools for neural networks           \\
  28 &\pkg{NeuralSens}        & UT        & tools for neural networks           \\
  29 &\pkg{NlinTS}            & TS        & Time Series                         \\
  30 &\pkg{nnetpredint}       & UT        & confidence intervals for NN          \\
  31 &\pkg{nnfor}             & TS        & Times Series, uses neuralnet         \\
  32 &\pkg{nntrf}             & UT        & \\
  33 &\pkg{onnx}              &           & provides an open source format       \\
  34 &\pkg{OptimClassifier}   &           & choose classifier parameters, nnet   \\
  35 &\pkg{OSTSC}             &           & solving oversampling classification  \\
  36 &\pkg{passt}             &           & \\
  36 &\pkg{pnn}               &           & Probabilistic                        \\
  37 &\pkg{polyreg}           &           & polyregression as alternative to NN  \\
  38 &\pkg{predictoR}         &           & shiny interface, neuralnet           \\
  39 &\pkg{ProcData}          &           & \\
  40 &\pkg{QuantumOps}        &           & classifies MNIST, Schuld (2018), removed keyword, in 2019 \\
  41 &\pkg{quarrint}          &           & specified classifier for quarry data \\
  42 &\pkg{rasclass}          &           & classifier for raster images, nnet?  \\
  43 &\pkg{rcane}             &           & \\
  44 &\pkg{regressoR}         &           & a manual rich version of predictoR   \\
  45 &\pkg{rnn}               &           & Recurrent                            \\
  46 &\pkg{RTextTools}        &           & \\
  47 &\pkg{ruta}              &           & \\
  48 &\pkg{simpleNeural}      &           & \\
  49 &\pkg{softmaxreg}        &           & \\
  50 &\pkg{Sojourn.Data}      &           & sojourn Accelerometer methods, nnet? \\
  51 &\pkg{spnn}              &           & classifier, probabilistic            \\
  52 &\pkg{studyStrap}        &           & \\
  53 &\pkg{TeachNet}          &           & classifier, selfbuilt, slow          \\
  54 &\pkg{tensorflow}        &           & \\
  55 &\pkg{tfestimators}      &           & \\
  56 &\pkg{trackdem}          &           & classifier for particle tracking     \\
  57 &\pkg{TrafficBDE}        & RE*       & \\
  58 &\pkg{tsfgrnn}           &           & \\
  59 &\pkg{yap}               &           & \\
  60 &\pkg{yager}             & RE*       & \\
  61 &\pkg{zFactor}           & AP        & 'compressibility' of hydrocarbon gas \\
\end{tabular}

\end{center}


## Discussion and Recommendations

<!-- list may need to be updated. Already in bibliography RJreferences -->
<!-- JN?? I reran RWsearch. See RWSearch20200731.txt file on repo. Slight differences. -->

The following is a list of packages we included in this study, with brief descriptions.

1. \CRANpkg{AMORE} \citep{R-AMORE},
2. \CRANpkg{ANN2} \citep{R-ANN2},
3. \CRANpkg{appnn} \citep{R-appnn},
4. \CRANpkg{autoencoder} \citep{R-autoencoder},
5. \CRANpkg{automl} \citep{R-automl},
6. \CRANpkg{BNN} \citep{R-BNN},
7. \CRANpkg{brnn} \citep{R-brnn},
8. \CRANpkg{Buddle} \citep{R-Buddle},
9. \CRANpkg{CaDENCE} \citep{R-CaDENCE},
10. \CRANpkg{cld2} \citep{R-cld2},
11. \CRANpkg{cld3} \citep{R-cld3},
12. \CRANpkg{condmixt} \citep{R-condmixt},
13. \CRANpkg{DamiaNN} \citep{R-DamiaNN},
14. \CRANpkg{deep} \citep{R-deep},
15. \CRANpkg{deepdive} \citep{R-deepdive},
16. \CRANpkg{deepnet} \citep{R-deepnet},
17. \CRANpkg{deepNN} \citep{R-deepNN},
18. \CRANpkg{DNMF} \citep{R-DNMF},
19. \CRANpkg{elmNNRcpp} \citep{R-elmNNRcpp},
20. \CRANpkg{ELMR} \citep{R-ELMR},
21. \CRANpkg{EnsembleBase} \citep{R-EnsembleBase},
22. \CRANpkg{evclass} \citep{R-evclass},
23. \CRANpkg{gamlss.add} \citep{R-gamlss.add},
24. \CRANpkg{gcForest} \citep{R-gcForest},
25. \CRANpkg{GMDH} \citep{R-GMDH},
26. \CRANpkg{GMDH2} \citep{R-GMDH2},
27. \CRANpkg{GMDHreg} \citep{R-GMDHreg},
28. \CRANpkg{gnn} \citep{R-gnn},
29. \CRANpkg{grnn} \citep{R-grnn},
30. \CRANpkg{h2o} \citep{R-h2o},
31. \CRANpkg{hybridEnsemble} \citep{R-hybridEnsemble},
32. \CRANpkg{isingLenzMC} \citep{R-isingLenzMC},
33. \CRANpkg{keras} \citep{R-keras},
34. \CRANpkg{kerasR} \citep{R-kerasR},
35. \CRANpkg{leabRa} \citep{R-leabRa},
36. \CRANpkg{learNN} \citep{R-learNN},
37. \CRANpkg{LilRhino} \citep{R-LilRhino},
38. \CRANpkg{minpack.lm} \citep{R-minpack.lm}, 
39. \CRANpkg{MachineShop} \citep{R-MachineShop},
40. \CRANpkg{monmlp} \citep{R-monmlp},
41. \CRANpkg{neural} \citep{R-neural},
42. \CRANpkg{neuralnet} \citep{R-neuralnet},
43. \CRANpkg{NeuralNetTools} \citep{R-NeuralNetTools},
44. \CRANpkg{NeuralSens} \citep{R-NeuralSens},
45. \CRANpkg{NlinTS} \citep{R-NlinTS},
46. \CRANpkg{nlsr} \citep{R-nlsr},
47. \CRANpkg{nnet} \citep{R-nnet},
48. \CRANpkg{nnetpredint} \citep{R-nnetpredint},
49. \CRANpkg{nnfor} \citep{R-nnfor},
50. \CRANpkg{nntrf} \citep{R-nntrf},
51. \CRANpkg{nnli2bRcpp} \citep{R-nnlib2Rcpp},
52. \CRANpkg{onnx} \citep{R-onnx},
53. \CRANpkg{OptimClassifier} \citep{R-OptimClassifier},
52. \CRANpkg{OSTSC} \citep{R-OSTSC},
53. \CRANpkg{pnn} \citep{R-pnn},
54. \CRANpkg{polyreg} \citep{R-polyreg},
55. \CRANpkg{predictoR} \citep{R-predictoR},
56. \CRANpkg{qrnn} \citep{R-qrnn},
57. \CRANpkg{QuantumOps} \citep{R-QuantumOps},
58. \CRANpkg{quarrint} \citep{R-quarrint},
59. \CRANpkg{radiant.model} \citep{R-radiant.model},
60. \CRANpkg{rasclass} \citep{R-rasclass},
61. \CRANpkg{rcane} \citep{R-rcane},
62. \CRANpkg{regressoR} \citep{R-regressoR},
63. \CRANpkg{rminer} \citep{R-rminer},
64. \CRANpkg{rnn} \citep{R-rnn},
65. \CRANpkg{RSNNS} \citep{R-RSNNS},
66. \CRANpkg{ruta} \citep{R-ruta},
67. \CRANpkg{simpleNeural} \citep{R-simpleNeural},
68. \CRANpkg{snnR} \citep{R-snnR},
69. \CRANpkg{softmaxreg} \citep{R-softmaxreg},
70. \CRANpkg{Sojourn.Data} \citep{R-Sojourn.Data},
71. \CRANpkg{spnn} \citep{R-spnn},
72. \CRANpkg{TeachNet} \citep{R-TeachNet},
73. \CRANpkg{tensorflow} \citep{R-tensorflow},
74. \CRANpkg{tfestimators} \citep{R-tfestimators},
75. \CRANpkg{trackdem} \citep{R-trackdem},
76. \CRANpkg{TrafficBDE} \citep{R-TrafficBDE},
77. \CRANpkg{tsensembler} \citep{R-tsensembler},
78. \CRANpkg{validann} \citep{R-validann},
80. \CRANpkg{zFactor} \citep{R-zFactor}.
81.
82.
83.
84.
85.
86.
87.

<!-- (NOTE TO MENTORS: based on latest 2019 Run 04 or Run 03) -->

A. Recommended: 2nd order algorithms
Out of all the algorithms, these second algorithms generally performed better in terms of convergence 
despite being set to a much lower number of iterations, 200, than the first-order algorithms. Moreover, 
they performed better in terms of speed. The best  in this class were. \CRANpkg{minpack.lm} and. \CRANpkg{nlsr}, 
tied at rank number 1. The Levenberg-Marquardt (LM) algorithm used is fast and converges well. stats::nls() 
is used. However, these packages require a handwritten formula that may not be ideal for certain 
situations. A more popular package for neural networks is nnet. This might be because it is part of base R. 
It implements the BFGS algorithm with stats::optim(). 

Ranked directly after are some packages that depend on nnet or use the same functions. They differ in 
how well they decide initial parameters. rminer (rank 4), MachineShop (rank 5), and radiant.model (rank 7) 
use nnet. Note, radiant.model has its iterations set to 10000, which originally made it slower yet converge
better. We used a modified version of the package. At rank 6 is validann's BFGS algorithm using stats::optim(). 
Its use of optim's L-BFGS-B ranked at number 9 with CaDENCE's use of optim's BFGS.. \CRANpkg{monmlp}, from 
the same author as CaDENCE (Alex Cannon), uses the package. \CRANpkg{optimx}'s BFGS \citep{R-optimx}.  

Alex Cannon also implemented a quantile regression neural network in qrnn with stats::nlm(). It requires 
more iterations and is not as fast compared to the other second-order algorithms. However, it is a 
valuable implementation of quantile regression. Last but not least is \CRANpkg{brnn}'s Gauss Newton algorithm 
which ranks at number 8. brnn is easy to use but does not converge as well due to a hidden constraint: a 
missing first parameter. Furthermore, brnn's algorithm minimizes the sum of squared errors and a penalty 
on parameters instead of just the sum of squared errors. This may prevent parameters to get highly correlated, 
especially with an almost degenerated Jacobian matrix.

B. Recommended: 1st order algorithms
validann optim CG -slow
RSNNS SCG
h2o back-propagation
RSNNS Rprop
ANN2 adam
CaDENCE Rprop -SLOW
deepnet BP
AMORE ADAPTgdwm
AMORE ADAPTgd
ANN2 sgd
automl trainwgrad
ANN2 rmsprop
RSNNS BackpropChunk
RSNNS BackWeightDecay
RSNNS Std_Backpropagation
RSNNS BackpropMomentum
automl trainwpso
validann optim NelderMead
snnR Semi Smooth Newton
RSNNS BackpropBatch
validann optim SANN
monmlp optimx Nelder Mead

C. Not recommended: 1st order algorithms <- DISCUSS CUTOFF
By package
ELMR, elmNNRcpp - fast ELM algorithms. Unfortunately, can't finetune, does not converge well.
neuralnet: a large ammount of iterations, slow, erratic failures
tensorflow: NOT EASY TO USE, subsequently keras, tfestimators, ruta ...
user needs to understand the language
However, advanced users might be able to highly specify a neural network to their needs
(customization?)

By algorithm:
neuralnet rprop+
neuralnet rprop-
neuralnet slr - once ranked well with 100000 iterations
AMORE BATCHgd
CaDENCE pso psoptim - need to reconfigure?
elmNNRcpp - fast, no iterations
RSNNS Quickprop (?)
AMORE BATCHgdwm
tensorflow MomentumOptimizer
tensorflow AdamOptimizer
ELMR - fast, no iterations
tensorflow GradientDescentOptimizer
keras rmsprop
keras adagrad
keras sgd
keras adadelta
tensorflow AdagradOptimizer
keras adam
tensorflow FtrlOptimizer
neuralnetwork sag
tensorflow AdadeltaOptimizer
neuralnet backprop - note, might not actually reflect standings, somehow from template to template the learning rate disappeared. Will fix this in future runs


D. Untested => TO DO - LIST

## Conclusion => TO DO AFTER 2020 CODE

??JN: Can we start to put in some major findings? i.e., important positive findings,
big negatives?

### Positives (no particular order)
1. the existence of algorithms that converge well
2. nnet, which uses optim's BFGS, is already often chosen to represent neural networks for packages that are either a collection of independent machine learning algorithms, ensembles, or even applications in a field such as ...
3. the wide variety of neural networks available to users of R, from libraries of other programming languages to many different types of algorithms, hyperparameters, and uses

### Negatives (no particular order)
1. bad documentation
2. the lack of packages that expand the number of unique second order algorithms. (Perhaps even the existence of what can be considered as repetitive packages?)
3. the lack of clear default values, or bad default values

## Future work
As the field of neural networks continue to grow, there will always be more algorithms to validate. 
For current algorithms in R, our research should be extended to encompass more types of neural networks 
and their data formats (classifier neural networks, recurrent neural networks, and so on). Different 
rating schemes and different parameters for package functions can also be tried out.

## Acknowledgements
This work was possible due to the support of the Google Summer of Code initiative for R
during years 2019 and 2020.
Students Salsabila Mahdi (2019 and 2020) and Akshaj Verma (2019) are grateful to 
Google for the financial support.

\bibliography{RJreferences}

- The dreamed NN package: Recommendation to package authors
- Conclusion
- Acknowledgments

<!-- For the comments per package, I suggest one small text per package, at least for the best 12/15 packages, and some text for the significant bad or painful packages like tensorflow, keras, h2o. etc. -->

For the acknowledgments, maybe : «  » + later some acknowledgments to the referees.

How do we proceed?

## Appendix A

Consider a set of observations $y_i$ and its corresponding predictions $\hat y_i$ for $i=1,\dots,n$.
The three metrics used were:
$$
MAE = \frac1n\sum_{i=1}^n|y_i - \hat y_i|,~
RMSE = \frac1n\sqrt{\sum_{i=1}^n(y_i - \hat y_i)^2},~
WAE = \frac1n\max_{i=1,\dots,n}|y_i - \hat y_i|.
$$
These values represent the absolute, the squared and the maximum norm of residual vectors.
